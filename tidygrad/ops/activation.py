# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_ops.activation.ipynb.

# %% auto 0
__all__ = ['Relu', 'Sigmoid']

# %% ../../nbs/02_ops.activation.ipynb 2
import numpy as np
from tidygrad.ops import UnaryElementwiseOp

# %% ../../nbs/02_ops.activation.ipynb 3
class Relu(UnaryElementwiseOp):
    """Take the sigmoid of a tensor"""

    name_template = "relu({})"

    def __init__(self, a, name=None):
        super().__init__(a, name=name)
        # self.out = Tensor(np.maximum(0, self.args[0].data), name=self.name, op=self)
        self.set_out(np.maximum(0, self.args[0].data))

    def backward(self):
        self.check_backward()
        self.parents[0].grad += self.out.grad * (self.out.data > 0)

# %% ../../nbs/02_ops.activation.ipynb 4
class Sigmoid(UnaryElementwiseOp):
    """Take the sigmoid of a tensor"""

    name_template = "sigmoid({})"

    def __init__(self, a, name=None):
        super().__init__(a, name=name)
        self.set_out(1 / (1 + np.exp(-self.args[0].data)))

    def backward(self):
        self.check_backward()
        with np.errstate(under="ignore"):  # Triggered by infinitesimally small 1-data
            self.parents[0].accum_grad(
                self.out.grad * self.out.data * (1 - self.out.data)
            )
