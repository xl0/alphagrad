[
  {
    "objectID": "tensor.html",
    "href": "tensor.html",
    "title": "Tensor class",
    "section": "",
    "text": "source\n\nTensor\n\n Tensor (data, name=None, op=None, eps=1e-08, requires_grad=False)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nsimplify_trace\n\n simplify_trace (trace)\n\n\nsource\n\n\nTensor\n\n Tensor (data, name=None, op=None, eps=1e-08, requires_grad=False)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TidyGrad",
    "section": "",
    "text": "A tidy library for gradient-based optimization\n\n\n\nTests\n\n\n\n\npip install -e tidygrad\n\n\n\npip install -e .[dev] # Local install with dev dependencies\n\nHack\nHack\nHack\n(edit and run the notebooks in the ./nbs directory).\n\nnbdev_prepare\nnbdev_prepare will:\n\nExport all norebooks into the ./tidygrad directory.\n\nNote: the notebooks themselves have an export cell, so they are exported every time you run them.\n\nRun all notebooks (equivalent of testing)\nGenerate REDME.md from the index.ipynb\nGenerate the docs\n\n\n\n\n\n# Later"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "TidyGrad",
    "section": "",
    "text": "pip install -e tidygrad"
  },
  {
    "objectID": "index.html#dev",
    "href": "index.html#dev",
    "title": "TidyGrad",
    "section": "",
    "text": "pip install -e .[dev] # Local install with dev dependencies\n\nHack\nHack\nHack\n(edit and run the notebooks in the ./nbs directory).\n\nnbdev_prepare\nnbdev_prepare will:\n\nExport all norebooks into the ./tidygrad directory.\n\nNote: the notebooks themselves have an export cell, so they are exported every time you run them.\n\nRun all notebooks (equivalent of testing)\nGenerate REDME.md from the index.ipynb\nGenerate the docs"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "TidyGrad",
    "section": "",
    "text": "# Later"
  },
  {
    "objectID": "tests/test_ops.html",
    "href": "tests/test_ops.html",
    "title": "Binary elementwise ops",
    "section": "",
    "text": "import numpy as np\nfrom tidygrad.tensor import Tensor\nfrom tidygrad.utils.grad_check import grad_check\n\n\ndef run_test_binary_elementwise(func, shape1, shape2=None, pos_only=False):\n    \"\"\"Test a binary elementwise function, like add, mul, etc\"\"\"\n    shape2 = shape1 if shape2 is None else shape2\n    if pos_only:\n        a = Tensor(np.abs(np.random.randn(*shape1)) + 1e-8, name=\"a\", requires_grad=True)\n        b = Tensor(np.abs(np.random.randn(*shape2)) + 1e-8, name=\"b\", requires_grad=True)\n    else:\n        a = Tensor(np.random.randn(*shape1), name=\"a\", requires_grad=True)\n        b = Tensor(np.random.randn(*shape2), name=\"b\", requires_grad=True)\n\n    t = func(inputs=None, params=(a, b))\n    t.backward()\n    grad_check(func=func, inputs=None, params=(a, b), verbose=False)\n\n\ndef run_test_unary_elementwise(func, shape, pos_only=False, offset=1e-3):\n    \"\"\"Test a unary elementwise function, like exp, log, etc\"\"\"\n    if pos_only:\n        # Mostly for log(a) - it's positive only and is instable too close to zero.\n        a = Tensor(\n            np.abs(np.random.randn(*shape)) + offset, name=\"a\", requires_grad=True\n        )\n    else:\n        a = Tensor(np.random.randn(*shape), name=\"a\", requires_grad=True)\n\n    t = func(inputs=None, params=(a,))\n    t.backward()\n    grad_check(func=func, inputs=None, params=(a,))\n\n\na = Tensor(np.random.randn(2, 3), name=\"a\", requires_grad=True)\nb = Tensor(np.random.randn(2, 3), name=\"b\", requires_grad=True)\n\nc = a + b\n\nloss = c.sum()\n\nloss.backward()\n\n\ndef add_func(inputs, params: tuple = ()):\n    a, b = params\n    loss = a.add(b, \"t\").sum()\n    return loss\n\nrun_test_binary_elementwise(add_func, (1, 1))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for a: 0.0000%\n\n\n\ndef sub_func(inputs, params: tuple = ()):\n    a, b = params\n    loss = a.sub(b, \"t\").sum(\"loss\")\n    return loss\n\nrun_test_binary_elementwise(sub_func, (100, 100))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for a: 0.0000%\n\n\n\ndef mul_func(inputs, params: tuple = ()):\n    a, b = params\n    loss = a.mul(b, \"t\").sum(\"loss\")\n    return loss\n\nrun_test_binary_elementwise(mul_func, (100, 100))\n\nMax fractional gradient difference for b: 0.0001%\nMax fractional gradient difference for a: 0.0002%\n\n\n\ndef pow_func(inputs, params: tuple = ()):\n    a = params[0]\n    loss = a.pow(2, \"t\").sum(\"loss\")\n    return loss\n\n\ndef run_test_pow(shape):\n    a = Tensor(np.random.randn(*shape), name=\"a\", requires_grad=True)\n    a.data = np.where(np.abs(a.data) &lt; 1e-5, 1e-5, a.data)\n\n    t = pow_func(inputs=None, params=(a,))\n\n    t.backward()\n\n    grad_check(func=pow_func, inputs=None, params=(a,))\n\n\n# XXX pow is unstable for values close to zero\n# run_test_pow((100, 100))\n\n\nUnary elementwise functions\n\ndef log_func(inputs, params: tuple = ()):\n    (a, ) = params\n\n    loss = a.log(\"t\").sum(\"loss\")\n    return loss\n\nrun_test_unary_elementwise(log_func, (100, 100), pos_only=True)\n\nMax fractional gradient difference for a: 0.1248%\n\n\n\ndef exp_func(inputs, params: tuple = ()):\n    (a, ) = params\n\n    loss = a.exp(\"t\").sum(\"loss\")\n    return loss\n\nrun_test_unary_elementwise(exp_func, (100, 100))\n\nMax fractional gradient difference for a: 0.0028%\n\n\n\nimport tidygrad.func as F\n\n# from tidygrad.func import relu, sigmoid, tanh, softmax, gelu, new_gelu\n\n\ndef sigmoid_func(inputs, params: tuple = ()):\n    (a, ) = params\n    t = F.sigmoid(a)\n    return t.sum(\"loss\")\n\nrun_test_unary_elementwise(sigmoid_func, (100, 100))\n\nMax fractional gradient difference for a: 0.0005%\n\n\n\ndef tanh_func(inputs, params: tuple = ()):\n    (a,) = params\n    t = F.tanh(a)\n    return t.sum(\"loss\")\n\n\nrun_test_unary_elementwise(tanh_func, (100, 100))\n\nMax fractional gradient difference for a: 0.0010%\n\n\n\ndef relu_func(inputs, params: tuple = ()):\n    (a,) = params\n    t = F.relu(a, \"t\")\n    return t.sum(\"loss\")\n\n\nrun_test_unary_elementwise(relu_func, (100, 100))\n\nMax fractional gradient difference for a: 0.0000%\n\n\n\ndef gelu_func(inputs, params: tuple = ()):\n    (a,) = params\n    t = F.gelu(a)\n    return t.sum(\"loss\")\n\n\n# XXX Stability issues\n# run_test_unary_elementwise(gelu_func, (100, 100))\n\n\ndef softmax_func(inputs, params: tuple = ()):\n    (a, ) = params\n    n_batch, n_classes = a.shape\n    y = np.zeros(a.shape)\n    np.random.seed(42)\n    y[np.arange(n_batch), np.random.randint(0, n_classes, n_batch)] = 1\n    y = Tensor(y, name=\"y\")\n    sm = F.softmax(a, \"t\")\n\n    cross_entropy = y * sm.log() + (1-y) * (1 - sm).log()\n    #\n    return cross_entropy.sum(\"loss\")\n\nrun_test_unary_elementwise(softmax_func, (1, 5))\n\nMax fractional gradient difference for a: 0.0007%\n\n\n\ndef matmul_func(inputs, params: tuple[Tensor] = ()):\n    a, b = params\n    t = a.mmul(b, \"t\")\n    return t.sum(\"loss\")\n\ndef run_test_matmul(shape1, shape2):\n    a = Tensor(np.random.randn(*shape1), name=\"a\", requires_grad=True)\n    b = Tensor(np.random.randn(*shape2), name=\"b\", requires_grad=True)\n    t = matmul_func(inputs=None, params=(a, b))\n    t.backward()\n\n    grad_check(func=matmul_func, inputs=None, params=(a, b))\n\nrun_test_matmul((10, 100), (100, 50))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for a: 0.0000%\n\n\n\n\nBroadcasting\n\nrun_test_binary_elementwise(add_func, (2, 10, 1), (10, 100))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for a: 0.0000%\n\n\n\nrun_test_matmul((2, 10, 100), (100, 10))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for a: 0.0000%\n\n\n\n\nTest loss functions\n\n# def lt_func(inputs, params: tuple = ()):\n#     a, b = params\n#     loss = (a &lt; b).sum(\"loss\")\n#     return loss\n\n# run_test_binary_elementwise(lt_func, (100, 100), (100, 100))\n\n# a = Tensor(np.random.randn(100, 100), name=\"a\")\n# b = Tensor(np.random.randn(100, 100), name=\"b\")\n\n# t = lt_func(inputs=None, params=(a, b))\n# t.backward()\n\n\n# from tidygrad.functional import BCE_loss\n\n\ndef bceloss_func(inputs, params: tuple = ()):\n    y = inputs[0]\n    x = params[0]\n\n    loss = F.BCE_loss(x, y).sum(\"loss\")\n    return loss\n\nx = Tensor(np.random.randn(100), name=\"x\", requires_grad=True)\ny = Tensor(np.random.randn(100), name=\"y\", requires_grad=True)\n\nt = bceloss_func(inputs=(y, ), params=(x, ))\nt.backward()\n\ngrad_check(func=bceloss_func, inputs=(y, ), params=(x, ))\n\nMax fractional gradient difference for x: 0.0029%\n\n\n\n\nTest Dropout\n\n# from tidygrad.functional import dropout\n\n\ndef dropout_func(inputs, params: tuple = ()):\n    p = params[0]\n\n    np.random.seed(1337)\n    t = F.dropout(p, 0.3, training=True)\n    return t.sum(\"loss\")\n\np = Tensor(np.random.randn(100), name=\"p\", requires_grad=True)\n\nt = dropout_func(inputs=None, params=(p, ))\nt.backward()\n\ngrad_check(func=dropout_func, inputs=None, params=(p, ))\n\nMax fractional gradient difference for p: 0.0000%\n\n\n\n# from tidygrad.functional import embedding\n\n\n\nTest Embedding\n\ndef embedding_func(inputs, params: tuple = ()):\n    idxs = inputs[0]\n    w = params[0]\n    t = F.embedding(w, idxs, \"t\")\n    return t.sum(\"loss\")\n\nidxs = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nw = Tensor(np.random.randn(10, 100), name=\"w\", requires_grad=True)\n\nt = embedding_func(inputs=(idxs, ), params=(w, ))\nt.backward()\n\ngrad_check(func=embedding_func, inputs=(idxs, ), params=(w, ))\n\nMax fractional gradient difference for w: 0.0000%\n\n\n\n\nTest sum and mean and std\n\ndef sum_test(inputs, params: tuple = ()):\n    a = params[0]\n    t = a.sum(\"t\")\n    return t.sum(\"loss\")\n\nrun_test_unary_elementwise(sum_test, (100, 100))\n\nMax fractional gradient difference for a: 0.0000%\n\n\n\ndef mean_test(inputs, params: tuple = ()):\n    a = params[0]\n    t = a.mean(\"t\")\n    return t.sum(\"loss\")\n\nrun_test_unary_elementwise(mean_test, (100, 100))\n\nMax fractional gradient difference for a: 0.0000%\n\n\n\ndef std_test(inputs, params: tuple = ()):\n    a = params[0]\n    t = a.std(\"t\")\n    return t.sum(\"loss\")\n\n\nrun_test_unary_elementwise(std_test, (100, 100))\n\nMax fractional gradient difference for a: 0.0049%\n\n\n\na = Tensor(np.random.randn(100, 100), name=\"a\", requires_grad=True)\n\na**3\n\n\nTensor[100, 100](name=\"\" op=Pow parents=[a]):\n    v=array[100, 100] n=10000 (78Kb) x∈[-41.412, 47.474] μ=0.066 σ=3.739\n    ∇=array[100, 100] n=10000 (78Kb) all_zeros\n\n\n\n\n# from tidygrad.functional import stack, concat\n\n\ndef stack_test(inputs, params: tuple = ()):\n    t = F.stack(params, name=\"t\")\n    return t.sum(\"loss\")\n\n\nrun_test_binary_elementwise(stack_test, (100, 100), (100, 100))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for a: 0.0000%\n\n\n\ndef concat_test(inputs, params: tuple = ()):\n    t = F.concat(params, name=\"t\")\n    return t.sum(\"loss\")\n\n\nrun_test_binary_elementwise(concat_test, (100, 100), (100, 100))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for a: 0.0000%\n\n\n\nfrom tidygrad.func import layer_norm\n\n\ndef layer_norm_test(inputs, params):\n    a, w, b = params\n    t = layer_norm(a, w, b)\n    return t.sum(\"loss\")\n\na = Tensor(np.random.randn(2, 100, 100), name=\"a\", requires_grad=True)\nw = Tensor(np.random.randn(100), name=\"w\", requires_grad=True)\nb = Tensor(np.random.randn(100), name=\"b\", requires_grad=True)\n\nt = layer_norm_test(inputs=None, params=(a, w, b))\nt.backward()\n\ngrad_check(func=layer_norm_test, inputs=None, params=(a, w, b))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for w: 0.0000%\nMax fractional gradient difference for a: 0.0074%"
  },
  {
    "objectID": "func.html",
    "href": "func.html",
    "title": "Functions that operate on tensors",
    "section": "",
    "text": "source\n\nembedding\n\n embedding (input:tidygrad.tensor.Tensor, indices, name=None)\n\n\nsource\n\n\ndropout\n\n dropout (x:tidygrad.tensor.Tensor, p=0.5, training=True)\n\nApply Dropout to a tensor\n\nsource\n\n\ntranspose\n\n transpose (a:tidygrad.tensor.Tensor, dim0, dim1, name=None)\n\nTranspose a tensor\n\nsource\n\n\nslice\n\n slice (a:tidygrad.tensor.Tensor, key, name=None)\n\n\nsource\n\n\nbroadcast\n\n broadcast (a:tidygrad.tensor.Tensor, target_shape, name=None)\n\nBroadcast a tensor to the given shape\n\nsource\n\n\nsum\n\n sum (a:tidygrad.tensor.Tensor, name=None, axis=None, keepdims=False)\n\nSum-reduce a tensor along the given axis (int or tuple of ints)\n\nsource\n\n\nmatmul\n\n matmul (a:tidygrad.tensor.Tensor, b:tidygrad.tensor.Tensor, name=None)\n\nMatrix multiplication of two tensors\n\nsource\n\n\nlogexp\n\n logexp (a:tidygrad.tensor.Tensor, name=None)\n\nExponentiate a tensor\n\nsource\n\n\nexp\n\n exp (a:tidygrad.tensor.Tensor, name=None)\n\nExponentiate a tensor\n\nsource\n\n\nlog\n\n log (a:tidygrad.tensor.Tensor, name=None)\n\nTake the natural logarithm of a tensor\n\nsource\n\n\npow\n\n pow (a:tidygrad.tensor.Tensor, power:tidygrad.tensor.Tensor, name=None)\n\nRaise a tensor to a power (a**power)\n\nsource\n\n\nneg\n\n neg (a:tidygrad.tensor.Tensor, name=None)\n\nNegate a tensor (-a)\n\nsource\n\n\ndiv\n\n div (a:tidygrad.tensor.Tensor, b:tidygrad.tensor.Tensor, name=None)\n\nDivide two tensors (a/b)\n\nsource\n\n\nmul\n\n mul (a:tidygrad.tensor.Tensor, b:tidygrad.tensor.Tensor, name=None)\n\nMultiply two tensors\n\nsource\n\n\nsub\n\n sub (a:tidygrad.tensor.Tensor, b:tidygrad.tensor.Tensor, name=None)\n\nSubtract two tensors\n\nsource\n\n\nadd\n\n add (a:tidygrad.tensor.Tensor, b:tidygrad.tensor.Tensor, name=None)\n\nAdd two tensors\n\nsource\n\n\nrelu\n\n relu (input:tidygrad.tensor.Tensor, name=None)\n\n\nsource\n\n\ntanh\n\n tanh (input:tidygrad.tensor.Tensor, name=None)\n\n\nsource\n\n\nsigmoid\n\n sigmoid (input:tidygrad.tensor.Tensor, name=None)\n\n\nsource\n\n\ngelu\n\n gelu (input:tidygrad.tensor.Tensor)\n\n\nsource\n\n\nsigmoid_gelu\n\n sigmoid_gelu (x:tidygrad.tensor.Tensor)\n\n\nsource\n\n\nsoftmax\n\n softmax (input:tidygrad.tensor.Tensor, name=None)\n\n\nsource\n\n\nlayer_norm\n\n layer_norm (x:tidygrad.tensor.Tensor, w:tidygrad.tensor.Tensor,\n             b:tidygrad.tensor.Tensor, eps=1e-05)\n\n\nsource\n\n\nconcat\n\n concat (tensors:list[tidygrad.tensor.Tensor], axis=0, name=None)\n\n\nsource\n\n\nstack\n\n stack (tensors:list[tidygrad.tensor.Tensor], axis=0, name=None)\n\n\nsource\n\n\nBCE_loss\n\n BCE_loss (logits:tidygrad.tensor.Tensor, target:tidygrad.tensor.Tensor,\n           reduction='mean')\n\n\nsource\n\n\nCrossEntropy_loss\n\n CrossEntropy_loss (logits:tidygrad.tensor.Tensor,\n                    target:tidygrad.tensor.Tensor, reduction='mean')"
  },
  {
    "objectID": "examples/gpt2_training.html",
    "href": "examples/gpt2_training.html",
    "title": "GPT2-Nano training",
    "section": "",
    "text": "import tidygrad as tg\nfrom tidygrad import Tensor\nimport tidygrad.tensor\nimport numpy as np\n\nimport huggingface_hub\n\nimport datasets\n\n\n# ds = datasets.load_dataset(\"roneneldan/TinyStories\")\n\n\nn_vocab = 1024\nn_layers = 2\nn_heads = 4\nndim = 128\nctx_len = 32\n\n\ndef gpt2_new(n_vocab, n_layers, n_heads, ndim):\n    shape_dict = {\n        \"wte\": [n_vocab, ndim],\n        \"wpe\": [ctx_len, ndim],\n        \"ln_f.weight\": [ndim],\n        \"ln_f.bias\": [ndim],\n    }\n\n    for i in range(n_layers):\n        shape_dict[f\"h.{i}.ln_1.weight\"] = [ndim]\n        shape_dict[f\"h.{i}.ln_1.bias\"] = [ndim]\n\n        shape_dict[f\"h.{i}.attn.c_attn.weight\"] = [ndim, 3 * ndim]\n        shape_dict[f\"h.{i}.attn.c_attn.bias\"] = [3 * ndim]\n\n        shape_dict[f\"h.{i}.attn.c_proj.weight\"] = [ndim, ndim]\n        shape_dict[f\"h.{i}.attn.c_proj.bias\"] = [ndim]\n\n        shape_dict[f\"h.{i}.ln_2.weight\"] = [ndim]\n        shape_dict[f\"h.{i}.ln_2.bias\"] = [ndim]\n\n        shape_dict[f\"h.{i}.mlp.c_fc.weight\"] = [ndim, 4 * ndim]\n        shape_dict[f\"h.{i}.mlp.c_fc.bias\"] = [4 * ndim]\n\n        shape_dict[f\"h.{i}.mlp.c_proj.weight\"] = [4 * ndim, ndim]\n        shape_dict[f\"h.{i}.mlp.c_proj.bias\"] = [ndim]\n\n    return tg.model.Model(shape_dict)\n\nmodel = gpt2_new(n_vocab=n_vocab, n_layers=n_layers, n_heads=n_heads, ndim=ndim)\n\nt = Tensor(123, requires_grad=False) t1 = t + t\nt1.requires_grad is False t1.parents is []\nt1.requires_grad(True)\nt1.requires_grad is True\nBut it has no parents!!!1\nt1.op should be Load, not Add\n\ndef gpt2_init(model):\n    for k in model.params.keys():\n        if k.endswith(\".weight\"):\n            model.params[k] = Tensor(np.random.randn(*model.params[k].shape), name=k) * 0.02\n        elif k.endswith(\".bias\"):\n            model.params[k] = Tensor(np.zeros(model.params[k].shape), name=k)\n\n    model.params[\"wte\"] = Tensor(np.random.randn(*model.params[\"wte\"].shape), name=\"wte\") * 0.02\n    model.params[\"wpe\"] = Tensor(np.random.randn(*model.params[\"wpe\"].shape), name=\"wpe\") * 0.01\n\ngpt2_init(model)\nmodel.requires_grad(True)\n\n\nmodel = tg.model.Model(\"model.safetensors\")\n\n\ntidygrad.tensor._num_tensors\n\n596\n\n\n\nimport tidygrad.func as F\n\n\ndef gpt2_transformer_block(model: tg.model.Model, x, n_heads, i):\n    def get_params(s):\n        return model.params[f\"h.{i}.{s}\"]\n\n    ln_1 = F.layer_norm(x, get_params(\"ln_1.weight\"), get_params(\"ln_1.bias\"))\n\n    attn_w_qkv = get_params(\"attn.c_attn.weight\")\n    attn_b_qkv = get_params(\"attn.c_attn.bias\")\n\n    attn_w_q, attn_w_k, attn_w_v = attn_w_qkv.split(3, axis=-1)\n    attn_b_q, attn_b_k, attn_b_v = attn_b_qkv.split(3, axis=-1)\n\n    q = ln_1.mmul(attn_w_q) + attn_b_q\n    k = ln_1.mmul(attn_w_k) + attn_b_k\n    v = ln_1.mmul(attn_w_v) + attn_b_v\n\n    q_chunked = F.stack(q.split(n=n_heads, axis=-1), axis=0)\n    k_chunked = F.stack(k.split(n=n_heads, axis=-1), axis=0)\n    v_chunked = F.stack(v.split(n=n_heads, axis=-1), axis=0)\n\n    dim = q_chunked.shape[-1]\n    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(dim / n_heads)\n\n    mask = np.tril(np.ones(attention.shape), k=0)\n    ee = np.exp(attention) * mask\n\n    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n\n    attention_output = softmaxed.mmul(v_chunked)\n    attention_chunks = attention_output.split(axis=0, n=n_heads)\n    # print(\"attention_chunks\", attention_chunks)\n\n    attention_reshaped = F.concat(attention_chunks, axis=-1)\n    attention_reshaped = attention_reshaped[0]\n    # print(\"attention_reshaped\", attention_reshaped)\n\n    cproj_w = get_params(\"attn.c_proj.weight\")\n    cproj_b = get_params(\"attn.c_proj.bias\")\n    # attention_reshaped = Tensor(attention_reshaped_np)\n\n    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n\n    after_residual = crosstalk + x\n    # print(\"after_residual\", after_residual)\n    ln2_w = get_params(\"ln_2.weight\")\n    ln2_b = get_params(\"ln_2.bias\")\n\n    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n\n    mlp_c_fc_w = get_params(\"mlp.c_fc.weight\")\n    mlp_c_fc_b = get_params(\"mlp.c_fc.bias\")\n\n    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n    # print(\"after_up\", after_up)\n\n    after_up_a = F.gelu(after_up)\n    # print(\"after_up_a\", after_up_a)\n\n    mlp_c_proj_w = get_params(\"mlp.c_proj.weight\")\n    mlp_c_proj_b = get_params(\"mlp.c_proj.bias\")\n\n    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n\n    output = after_down + after_residual\n    return output\n\ndef gpt2(model, input, n_layers, n_heads):\n    def get_params(s):\n        return model.params[s]\n    \n    input = np.array(input)\n\n    token_embeddings = F.embedding(get_params(\"wte\"), input)\n    position_embeddings = F.embedding(get_params(\"wpe\"), np.arange(input.shape[-1]))\n\n    x = token_embeddings + position_embeddings\n\n    # print(\"first embedding\", x)\n\n    for i in range(n_layers):\n        # print(\"layer\", i)\n        x = gpt2_transformer_block(model=model, x=x, n_heads=n_heads, i=i)\n\n    return F.layer_norm(x, w=get_params(\"ln_f.weight\"), b=get_params(\"ln_f.bias\"))\n\n\n# res = gpt2(model, np.arange(256).reshape(2, -1), n_layers=n_layers, n_heads=n_heads)\n# res.sum().backward()\n\n\n# from tidygrad.training import one_hot_encode_batch\n\n\ndef one_hot_encode(batch, n_classes):\n    batch_size, sequence_length = batch.shape\n    one_hot = np.zeros((batch_size, sequence_length, n_classes))\n    rows, cols = np.indices((batch_size, sequence_length))\n    one_hot[rows, cols, batch] = 1\n    return one_hot\n\n\ndef language_modeling_loss(model, input, target, n_layers, n_heads):\n    res = gpt2(model, input, n_layers, n_heads)\n    # print(\"res\", res)\n    # print(\"wte\", model.params[\"wte\"])\n    logits = res.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n\n    # print(\"logits\", logits)\n    loss = F.CrossEntropy_loss(logits, one_hot_encode(target, n_classes=n_vocab))\n    return loss\n\n# loss = language_modeling_loss(\n#     model, input=np.random.randint(0, n_vocab, size=(2, ctx_len)), target=np.random.randint(0, n_vocab, size=(2, ctx_len)), n_layers=n_layers, n_heads=n_heads\n# )\n\n# print(\"loss\", loss)\n\n\n# np.seterr(all=\"raise\")\n# l = loss.sum()\n# print(loss)\n\n# l.backward()\n\n\n# with open(\"datasets/TinyStories/TinyStories.txt\", \"r\") as file:\n#     tokens = file.read()\n\n\n# Dataset:\n\n# dataset = [\"Lilly gsdsgfsdfsd sf sfds\"] &lt;- You can no sample from ths\n\n# dataset = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15.....]\n\n# ctx len = 5\n\n# dataset[0] = [1,2,3,4,5]\n# dataset[1] = [2,3,4,5,6]\n# dataset[2] = [3,4,5,6,7]\n# dataset[3] = [4,5,6,7,8]\n\nfrom tidygrad.utils.datasets import Dataset, DataLoader\n\ntokens = np.load(\"./datasets/TinyStories/TinyStories_1percent_ids.npy\")\n\nclass TSDataset(Dataset):\n    def __init__(self, token_array, ctx_len):\n        self.token_array = token_array\n        self.ctx_len = ctx_len\n\n    def __len__(self):\n        return len(self.token_array) - self.ctx_len - 1\n\n    def __getitem__(self, i):\n        return self.token_array[i:i + self.ctx_len], self.token_array[i + 1:i + self.ctx_len + 1]\n\n    def collate_fn(self, batch):\n        # print(\"batch\", batch) # [(x1, y1), (x2, y2), (x3, y3)]\n        return np.stack([x for x, y in batch]), np.stack([y for x, y in batch])\n\ndataset = TSDataset(tokens, 2)\n\n\nimport math\n\n\nclass TSDataLoader(DataLoader):\n    def __init__(self, dataset, batch_size, batch_tfms=None, ctx_len=ctx_len, fake_epoch_len=256, seed=1337):\n        super().__init__(dataset=dataset, batch_size=batch_size, batch_tfms=batch_tfms)\n        self.fake_epoch_len = fake_epoch_len\n        self.ctx_len = ctx_len\n        self.rng = np.random.default_rng(seed)\n\n    def __len__(self):\n        return min((len(self.dataset) // self.batch_size) // self.ctx_len, self.fake_epoch_len)\n\n    def __iter__(self):\n        self.i = 0\n        return self\n\n    def __next__(self):\n        if self.i &gt;= min(len(self), self.fake_epoch_len):\n            raise StopIteration\n\n        idxs = self.rng.integers(0, len(self.dataset), size=(self.batch_size, ))\n\n        batch = [self.dataset[i] for i in idxs]\n        batch = self.dataset.collate_fn(batch)\n\n        self.i += 1\n\n        return batch\n\ndataloader = TSDataLoader(dataset, batch_size=128)\n\n\nfrom tidygrad.utils.data import DataLoaders\n\n\nX, y = next(iter(dataloader))\n\nprint(\"X\", X.shape)\nprint(\"y\", y.shape)\n\nX (128, 2)\ny (128, 2)\n\n\n\nfrom tidygrad.training import Learner\n\nfrom tidygrad.optim import Adam\nfrom functools import partial\n\n\nimport tidygrad.tensor\n\n\ndef loss_function(X, y):\n    # y = Tensor(y)\n    logits = X.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n\n    # print(\"X\", X)\n    # print(\"y\", y)\n    # print(\"logits\", logits)\n\n    one_one_hot = one_hot_encode(y, n_vocab)\n\n    loss = F.CrossEntropy_loss(logits, one_one_hot, reduction=\"sum\")\n\n    print(\"loss\", loss)\n    loss = loss.mean()\n\n    print(\"post_epoch num tensors\", tidygrad.tensor._num_tensors)\n\n    return loss\n\n\nfrom tidygrad.training import DictLoggerCallback, ProgressBarCallback, Loss\n\n\nclass OneBatchCallback:\n    def __init__(self):\n        self.i = 0\n\n    def post_loss(self, learner):\n        print(\"post_batch_backward\", self.i)\n        if self.i == 1:\n            raise Exception(\"post_batch_backward\")\n        self.i += 1\n\nclass MemleakCallback:\n    def __init__(self):\n        self.i = 0\n        print(\"init\")\n\n    def post_epoch(self, learner):\n        print(\"post_epoch num tensors\", tidygrad.tensor._num_tensors)\n\n\nmodel_funct = partial(gpt2, n_layers=n_layers, n_heads=n_heads)\n\ndef model_funct(input):\n    return gpt2(model, input, n_layers=n_layers, n_heads=n_heads)\n\noptim = Adam(lr=0.001, params=model.parameter_list())\n\nler = Learner(\n    model=model_funct,\n    dataloaders=DataLoaders(train=dataloader, test=dataloader),\n    loss_func=loss_function,\n    optimizer=optim,\n    callbacks=[DictLoggerCallback(metrics=[Loss()]),\n               ProgressBarCallback(metrics=[\n                   \"loss\", \n               ], plot_train_skip_ylim=15, plot_smooth_training=5),\n               MemleakCallback()],\n)\n\ninit\n\n\n\n# ler.fit(epochs=50)\n\n\n# import json\n\n# json.dump(ler.history, open(\"history.json\", \"w\"), indent=2)\n\n\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nTS_PATH = Path(\"./datasets/TinyStories/\")\n\nfrom tokenizers import Tokenizer\ntokenizer = Tokenizer.from_file(str(TS_PATH / \"wordpiece_1024.json\"))\n\n\ndef gpt2_language_modeling(model, input, n_layers, n_heads, temperature=0):\n    res = gpt2(model, input, n_layers, n_heads)\n\n    last_position = res[:, -1, :]\n\n    # print(\"wte\", model.params[\"wte\"])\n    logits = last_position.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n    return logits, logits.data.argmax(axis=-1)\n\n\ngpt2_language_modeling(model, [[1,2,3,5]], n_layers=n_layers, n_heads=n_heads)\n\n(Tensor[1, 1024](name=\"\" op=Load):\n     v=array[1, 1024] f32 4Kb x∈[-12.651, 7.492] μ=-3.202 σ=3.977\n     ,\n array([16]))\n\n\n\ntext = \"Once\"\n# text = \"&lt;|endoftext|&gt;\"\ntokens = tokenizer.encode(text).ids  # returns a list of integers\nprint(tokens)\nprint(\"=== Generating ===\")\nprint(\"Input: \", tokenizer.decode(tokens))\n\nwith tidygrad.no_grad():\n    for i in tqdm(range(30)):\n        logits, res = gpt2_language_modeling(model, [tokens], n_layers=n_layers, n_heads=n_heads)\n        tokens.append(int(logits.data.argmax(axis=-1)))\n        del logits, res\n        # gc.collect()\n        # print(tokens)\n        print(\"Output:\", tokenizer.decode(tokens))\n\n[302]\n=== Generating ===\nInput:  Once\nOutput: Once upon\nOutput: Once upon a\nOutput: Once upon a time\nOutput: Once upon a time ,\nOutput: Once upon a time , a\nOutput: Once upon a time , a time\nOutput: Once upon a time , a time ,\nOutput: Once upon a time , a time , a\nOutput: Once upon a time , a time , a time\nOutput: Once upon a time , a time , a time ,\nOutput: Once upon a time , a time , a time , there\nOutput: Once upon a time , a time , a time , there was\nOutput: Once upon a time , a time , a time , there was a\nOutput: Once upon a time , a time , a time , there was a little\nOutput: Once upon a time , a time , a time , there was a little girl\nOutput: Once upon a time , a time , a time , there was a little girl named\nOutput: Once upon a time , a time , a time , there was a little girl named Lily\nOutput: Once upon a time , a time , a time , there was a little girl named Lily .\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily .\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She was\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She was very\nOutput: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She was very happy\n\n\n\n\n\n/tmp/ipykernel_625194/3847700028.py:11: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  tokens.append(int(logits.data.argmax(axis=-1)))"
  },
  {
    "objectID": "examples/gpt2_v2.html",
    "href": "examples/gpt2_v2.html",
    "title": "GPT2 inference",
    "section": "",
    "text": "import tidygrad\nfrom tidygrad.tensor import Tensor\n\nimport tidygrad.func as F\n# from tidygrad.func import  embedding, layer_norm, stack, concat\nimport numpy as np\nfrom lovely_numpy import Lo\n\nfrom transformers import GPT2Tokenizer\n\n\nfrom safetensors import safe_open\n\n\n# !wget -c https://huggingface.co/gpt2/resolve/main/model.safetensors        -O ./downloaded_weights/gpt2.safetensors\n# !wget -c https://huggingface.co/gpt2-medium/resolve/main/model.safetensors -O ./downloaded_weights/gpt2-medium.safetensors\n# !wget -c https://huggingface.co/gpt2-large/resolve/main/model.safetensors  -O ./downloaded_weights/gpt2-large.safetensors\n# !wget -c https://huggingface.co/gpt2-xl/resolve/main/model.safetensors     -O ./downloaded_weights/gpt2-xl.safetensors\n\n\nclass Gpt2Variant:\n    def __init__(self, weight_file, n_head, n_layer):\n        self.weight_file = weight_file\n        self.n_head = n_head\n        self.n_layer = n_layer\n\ngpt2_variants = {\n    \"gpt2\": Gpt2Variant(\"gpt2.safetensors\", 12, 12),\n    \"gpt2-medium\": Gpt2Variant(\"gpt2-medium.safetensors\", 16, 24),\n    \"gpt2-large\": Gpt2Variant(\"gpt2-large.safetensors\", 20, 36),\n    \"gpt2-xl\": Gpt2Variant(\"gpt2-xl.safetensors\", 25, 48),\n}\n\ngpt2_variant = \"gpt2\"\nweights_dir = \"./downloaded_weights/\"\n\ntext = \"In a hole in the ground there lived a\"\ntokenizer = GPT2Tokenizer.from_pretrained(gpt2_variant)\n\n# tokens = tokenizer.encode(text)  # returns a list of integers\n# tokens = Tensor(tokens)\n\ntokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nmodel = safe_open(weights_dir + gpt2_variants[gpt2_variant].weight_file, framework=\"np\")\n\n\nTensor(model.get_tensor(\"wte.weight\"))\n\nTensor[50257, 768](name=\"\" op=Load):\n    v=array[50257, 768] f32 n=38597376 (0.1Gb) x∈[-1.270, 1.785] μ=0.000 σ=0.144\n    \n\n\n\nimport safetensors\n\n\nimport tidygrad.func as F\n\nnn.Module capabilities:\n\nAbstract neural network “modules”, like Linear of Conv2D.\nAssignment tracks parameters\n\nclass MyModel(Module): def init(): self.w1 = Tensort(…) self.b2 = Tens….\n# w1, b1 are tracked as parameters\nThen you can call model.parameters() to get a list of parameters.\n\nSave / load weights. Also, count weights.\nFun forward/backward pass on the model.\n\n\nPytorch\nclass nn.Linear(): ….\nclass Model(nn.Module): init: self.l1 = nn.Linear(…) self.ln = …\nforward(x):\n    x = self.l1(x) \n    x = self.conv(x)\n    ....\n    return x\nmodel = Model(…)\ny = model(x)\n\n\nTidyGrad\ny = x.mmul(w) + b\nclass ModelTensors(Dict): init\nload(st: safetensor):\n    for k in st.keys():\n        self.params[k] = st.get_tensor(k)\n\nsave():\n    .....\n    return st\nmodel = ModelTensors\na = model[“h0.ln1.w”] # Returns Tensor a = models.h0.ln1.w\nmodel.parameters() ==&gt; Return list of params\noptim = SGD(model.params(), lr=9000)\ndef transformer()…\nloss = transformer(X, y, model) loss.backwards()\noptim.step()\n\ndef transformer_block(model, i, input, n_head):\n    dim = input.shape[-1]\n    assert dim % n_head == 0\n\n    ln_1_w = model.get_tensor(f\"h.{i}.ln_1.weight\")\n    ln_1_b = model.get_tensor(f\"h.{i}.ln_1.bias\")\n\n    ln_1 = F.layer_norm(input, ln_1_w, ln_1_b)\n    # ln_1.ad\n\n    attn_w_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.weight\")\n    attn_b_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.bias\")\n\n    attn_w_q, attn_w_k, attn_w_v = np.split(attn_w_qkv, 3, axis=-1)\n    attn_b_q, attn_b_k, attn_b_v = np.split(attn_b_qkv, 3, axis=-1)\n\n    q = ln_1.mmul(attn_w_q) + attn_b_q\n    k = ln_1.mmul(attn_w_k) + attn_b_k\n    v = ln_1.mmul(attn_w_v) + attn_b_v\n\n    # q_chunked = split_tensor(q, axis=-1, n=12)\n    # k_chunked = split_tensor(k, axis=-1, n=12)\n    # v_chunked = split_tensor(v, axis=-1, n=12)\n\n    q_chunked = F.stack(q.split(n=n_head, axis=-1), axis=0)\n    k_chunked = F.stack(k.split(n=n_head, axis=-1), axis=0)\n    v_chunked = F.stack(v.split(n=n_head, axis=-1), axis=0)\n\n    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(dim / n_head)\n\n    mask = np.tril(np.ones(attention.shape), k=0)\n    ee = np.exp(attention) * mask\n\n    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n\n    attention_output = softmaxed.mmul(v_chunked)\n    attention_chunks = attention_output.split(axis=0, n=n_head)\n    # print(\"attention_chunks\", attention_chunks)\n\n    attention_reshaped = F.concat(attention_chunks, axis=-1)\n    attention_reshaped = attention_reshaped[0]\n    # print(\"attention_reshaped\", attention_reshaped)\n\n    cproj_w = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.weight\"))\n    cproj_b = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.bias\"))\n    # attention_reshaped = Tensor(attention_reshaped_np)\n\n    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n\n    after_residual = crosstalk + input\n    # print(\"after_residual\", after_residual)\n\n    ln2_w = Tensor(model.get_tensor(f\"h.{i}.ln_2.weight\"), name=\"ln2_w\")\n    ln2_b = Tensor(model.get_tensor(f\"h.{i}.ln_2.bias\"), name=\"ln2_b\")\n\n    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n\n    mlp_c_fc_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.weight\"), name=\"fc_w\")\n    mlp_c_fc_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.bias\"), name=\"fc_b\")\n\n    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n    # print(\"after_up\", after_up)\n\n    after_up_a = F.gelu(after_up)\n    # print(\"after_up_a\", after_up_a)\n\n    mlp_c_proj_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.weight\"), name=\"proj_w\")\n    mlp_c_proj_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.bias\"), name=\"proj_b\")\n\n    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n\n    output = after_down + after_residual\n    return output\n\n# res = transformer_block(model, 0, embeddings)\n\n\ndef transformer(model, tokens, n_layer, n_head):\n    wte = Tensor(model.get_tensor(\"wte.weight\"))\n    wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n\n    token_embeddings = F.embedding(wte, tokens)\n\n    positions = np.arange(len(tokens))\n    position_embeddings = F.embedding(wpe, positions)\n\n    embeddings = token_embeddings + position_embeddings\n\n    for i in range(n_layer):\n        # print(\"Layer\", i)\n        embeddings = transformer_block(model, i, embeddings, n_head)\n        # print(\"Embedding out:\", embeddings)\n        # print(tidygrad.tensor._num_tensors)\n        # print(tidygrad.tensor._num_ops)\n\n    ln_f_w = Tensor(model.get_tensor(\"ln_f.weight\"))\n    ln_f_b = Tensor(model.get_tensor(\"ln_f.bias\"))\n\n    res = F.layer_norm(embeddings, ln_f_w, ln_f_b)\n\n    return res\n\n# tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n# with tidygrad.no_grad():\n#     res = transformer(model, tokens, gpt2_variants[gpt2_variant].n_layer, gpt2_variants[gpt2_variant].n_head)\n#     print(res)\n\n# import gc\n# del res\n\n# gc.collect()\n\n\nmodel.get_tensor(\"wte.weight\").shape\n\n(50257, 768)\n\n\n\ntext = \"In a hole in the ground there lived a\"\ntokenizer = GPT2Tokenizer.from_pretrained(gpt2_variant)\n\ntokens = tokenizer.encode(text)  # returns a list of integers\nprint(tokens)\n# tokens = list(range(1000))\n\ndef gpt2_language_model(model, token_ids, n_layer, n_head):\n    wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))\n    res = transformer(model, token_ids, n_layer, n_head)\n\n    res = res[-1, :]\n    logits = res.mmul(wte)\n    return logits, res\n\nwith tidygrad.no_grad():\n    logits, res = gpt2_language_model(model, tokens, n_layer=gpt2_variants[gpt2_variant].n_layer, n_head=gpt2_variants[gpt2_variant].n_head)\n    print(res)\ntokenizer.decode(logits.data.argmax(axis=-1))\n\n[818, 257, 7604, 287, 262, 2323, 612, 5615, 257]\nTensor[768](name=\"\" op=Load):\n    v=array[768] f32 3Kb x∈[-50.634, 188.688] μ=0.388 σ=8.312\n    \n\n\n/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/activation.py:33: RuntimeWarning: underflow encountered in exp\n  self.set_out(1 / (1 + np.exp(-self.args[0].data)))\n/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/activation.py:33: RuntimeWarning: overflow encountered in exp\n  self.set_out(1 / (1 + np.exp(-self.args[0].data)))\n\n\n' man'\n\n\n\nres.data.dtype\n\ndtype('float32')\n\n\n\n# import gc\n# del logits, res\n# gc.collect()\n\n\nfrom tqdm.auto import tqdm\n\n\nTensor(np.random.randn(5,5)).data.dtype\n\ndtype('float32')\n\n\n\na = np.random.randn(5, 5).astype(np.float32)\nb = np.random.randn(5, 5).astype(np.float32)\n\n(a+b).dtype\n\ndtype('float32')\n\n\n\na = np.zeros((1000_000, 1000))\n\n\nprint(tokenizer.special_tokens_map)\n\ntext = \"&lt;|endoftext|&gt;\"\ntokens = tokenizer.encode(text)  # returns a list of integers\n\n{'bos_token': '&lt;|endoftext|&gt;', 'eos_token': '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;'}\n\n\n\n\n\n[50256]\n\n\n\ntext = \"&lt;|endoftext|&gt; In a hole in the ground there lived a\"\n# text = \"&lt;|endoftext|&gt;\"\ntokens = tokenizer.encode(text)  # returns a list of integers\n\n\nprint(\"=== Generating ===\")\nprint(\"Input: \", tokenizer.decode(tokens))\n\nwith tidygrad.no_grad():\n    for i in tqdm(range(100)):\n        logits, res = gpt2_language_model(model, tokens, n_layer=gpt2_variants[gpt2_variant].n_layer, n_head=gpt2_variants[gpt2_variant].n_head)\n        tokens.append(logits.data.argmax(axis=-1))\n        del logits, res\n        # gc.collect()\n        print(\"Output:\", tokenizer.decode(tokens))\n\n=== Generating ===\nInput:  &lt;|endoftext|&gt; In a hole in the ground there lived a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet.\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage.\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage.\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage.\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage.\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage.\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\nOutput: &lt;|endoftext|&gt; In a hole in the ground there lived a man who had been killed by a bullet. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of great courage and courage. He was a man of!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\n\n\n\n\n/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/common.py:263: RuntimeWarning: underflow encountered in exp\n  self.set_out(np.exp(self.args[0].data))\n/tmp/ipykernel_622983/3569323853.py:32: RuntimeWarning: underflow encountered in exp\n  ee = np.exp(attention) * mask\n/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/common.py:263: RuntimeWarning: overflow encountered in exp\n  self.set_out(np.exp(self.args[0].data))\n/tmp/ipykernel_622983/3569323853.py:32: RuntimeWarning: overflow encountered in exp\n  ee = np.exp(attention) * mask\n/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/common.py:185: RuntimeWarning: invalid value encountered in multiply\n  self.set_out(self.args[0].data * self.args[1].data)\n/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/activation.py:33: RuntimeWarning: underflow encountered in divide\n  self.set_out(1 / (1 + np.exp(-self.args[0].data)))"
  },
  {
    "objectID": "10_utils.grad_check.html",
    "href": "10_utils.grad_check.html",
    "title": "Tidy Utils",
    "section": "",
    "text": "source\n\ngrad_check\n\n grad_check (func, inputs, params:tuple=(), eps=1e-05, n=1000,\n             verbose=False)\n\n\nfrom lovely_numpy import Lo\nfrom tidygrad import Tensor\n\n\nLo((np.random.randn(32, 28 * 28) @ (np.random.randn(28 * 28, 100) * 0.1) + np.random.randn(100)) @ (np.random.randn(100, 10) * 0.1))\n\narray[32, 10] n=320 (2.5Kb) x∈[-7.871, 6.829] μ=-0.001 σ=2.903\n\n\n\ni = 0\n\n\ni += 1\nnp.random.seed(i)\n\nx = Tensor(np.random.randn(32, 28 * 28), \"X\")\n# Create a 1-hot encoded tensor with 1 random 1\ny = np.zeros((32, 10))\ny[np.arange(32), np.random.choice(10, 32)] = 1\ny = Tensor(y, \"y\")\n\nw1 = Tensor(np.random.randn(28 * 28, 100) * 0.1, \"w1\", requires_grad=True)\nb1 = Tensor(np.random.randn(100), \"b1\", requires_grad=True)\nw2 = Tensor(np.random.randn(100, 10) * 0.1, \"w2\", requires_grad=True)\n\ndef NN(inputs, params: tuple):\n    x, y = inputs\n    w1, b1, w2 = params\n    z1 = x.mmul(w1, \"tmp\").add(b1, \"z1\")\n    a1 = tg.sigmoid(z1)\n    z2 = a1.mmul(w2)\n\n    loss = -tg.BCE_loss(z2, y).sum(\"loss\")\n\n    return loss\n\ndebug = []\nloss = NN(inputs=(x, y), params=(w1, b1, w2))\n\nloss.backward()\n\n# grad_check(NN, (x, y), (w1, b1, w2))\n\nMax fractional gradient difference for w2: 0.0011%\nMax fractional gradient difference for b1: 0.0010%\nMax fractional gradient difference for w1: 0.0159%"
  },
  {
    "objectID": "ops.activation.html",
    "href": "ops.activation.html",
    "title": "Functional operations: Activation functions",
    "section": "",
    "text": "source\n\nRelu\n\n Relu (a, name=None)\n\nTake the sigmoid of a tensor\n\nsource\n\n\nSigmoid\n\n Sigmoid (a, name=None)\n\nTake the sigmoid of a tensor\n\n## | export\n\n# XXX This is numerically unstable. Fix it.\n\nclass Tanh(UnaryElementwiseOp):\n    \"\"\"Take the tanh of a tensor\"\"\"\n\n    name_template = \"tanh({})\"\n\n    def __init__(self, a, name=None):\n        assert 1, \"XXX Fix me first\"\n        super().__init__(a, name=name)\n        ex = np.exp(self.args[0].data)\n        emx = np.exp(-self.args[0].data)\n        self.set_out((ex-emx) / (ex+emx))\n\n    def backward(self):\n        self.check_backward()\n        with np.errstate(under=\"ignore\"):  # Triggered by infinitesimally small 1-data\n            self.parents[0].accum_grad(self.out.grad * (1 - self.out.data**2))"
  },
  {
    "objectID": "10_utils.data.html",
    "href": "10_utils.data.html",
    "title": "tidygrad",
    "section": "",
    "text": "source\n\ndownload_file\n\n download_file (path, url)\n\n\nsource\n\n\nDataLoaders\n\n DataLoaders (train:__main__.DataLoader, test:__main__.DataLoader)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nDataLoader\n\n DataLoader (dataset, batch_size=64, shuffle=True, batch_tfms=())\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nDataset\n\n Dataset ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "11_utils.datasets.html",
    "href": "11_utils.datasets.html",
    "title": "tidygrad",
    "section": "",
    "text": "source\n\nMNIST\n\n MNIST (path='datasets', url=None, train=True, item_tfms=(),\n        fashion=False)\n\nMNIST Dataset\n\nsource\n\n\nload_fashion_mnist\n\n load_fashion_mnist (path, url_base)\n\n\nsource\n\n\nload_mnist\n\n load_mnist (path, url)\n\n\nsource\n\n\nmnist_batch_tfm\n\n mnist_batch_tfm (x, y)\n\n\nfrom matplotlib import pyplot as plt\nfrom lovely_numpy import Lo\n\n\nmnist = MNIST()\nplt.imshow(mnist.x[0])\nLo(mnist.x), Lo(mnist.y), Lo(mnist.x[0])\n\nFile datasets/mnist.npz already exists\n\n\n(array[60000, 28, 28] f32 n=47040000 (0.2Gb) x∈[0., 255.000] μ=33.318 σ=78.567,\n array[60000] i32 0.2Mb x∈[0, 9] μ=4.454 σ=2.889,\n array[28, 28] f32 n=784 (3.1Kb) x∈[0., 255.000] μ=35.108 σ=79.649)\n\n\n\n\n\n\nmnist_dl = DataLoader(mnist, batch_size=64, batch_tfms=[mnist_batch_tfm])\nx, y = next(iter(mnist_dl))\nx, y\n\n\n(Tensor[64, 784](name=\"?\" op=Load):\n     v=array[64, 784] f32 n=50176 (0.2Mb) x∈[-0.500, 0.500] μ=-0.368 σ=0.309\n     ∇=array[64, 784] f32 n=50176 (0.2Mb) all_zeros,\n Tensor[64](name=\"?\" op=Load):\n     v=array[64] i32 x∈[0, 9] μ=4.719 σ=3.223\n     ∇=array[64] f32 all_zeros)\n\n\n\n\nmnist = MNIST(fashion=True)\nplt.imshow(mnist.x[0])\n\nDownloading https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz to datasets/train-images-idx3-ubyte.gz\nDownloading https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz to datasets/train-labels-idx1-ubyte.gz\nDownloading https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz to datasets/t10k-images-idx3-ubyte.gz\nDownloading https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz to datasets/t10k-labels-idx1-ubyte.gz\n\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\n\nLo(mnist.x), Lo(mnist.y), Lo(mnist.x[0])\n\n(array[60000, 28, 28] u8 n=47040000 (45Mb) x∈[0, 255] μ=72.940 σ=90.021,\n array[60000] u8 59Kb x∈[0, 9] μ=4.500 σ=2.872,\n array[28, 28] u8 n=784 x∈[0, 255] μ=97.254 σ=101.792)"
  },
  {
    "objectID": "ops.common.html",
    "href": "ops.common.html",
    "title": "Operations: Common",
    "section": "",
    "text": "from fastcore.test import test_eq, test_fail\n\ntest_eq(calculate_target_shape((1, 2, 3), (2, 3)), (1, 2, 3))\ntest_eq(calculate_target_shape((1, 2, 3), (2, 1)), (1, 2, 3))\ntest_eq(calculate_target_shape((1, 2, 3), (1, 3)), (1, 2, 3))\ntest_eq(calculate_target_shape((1, 2, 3), (1, 1)), (1, 2, 3))\n\ntest_eq(calculate_target_shape((1, 5), (3, 1)), (3, 5))\n\ntest_fail(calculate_target_shape, args=((1, 2, 3), (2, 2)), contains=\"Cannot broadcast\")\n\n\nsource\n\nUnaryElementwiseOp\n\n UnaryElementwiseOp (a, name=None)\n\nBase class for unary elementwise operations\n\nsource\n\n\nBinaryElementwiseOp\n\n BinaryElementwiseOp (a, b, name=None)\n\nBase class for binary elementwise operations\n\nsource\n\n\nBaseOp\n\n BaseOp (*args, name:str=None)\n\nBase class for all operations\n\nsource\n\n\nLoad\n\n Load (name=None)\n\nLoad a tensor\n\nsource\n\n\nAdd\n\n Add (a, b, name=None)\n\nAdd two tensors\n\nsource\n\n\nSub\n\n Sub (a, b, name=None)\n\nSubtract two tensors\n\nsource\n\n\nMul\n\n Mul (a, b, name=None)\n\nMultiply two tensors\n\nsource\n\n\nDiv\n\n Div (a, b, name=None)\n\nDivide two tensors\n\nsource\n\n\nNeg\n\n Neg (a, name=None)\n\nNegate a tensor\n\nsource\n\n\nPow\n\n Pow (a, power, name=None)\n\nRaise a tensor to a power\n\nsource\n\n\nLog\n\n Log (a, name=None)\n\nTake the natural logarithm of a tensor\n\nsource\n\n\nExp\n\n Exp (a, name=None)\n\nExponentiate a tensor\n\nsource\n\n\nExpLog\n\n ExpLog (a, name=None)\n\nExponentiate a tensor\n\nsource\n\n\nMatmul\n\n Matmul (a, b, name=None)\n\nMatrix multiplication of two tensors\n\nsource\n\n\nfast_mmul\n\n fast_mmul (a, b)\n\nFast matrix multiplication for 2D tensors\n\nsource\n\n\nSum\n\n Sum (a, axis=None, keepdims=False, name=None)\n\nSum-reduce a tensor along the given axis (int or tuple of ints)\n\nsource\n\n\nBroadcast\n\n Broadcast (a, target_shape, name=None)\n\nBroadcast a tensor to the given shape\n\nsource\n\n\nSlice\n\n Slice (a, key, name=None)\n\nBase class for unary elementwise operations\n\n# class LessThan(BinaryElementwiseOp):\n#     name_template = \"({}&lt;{})\"\n\n#     def __init__(self, a, b, name=None):\n#         super().__init__(a, b, name=name)\n#         self.out = Tensor(\n#             data=self.args[0].data &lt; self.args[1].data, name=self.name, op=self\n#         )\n\n#     # def backward(self):\n#     #     self.parents[0].accum_grad(self.out.grad * (self.parents[0].data &lt; self.parents[1].data)\n#     #     self.parents[1].accum_grad(self.out.grad * (self.parents[0].data &gt;= self.parents[1].data)\n\n# class Where(BaseOp):\n#     name_template = \"where({})\"\n\n#     def __init__(self, a, b, c, name=None):\n#         super().__init__(a, b, c, name=name)\n#         self.parents = self.args\n#         self.out = Tensor(\n#             data=np.where(self.args[0].data, self.args[1].data, self.args[2].data),\n#             name=self.name,\n#             op=self,\n#         )\n\n#     def backward(self):\n#         # self.parents[0].accum_grad(self.out.grad * self.parents[1].data\n#         # self.parents[0].accum_grad(self.out.grad * self.parents[2].data\n\n#         self.parents[1].accum_grad(self.out.grad * self.parents[0].data\n#         self.parents[2].accum_grad(self.out.grad * (1 - self.parents[0].data)\n\n\nsource\n\n\nTranspose\n\n Transpose (a, dim0, dim1, name=None)\n\nTranspose a tensor\n\nsource\n\n\nDropout\n\n Dropout (a, p_drop=0.1, training=True, name=None)\n\nApply Dropout to a tensor\n\nsource\n\n\nEmbedding\n\n Embedding (a, indices, name=None)\n\nEmbedding layer"
  },
  {
    "objectID": "06_training.html",
    "href": "06_training.html",
    "title": "Training loop stuff",
    "section": "",
    "text": "source\n\nadd_callbacks\n\n add_callbacks (func)\n\n\nsource\n\n\nLoss\n\n Loss (train=True, valid=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nMultiClassAccuracy\n\n MultiClassAccuracy (train=True, valid=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nMetric\n\n Metric (train=True, valid=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nDictLoggerCallback\n\n DictLoggerCallback (metrics=None, history=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nLearner\n\n Learner (dataloaders, model, loss_func, optimizer, callbacks=[])\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\none_hot_encode_batch\n\n one_hot_encode_batch (y, n_classes)\n\n\nsource\n\n\nmetrics_last_pretty\n\n metrics_last_pretty (metrics, metrics_dict)\n\nReturn an str with the last values of the metrics, extended to match the length of the metric names, min 10 characters\n\nsource\n\n\nmetrics_names_pretty\n\n metrics_names_pretty (metrics)\n\nReturn an str with the names of the metrics, extended to at least 8 characters\n\nsource\n\n\nprint_metrics\n\n print_metrics (learner, metrics)\n\n\nsource\n\n\nprint_metrics_header\n\n print_metrics_header (metrics)\n\n\nsource\n\n\nProgressBarCallback\n\n ProgressBarCallback (metrics=['loss'], plot=True,\n                      plot_train_skip_ylim=10, plot_smooth_training=0)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmetrics\nlist\n[‘loss’]\nmetrics to display, must be in the `learner.history`` dict\n\n\nplot\nbool\nTrue\nplot the metrics\n\n\nplot_train_skip_ylim\nint\n10\nskip the first N training metrics when calculating the ylim\n\n\nplot_smooth_training\nint\n0\nsmooth the training metrics with a moving average of N\n\n\nReturns\nNone\n\n\n\n\n\n\nfrom tidygrad.utils.data import DataLoader, DataLoaders\nfrom tidygrad.utils.datasets import MNIST, mnist_batch_tfm\nfrom tidygrad.func import sigmoid, relu, BCE_loss, CrossEntropy_loss\nfrom tidygrad.optim import Adam\nfrom functools import partial\n\n\nBS = 1024\n\nmnist_train = DataLoader(MNIST(fashion=True), batch_size=BS, shuffle=True, batch_tfms=[mnist_batch_tfm])\nmnist_test = DataLoader(MNIST(train=False, fashion=True), batch_size=BS, shuffle=False, batch_tfms=[mnist_batch_tfm])\nINT_DIM = 128\n\nw1 = Tensor(np.random.randn(784, INT_DIM) * 0.1, \"w1\", requires_grad=True)\nb1 = Tensor(np.ones((1, INT_DIM)) * 0.1, \"b1\", requires_grad=True)\nw2 = Tensor(np.random.randn(INT_DIM, 10) * 0.1, \"w2\", requires_grad=True)\n# b2 = Tensor(np.zeros((1, 10)), \"b2\")\n\ndef linear_model(inputs, params, debug=list()):\n    inputs.data = inputs.data.reshape(inputs.data.shape[0], -1)\n    x = inputs\n    w1, b1, w2 = params\n    # print(\"model inputs\", x)\n\n    z1 = relu(x.mmul(w1, \"z1\") + b1)\n    z2 = z1.mmul(w2, \"z2\")\n\n    # print(\"model outputs\", z2)\n\n    return z2\n\nMM_func = partial(linear_model, params=[w1, b1, w2])\noptimizer = Adam([w1, b1, w2], lr=0.005)\n\nloss_f = lambda preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets.data, n_classes=10)).mean()\n# loss_f = lambda preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets.data, 10))\n\nstudent = Learner(\n    dataloaders=DataLoaders(mnist_train, mnist_test),\n    model=MM_func,\n    loss_func=loss_f,\n    optimizer=optimizer,\n    callbacks=[DictLoggerCallback(), ProgressBarCallback(metrics=[\n        \"loss\",\n        \"accuracy\",\n    ], plot_train_skip_ylim=15, plot_smooth_training=5)],\n)\n\n\nstudent.fit(epochs=5)\n\n\n\n\n\n\n\n\n\n\nEp  | loss       accuracy   | val_loss   val_accuracy\n0   | 0.044432   0.844727   | 0.047987   0.829536    \n1   | 0.041537   0.853516   | 0.044185   0.839193    \n2   | 0.032763   0.871094   | 0.041888   0.851020    \n3   | 0.035643   0.875000   | 0.038516   0.860352    \n4   | 0.031717   0.891602   | 0.037972   0.864475    \n\n\n\n/home/xl0/work/projects/grads/tidygrad/tidygrad/optim.py:54: RuntimeWarning: underflow encountered in square\n  self.vs[i] = self.beta2 * self.vs[i] + (1 - self.beta2) * p.grad**2\n/home/xl0/work/projects/grads/tidygrad/tidygrad/optim.py:54: RuntimeWarning: underflow encountered in multiply\n  self.vs[i] = self.beta2 * self.vs[i] + (1 - self.beta2) * p.grad**2"
  },
  {
    "objectID": "03_optim.html",
    "href": "03_optim.html",
    "title": "Optimizers",
    "section": "",
    "text": "source\n\nOptimizer\n\n Optimizer (params)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSGD\n\n SGD (params, lr=0.1, mom=0)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nAdam\n\n Adam (params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-08)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "ops.shape.html",
    "href": "ops.shape.html",
    "title": "Operations: Reshaping",
    "section": "",
    "text": "source\n\nStack\n\n Stack (tensors, axis=0, name=None)\n\nStack tensors along a new axis\n\nsource\n\n\nConcat\n\n Concat (tensors, axis=0, name=None)\n\nConcat tensors along an existing axis"
  },
  {
    "objectID": "04_model.html",
    "href": "04_model.html",
    "title": "tidygrad",
    "section": "",
    "text": "source\n\nModel\n\n Model (params:Union[dict[str,tuple],str,os.PathLike])\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "examples/tinystories_prepare.html",
    "href": "examples/tinystories_prepare.html",
    "title": "TinyStories dataset pre-processing.",
    "section": "",
    "text": "# Download and extract the TinyStories dataset\n\n# !wget -c https://huggingface.co/datasets/roneneldan/TinyStories/raw/main/TinyStories_all_data.tar.gz -O datasets/TinyStories/TinyStories_all_data.tar.gz\n# !cd datasets/TinyStories && tar -xvf TinyStories_all_data.tar.gz && cd ../..\n\n\nfrom tqdm import tqdm\n\nimport json\nimport os\nfrom pathlib import Path\nimport unidecode\n\n\nTS_PATH = Path(\"datasets/TinyStories/\")\n\n\nstories = []\n\nfor file in tqdm(list(sorted(os.listdir(TS_PATH)))):\n    if file.endswith(\".json\"):\n        with open(TS_PATH / file, \"r\") as f:\n            data = json.load(f)\n            for d in data:\n                story = d[\"story\"]\n                if not all(ord(c) &lt; 128 for c in story):\n                    story = unidecode.unidecode(story)\n\n                stories.append(story)\n\n                # if d[\"source\"] == \"GPT-3.5\":\n                #     gpt35_stories.append(story)\n                # elif d[\"source\"] == \"GPT-4\":\n                #     gpt4_stories.append(story)\n\n# with open(\"gpt35_stories.txt\", \"w\") as f:\n#     f.write(\"\\n\".join(gpt35_stories))\n\n# with open(\"gpt4_stories.txt\", \"w\") as f:\n#     f.write(\"\\n\".join(gpt4_stories))\n\n100%|██████████| 51/51 [03:08&lt;00:00,  3.69s/it]\n\n\n\nwith open(TS_PATH / \"TinyStories.txt\", \"w\") as f:\n    f.write(\"\\n\".join(stories))\n\n: \n\n\n\n\n\n'\\t\\n !\"$%&\\'()*+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyz|~'"
  },
  {
    "objectID": "examples/tinystories_tokenizer.html",
    "href": "examples/tinystories_tokenizer.html",
    "title": "tidygrad",
    "section": "",
    "text": "from tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers.trainers import WordPieceTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\ntokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\ntrainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[SEP]\", \"[PAD]\"], vocab_size=1024)\n\ntokenizer.pre_tokenizer = Whitespace()\n\nfrom pathlib import Path\n\nTS_PATH = Path(\"datasets/TinyStories/\")\n\n\n# res = tokenizer.train([ str(TS_PATH / \"TinyStories.txt\") ], trainer)\n\n\n# tokenizer.save(str(TS_PATH / \"wordpiece_1024.json\"))\n\n\ntokenizer = Tokenizer.from_file(str(TS_PATH / \"wordpiece_1024.json\"))\n\n\ntoken_ids = tokenizer.encode(\"Hello, y'all! How are you?\").ids\nprint(token_ids)\n\nprint(tokenizer.decode(token_ids))\n\n[247, 988, 14, 90, 9, 346, 3, 42, 235, 430, 264, 33]\nHe ##llo , y ' all ! H ##ow are you ?\n\n\n\nwith open(\"./datasets/TinyStories/TinyStories_1percent.txt\") as f:\n    text = f.read()\n\n\ntokenized_text = tokenizer.encode(text).ids\n\n\ntokenized_text[:10]\n\n[227, 193, 442, 430, 324, 16, 250, 449, 191, 242]\n\n\n\nimport numpy as np\n\n\ntokenized_text_np = np.array(tokenized_text).astype(np.int16)\n\n\nnp.save(\"./datasets/TinyStories/TinyStories_1percent_ids\", tokenized_text_np)"
  },
  {
    "objectID": "ops.conv.html",
    "href": "ops.conv.html",
    "title": "Functional operations: Convolution and Pooling",
    "section": "",
    "text": "def np_pad2d(a: np.ndarray, pad: tuple[int, int]) -&gt; np.ndarray:\n    h, w = pad\n    pad_widths = [(0, 0)] * (a.ndim - 2) + [(h, h), (w, w)]\n    return np.pad(a, pad_widths, mode=\"constant\")\n\ndef np_unpad2d(a: np.ndarray, pad: tuple[int, int]) -&gt; np.ndarray:\n    h, w = pad\n    return a[..., h:-h, w:-w]\n\n\n# Updated test for np_pad\nA = np.array([[1, 2], [3, 4]])\npad_A = np_pad2d(A, (1, 1))\nassert np.array_equal(pad_A, [[0, 0, 0, 0], [0, 1, 2, 0], [0, 3, 4, 0], [0, 0, 0, 0]])\nA_recovered = np_unpad2d(pad_A, (1, 1))\nassert np.array_equal(A, A_recovered)\n\n\nsource\n\nPad\n\n Pad (a, padding:Union[int,Tuple[int,int]], name=None)\n\nPad a tensor\n\nfrom matplotlib import pyplot as plt\nimport tidygrad as tg\n\n\nx = tg.Tensor(np.random.randn(1, 1, 4, 5), name=\"x\")\nbias = Pad(x, 1).out\n\nplt.imshow(bias.data[0, 0, :, :]);\n\n\n\n\n\ndef np_conv2d(input: np.ndarray, kernel: np.ndarray, bias: np.ndarray):\n    batch, input_channels, input_height, input_width = input.shape\n    output_channels, _input_channels, kernel_height, kernel_width = kernel.shape\n\n    assert kernel_height % 2 == 1, \"Only odd kernel sizes are supported for\"\n    assert kernel_width % 2 == 1, \"Only odd kernel sizes are supported for\"\n\n    assert (input_channels == _input_channels), f\"Input channels mismatch: {input_channels} != {_input_channels}\"\n    assert bias.shape == (output_channels, ), f\"Invalid bias shape: {bias.shape}\"\n\n    # bias = bias / (kernel_height * kernel_width)\n\n    output_height = input_height - kernel_height + 1\n    output_width = input_width - kernel_width + 1\n\n    y = np.zeros((batch, output_channels, output_height, output_width), dtype=input.dtype)\n    for r in range(output_height):\n        for c in range(output_width):\n            region = input[..., r:r + kernel_height, c:c + kernel_width]\n            y[..., r, c] = np.sum(region * kernel, axis=(-1, -2, -3)) + bias\n    return y\n\ndef np_kernel_grad(input: np.ndarray, kernel: np.ndarray, bias: np.ndarray, grad: np.ndarray):\n    batch, input_channels, input_height, input_width = input.shape\n    output_channels, _input_channels, kernel_height, kernel_width = kernel.shape\n\n    assert kernel_height % 2 == 1, \"Only odd kernel sizes are supported for\"\n    assert kernel_width % 2 == 1, \"Only odd kernel sizes are supported for\"\n\n    assert (input_channels == _input_channels), f\"Input channels mismatch: {input_channels} != {_input_channels}\"\n    assert bias.shape == (output_channels, ), f\"Invalid bias shape: {bias.shape}\"\n\n    output_height = input_height - kernel_height + 1\n    output_width = input_width - kernel_width + 1\n\n    # padded_input  # [ batch, input_channels, input_height, input_width\n    # kernel        # [ output_channels, input_channels, kernel_height, kernel_width ]\n    # grad          # [ batch, output_channels, output_height, output_width ]\n\n    # assert kernel_width == kernel_height, \"Only square kernels are supported for now\"\n\n    assert grad.shape[-2:] == (\n        output_height,\n        output_width,\n    ), f\"Invalid grad shape: {grad.shape}\"\n\n    grad_w = np.zeros_like(kernel)\n    for r in range(output_height):\n        for c in range(output_width):\n            p = input[:, :, r:r + kernel_height, c:c + kernel_width]\n\n            for ch in range(0, output_channels):\n                q = grad[:, ch, r, c]\n                grad_w[ch, :, :, :] += (p * q).sum(axis=0)\n\n    return grad_w\n\n\n\nRelationship Between Padding, Kernel Size, and Deconvolution\nFor a convolution operation, given:\n\nInput size \\(N\\)\nKernel size \\(k\\) (where \\(k \\leq N\\))\nPadding \\(p\\) (applied symmetrically on all sides)\n\nThe padded input will have a size of \\(N + 2p\\).\nAfter applying the convolution with the kernel, the output \\(y\\) will have a size:\n\\[\nN_y = N + 2p - k + 1\n\\]\n\nBack to Input Domain (Deconvolution)\nBefore convolving back to the input domain, we apply a new padding \\(p'\\) (unknown at this point) to \\(N_y\\), resulting in a length of \\(N_y + 2p'\\).\nAfter the deconvolution, we get:\n\\[\n(N_y + 2p') - k + 1 = N + 2p - k + 1 + 2p' - k + 1\n\\]\nSimplifying, we find:\n\\[\nN + 2(p + p') - 2k + 2 = N \\\\\n2(p + p') - 2k + 2 = 0 \\\\\np + p' = k - 1 \\\\\np' = k - p - 1 \\\\\n\\]\nThis equation gives us the value of \\(p'\\) required to get back to the input domain size \\(N\\) after deconvolution.\n\n\n\n\nBack-propagation through a convolutional layer\nFor a convolution operation, given:\n(even kernels only, stride size one)\n\nInput size \\(N\\)\nKernel size \\(k\\) (where \\(k \\leq N\\))\n\n\nForward pass:\n\nOutput size will be \\(N - (k+1)\\)\n\nFor example: Input 7x7, kernel 3 -&gt; output 5x5 Input 11x11, kernel 5 -&gt; output 7x7\n\n\nBack pass, calculating the gradient for the input.\nThe shape of the chain gradient will be the same the output.\nWe are going to apply a convolution to it with the rotated and transposed kernel, so we need to pad the gradient for the convolution to produce an output of input size.\nfor example (Forward ) Input 7x7, kernel 3 -&gt; output 5x5 (Backward) Gradient 5x5, padded by P, kernel 3 -&gt; output should be 7x7\nThus 2P (padding on both sides) should be input_size - output_size + (kernel_size - 1)\nAnd P should be (input_size - output_size + (kernel_size - 1)) / 2\nNote that (kernel_size - 1) is always even, and input_size and output_size are either both even or both odd, so this divides evenly.\n\nclass Conv2D(BaseOp):\n    def __init__(self, input, kernel, bias, stride=1, padding=0, name=None):\n        super().__init__(input, kernel, bias, name=name)\n        assert stride == 1, \"Only stride=1 is supported for now\"\n        if 1 in kernel.data.shape[-2:]:\n            assert padding == 0, \"Padding is not supported for 1x1 kernels\"\n        self.stride = stride\n        self.padding = (padding, padding) if isinstance(padding, int) else padding\n        self.parents = [*self.args] if self.requires_grad else []\n        self.input_data_padded = np_pad2d(self.args[0].data, self.padding)\n\n        data = np_conv2d(self.input_data_padded, kernel.data, bias.data)\n\n        self.set_out(data)\n        # self.out = Tensor(data, name=self.name, op=self)\n\n    def backward(self):\n        rot180 = lambda x: np.rot90(x, 2, axes=(-1, -2)).transpose(1, 0, 2, 3)\n        input, kernel, bias = self.args\n        grad = self.out.grad\n\n        pad_h = (input.data.shape[-2] - self.out.data.shape[-2] + (kernel.data.shape[-2] - 1))\n        pad_w = (input.data.shape[-1] - self.out.data.shape[-1] + (kernel.data.shape[-1] - 1))\n\n        assert pad_h % 2 == 0, \"Invalid padding\"\n        assert pad_w % 2 == 0, \"Invalid padding\"\n        pad_h //= 2\n        pad_w //= 2\n\n        padded_grad = np_pad2d(grad, (pad_h, pad_w))\n        input.accum_grad(np_conv2d(padded_grad, rot180(kernel.data), np.zeros(input.shape[1])))\n\n        kernel.accum_grad(np_kernel_grad(self.input_data_padded, kernel.data, bias.data, grad))\n        bias.accum_grad(grad.sum(axis=(0, -1, -2)))\n\n\nfrom tidygrad.utils.grad_check import grad_check\n\n\nh = 12\nw = 12\nkernel_h = 3\nkernel_w = 3\n\nin_ch = 2\nout_ch = 5\npad = 1\n\nx = tg.Tensor(np.random.randn(in_ch, h, w)[None], \"x\", requires_grad=True)\n\nfilter = np.random.randn(out_ch, in_ch, kernel_h, kernel_w)\nkernel = tg.Tensor(filter, \"w\", requires_grad=True)\n\nbias = tg.Tensor(np.random.randn(out_ch), \"b\", requires_grad=True)\nconv = Conv2D(x, kernel, bias, padding=pad).out\nloss = conv.mean()\nloss.backward()\n\ndef func(inputs, params):\n    x, kernel, bias = params\n    return Conv2D(x, kernel, bias, padding=pad).out.mean()\n\ngrad_check(func, None, (x, kernel, bias))\n\nMax fractional gradient difference for b: 0.0000%\nMax fractional gradient difference for w: 0.0000%\nMax fractional gradient difference for x: 0.0000%\n\n\n\nfrom tidygrad.utils.datasets import MNIST\n\n\nmnist = MNIST()\n\nx = tg.Tensor(mnist[1][0][None][None], \"x\")\nplt.show(plt.imshow(x.data[0, 0]))\n\nfilter = np.array([[1.0, 0, -1], [1, 0, -1], [1, 0, -1]])[None, None]\n\nkernel = tg.Tensor(filter.reshape(1, 1, 3, 3), \"w\")\nbias = tg.Tensor(np.zeros((1, )), \"b\")\n\nconv = Conv2D(x, kernel, bias, padding=10).out\n# conv = Conv2D(conv, kernel, bias, padding=1).out\n# conv = Conv2D(conv, kernel, bias, padding=1).out\nplt.imshow(conv.data[0, 0, :, :])\n\n\n\n\n&lt;matplotlib.image.AxesImage&gt;\n\n\n\n\n\n\nclass MaxPool2D(UnaryElementwiseOp):\n    def __init__(self, a, kernel_size, name=None):\n        super().__init__(a, name=name)\n        self.kernel_size = kernel_size\n\n\nA = np.arange(0, 36).reshape(1, 1, 6, 6)\nA.reshape(1, 1, 3, 3, 2, 2)\n\nprint(A)\n\nblock_size = 2\n\nshape = A.shape\nnew_shape = shape[:-2] + (\n    shape[-2] // block_size,\n    block_size,\n    shape[-1] // block_size,\n    block_size,\n)\nA.reshape(new_shape).swapaxes(-2, -3)\n\ndef factorial(a):\n    return np.prod(np.arange)\n\n[[[[ 0  1  2  3  4  5]\n   [ 6  7  8  9 10 11]\n   [12 13 14 15 16 17]\n   [18 19 20 21 22 23]\n   [24 25 26 27 28 29]\n   [30 31 32 33 34 35]]]]\n\n\n\nA = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n\nA = A.reshape(2, 2, 2, 2)\nprint(A)\nA.swapaxes(-2, -3)\n\n[[[[ 1  2]\n   [ 3  4]]\n\n  [[ 5  6]\n   [ 7  8]]]\n\n\n [[[ 9 10]\n   [11 12]]\n\n  [[13 14]\n   [15 16]]]]\n\n\narray([[[[ 1,  2],\n         [ 5,  6]],\n\n        [[ 3,  4],\n         [ 7,  8]]],\n\n\n       [[[ 9, 10],\n         [13, 14]],\n\n        [[11, 12],\n         [15, 16]]]])"
  },
  {
    "objectID": "tests/02_train_xor.html",
    "href": "tests/02_train_xor.html",
    "title": "Train a logical xor funciton using a neural network",
    "section": "",
    "text": "from tidygrad.tensor import Tensor\nfrom tidygrad.func import sigmoid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom fastcore.test import test_eq\n\n\n# Train an XOR network\nX = [\n    [0, 0],\n    [1, 0],\n    [0, 1],\n    [1, 1],\n]\n\n\nY = [0, 1, 1, 0]\n\n\n# np.random.seed(1337)\n\nw1 = Tensor(np.random.randn(2, 6), \"w1\", requires_grad=True)\nb1 = Tensor(np.zeros((1, 6)), \"b1\", requires_grad=True)\n\nw2 = Tensor(np.random.randn(6, 1), \"w2\", requires_grad=True)\n\nLR = 0.3\n\nlosses = []\n\n\ndef NN(inputs, params):\n    x, y = inputs\n    w1, b1, w2 = params\n    z1 = sigmoid(x.mmul(w1)+b1, \"z1\")\n    preds = sigmoid(z1.mmul(w2, \"z2\"), \"preds\")\n\n    diff = preds.sub(y, \"diff\")\n\n    l = diff.mul(diff, \"l\")\n    loss = l.sum(\"loss\")\n\n    return loss\n\n\nfor i in range(10000):\n    x = Tensor(np.array(X, dtype=float), \"x\")\n    y = Tensor(np.array(Y, dtype=float)[:, None], \"y\")\n\n    loss = NN((x, y), (w1, b1, w2))\n\n    loss.backward()\n\n    w1.data -= LR * w1.grad\n    b1.data -= LR * b1.grad\n    w2.data -= LR * w2.grad\n\n    w1.grad = np.zeros_like(w1.grad)\n    b1.grad = np.zeros_like(b1.grad)\n    w2.grad = np.zeros_like(w2.grad)\n\n    losses.append(loss.data)\n\nplt.plot(losses)\nplt.yscale(\"log\")\n\n\n\n\n\ntest_eq(loss.data &lt; 0.01, True)"
  },
  {
    "objectID": "tensor_helpers.html",
    "href": "tensor_helpers.html",
    "title": "Tensor class",
    "section": "",
    "text": "source\n\nsplit\n\n split (t:__main__.Tensor, n:int, axis:int)\n\n\nsource\n\n\nstd\n\n std (input:__main__.Tensor, name=None, axis=None, keepdims=False,\n      correction=1)\n\n\nsource\n\n\nmean\n\n mean (input:__main__.Tensor, name=None, axis=None, keepdims=False)\n\n\nsource\n\n\nTensor\n\n Tensor ()\n\nInitialize self. See help(type(self)) for accurate signature."
  }
]