{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | default_exp tensor\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy Tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "from lovely_numpy import lovely\n",
    "\n",
    "class Tensor:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def calculate_target_shape(s1, s2):\n",
    "    \"\"\"Calculate the target shape for broadcasting two tensors\"\"\"\n",
    "\n",
    "    # expand shaped to be the same length. Note (1,) * <negative> is empty\n",
    "    s2 = (1, ) * (len(s1) - len(s2)) + s2\n",
    "    s1 = (1, ) * (len(s2) - len(s1)) + s1\n",
    "\n",
    "    out_shape = ()\n",
    "    for dims in list(zip(reversed(s1), reversed(s2))):\n",
    "        if dims[0] != 1 and dims[1] != 1 and dims[0] != dims[1]:\n",
    "            raise ValueError(f\"Cannot broadcast {s1} and {s2}\")\n",
    "        out_shape = (max(dims), ) + out_shape\n",
    "\n",
    "    return out_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def maybe_broadcast_elementwise(a: Tensor, b: Tensor):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes\"\"\"\n",
    "    if a.data.shape != b.data.shape:\n",
    "        target_shape = calculate_target_shape(a.data.shape, b.data.shape)\n",
    "        # print(\n",
    "        #     f\"Elementwise broadcasted {a.data.shape} and {b.data.shape} to {target_shape}\"\n",
    "        # )\n",
    "        a = a.broadcast(target_shape) if a.data.shape != target_shape else a\n",
    "        b = b.broadcast(target_shape) if b.data.shape != target_shape else b\n",
    "\n",
    "    return a, b\n",
    "\n",
    "def maybe_broadcast_matmul(a: Tensor, b: Tensor):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes, except for the last two dimensions\"\"\"\n",
    "\n",
    "    a_short_shape = a.data.shape[:-2]\n",
    "    b_short_shape = b.data.shape[:-2]\n",
    "\n",
    "    if a_short_shape != b_short_shape:\n",
    "        target_shape = calculate_target_shape(a_short_shape, b_short_shape)\n",
    "        # print(\n",
    "        #     f\"Matmul broadcasted {a.data.shape} and {b.data.shape} to {target_shape + a.data.shape[-2:]} and {target_shape + b.data.shape[-2:]}\"\n",
    "        # )\n",
    "        a = (a.broadcast(target_shape + a.data.shape[-2:]) if a_short_shape != target_shape else a)\n",
    "        b = (b.broadcast(target_shape + b.data.shape[-2:]) if b_short_shape != target_shape else b)\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class BaseOp:\n",
    "    \"\"\"Base class for all operations\"\"\"\n",
    "\n",
    "    name_template = \"??\"\n",
    "\n",
    "    def __init__(self, *args, name: str = None):\n",
    "        assert isinstance(name, (str, type(None))), f\"name= should be str, got {type(name)}. You probably meant something else.\"\n",
    "\n",
    "        self.args = [arg if isinstance(arg, Tensor) else Tensor(data=np.asarray(arg, dtype=np.float32)) for arg in args]\n",
    "        self.name = (self.name_template.format(*[arg.name for arg in self.args]) if name is None else name)\n",
    "        self.requires_grad = any(arg.requires_grad for arg in self.args) and _grad\n",
    "\n",
    "    def set_out(self, data):\n",
    "        self.out = Tensor(data=data, requires_grad=self.requires_grad, name=self.name, op=self)\n",
    "\n",
    "    def check_backward(self):\n",
    "        # Add more checks here?\n",
    "        assert (self.out.requires_grad), f\"You are trying to backpropagate through a non-differentiable operation:\\n{self}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"{self.__class__.__name__}({', '.join([str(arg) for arg in self.args])})\")\n",
    "\n",
    "class BinaryElementwiseOp(BaseOp):\n",
    "    \"\"\"Base class for binary elementwise operations\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.args = maybe_broadcast_elementwise(*self.args)\n",
    "        self.parents = self.args if self.requires_grad else []\n",
    "\n",
    "class UnaryElementwiseOp(BaseOp):\n",
    "    \"\"\"Base class for unary elementwise operations\"\"\"\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.parents = self.args if self.requires_grad else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "\n",
    "class Load(BaseOp):\n",
    "    \"\"\"Load a tensor\"\"\"\n",
    "\n",
    "    name_template = \"?\"\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.parents = []\n",
    "\n",
    "\n",
    "class Add(BinaryElementwiseOp):\n",
    "    \"\"\"Add two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}+{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        # self.out = Tensor(data=self.args[0].data + self.args[1].data, name=self.name, op=self, requires_grad=self.requires_grad)\n",
    "        self.set_out(self.args[0].data + self.args[1].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad)\n",
    "        self.parents[1].accum_grad(self.out.grad)\n",
    "\n",
    "\n",
    "class Sub(BinaryElementwiseOp):\n",
    "    \"\"\"Subtract two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}-{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.set_out(self.args[0].data - self.args[1].data)\n",
    "        # self.out = Tensor(data=self.args[0].data - self.args[1].data, name=self.name, op=self)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad)\n",
    "        self.parents[1].accum_grad(-self.out.grad)\n",
    "\n",
    "\n",
    "class Mul(BinaryElementwiseOp):\n",
    "    \"\"\"Multiply two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}*{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        # self.out = Tensor(data=self.args[0].data * self.args[1].data, name=self.name, op=self)\n",
    "        self.set_out(self.args[0].data * self.args[1].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad * self.parents[1].data)\n",
    "        self.parents[1].accum_grad(self.out.grad * self.parents[0].data)\n",
    "\n",
    "\n",
    "class Div(BinaryElementwiseOp):\n",
    "    \"\"\"Divide two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}/{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        # self.out = Tensor(data=self.args[0].data / self.args[1].data, name=self.name, op=self)\n",
    "        self.set_out(self.args[0].data / self.args[1].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad / self.parents[1].data)\n",
    "        self.parents[1].accum_grad(\n",
    "            -self.out.grad * self.parents[0].data / (self.parents[1].data ** 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class Neg(UnaryElementwiseOp):\n",
    "    \"\"\"Negate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"(-{})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        # self.out = Tensor(-self.args[0].data, name=self.name, op=self)\n",
    "        self.set_out(-self.args[0].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(-self.out.grad)\n",
    "\n",
    "\n",
    "class Pow(UnaryElementwiseOp):\n",
    "    \"\"\"Raise a tensor to a power\"\"\"\n",
    "\n",
    "    # name_template = \"pow({},?)\"\n",
    "\n",
    "    def __init__(self, a, power, name=None):\n",
    "        self.name_template = f\"pow({{}},{power})\"\n",
    "        super().__init__(a, name=name)\n",
    "        self.power = power\n",
    "        self.set_out(self.args[0].data ** power)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(\n",
    "            (self.out.grad * self.power * self.parents[0].data ** (self.power - 1))\n",
    "        )\n",
    "\n",
    "\n",
    "class Log(UnaryElementwiseOp):\n",
    "    \"\"\"Take the natural logarithm of a tensor\"\"\"\n",
    "\n",
    "    name_template = \"log({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        # self.out = Tensor(np.log(self.args[0].data), name=self.name, op=self)\n",
    "        self.set_out(np.log(self.args[0].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad / self.parents[0].data)\n",
    "\n",
    "\n",
    "class Exp(UnaryElementwiseOp):\n",
    "    \"\"\"Exponentiate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"exp({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        # self.out = Tensor(np.exp(self.args[0].data), name=self.name, op=self)\n",
    "        self.set_out(np.exp(self.args[0].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad * self.out.data)\n",
    "\n",
    "\n",
    "class Matmul(BaseOp):\n",
    "    \"\"\"Matrix multiplication of two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}@{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.args = maybe_broadcast_matmul(*self.args)\n",
    "        self.parents = self.args if self.requires_grad else []\n",
    "        # self.out = Tensor(\n",
    "        #     np.matmul(self.args[0].data, self.args[1].data),\n",
    "        #     name=self.name,\n",
    "        #     op=self,\n",
    "        # )\n",
    "        self.set_out(np.matmul(self.args[0].data, self.args[1].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(\n",
    "            np.matmul(self.out.grad, self.parents[1].data.swapaxes(-1, -2))\n",
    "        )\n",
    "        self.parents[1].accum_grad(\n",
    "            np.matmul(self.parents[0].data.swapaxes(-1, -2), self.out.grad)\n",
    "        )\n",
    "\n",
    "\n",
    "class Sum(BaseOp):\n",
    "    \"\"\"Sum a tensor along the given axis (int or tuple of ints)\"\"\"\n",
    "\n",
    "    name_template = \"sum({})\"\n",
    "\n",
    "    def __init__(self, a, name=None, axis=None, keepdims=False):\n",
    "        super().__init__(a, name=name)\n",
    "        # self.axis = axis\n",
    "        self.parents = self.args if self.requires_grad else []\n",
    "        # self.out = Tensor(np.sum(self.args[0].data, axis=axis, keepdims=keepdims), name=self.name, op=self)\n",
    "        self.set_out(np.sum(self.args[0].data, axis=axis, keepdims=keepdims))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad)\n",
    "\n",
    "\n",
    "class Broadcast(BaseOp):\n",
    "    \"\"\"Broadcast a tensor to the given shape\"\"\"\n",
    "\n",
    "    name_template = \"broadcast({})\"\n",
    "\n",
    "    def __init__(self, a, target_shape, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.target_shape = target_shape\n",
    "        self.parents = self.args if self.requires_grad else []\n",
    "        self_shape = self.args[0].data.shape\n",
    "        assert self_shape != target_shape, \"Why are you broadcasting to the same shape?\"\n",
    "\n",
    "        if len(self_shape) < len(target_shape):\n",
    "            expanded_shape = (len(target_shape) - len(self_shape)) * (1,) + self_shape\n",
    "        else:\n",
    "            expanded_shape = self_shape\n",
    "\n",
    "        final_shape = ()\n",
    "        broadcasted_dims = ()\n",
    "\n",
    "        for s_expanded, s_target in reversed(list(zip(expanded_shape, target_shape))):\n",
    "            if s_expanded != s_target:\n",
    "                if s_expanded != 1:\n",
    "                    raise ValueError(f\"Cannot broadcast {self_shape} to {target_shape}\")\n",
    "                else:\n",
    "                    broadcasted_dims = (True,) + broadcasted_dims\n",
    "                    final_shape = (s_target,) + final_shape\n",
    "            else:\n",
    "                broadcasted_dims = (False,) + broadcasted_dims\n",
    "                final_shape = (s_expanded,) + final_shape\n",
    "\n",
    "        broadcasted_data = np.broadcast_to(self.args[0].data, final_shape)\n",
    "\n",
    "        assert final_shape == broadcasted_data.shape\n",
    "\n",
    "        data = broadcasted_data\n",
    "        self.broadcasted_dims = broadcasted_dims\n",
    "\n",
    "        # self.out = Tensor(data, name=self.name, op=self)\n",
    "        self.set_out(data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        axis = tuple([i for i, dim in enumerate(self.broadcasted_dims) if dim])\n",
    "        summed = self.out.grad.sum(axis=axis, keepdims=True)\n",
    "\n",
    "        if summed.shape != self.parents[0].data.shape:\n",
    "            summed = summed.reshape(self.parents[0].data.shape)\n",
    "\n",
    "        self.parents[0].accum_grad(summed)\n",
    "\n",
    "\n",
    "# class LessThan(BinaryElementwiseOp):\n",
    "#     name_template = \"({}<{})\"\n",
    "\n",
    "#     def __init__(self, a, b, name=None):\n",
    "#         super().__init__(a, b, name=name)\n",
    "#         self.out = Tensor(\n",
    "#             data=self.args[0].data < self.args[1].data, name=self.name, op=self\n",
    "#         )\n",
    "\n",
    "#     # def backward(self):\n",
    "#     #     self.parents[0].accum_grad(self.out.grad * (self.parents[0].data < self.parents[1].data)\n",
    "#     #     self.parents[1].accum_grad(self.out.grad * (self.parents[0].data >= self.parents[1].data)\n",
    "\n",
    "# class Where(BaseOp):\n",
    "#     name_template = \"where({})\"\n",
    "\n",
    "#     def __init__(self, a, b, c, name=None):\n",
    "#         super().__init__(a, b, c, name=name)\n",
    "#         self.parents = self.args\n",
    "#         self.out = Tensor(\n",
    "#             data=np.where(self.args[0].data, self.args[1].data, self.args[2].data),\n",
    "#             name=self.name,\n",
    "#             op=self,\n",
    "#         )\n",
    "\n",
    "#     def backward(self):\n",
    "#         # self.parents[0].accum_grad(self.out.grad * self.parents[1].data\n",
    "#         # self.parents[0].accum_grad(self.out.grad * self.parents[2].data\n",
    "\n",
    "#         self.parents[1].accum_grad(self.out.grad * self.parents[0].data\n",
    "#         self.parents[2].accum_grad(self.out.grad * (1 - self.parents[0].data)\n",
    "\n",
    "\n",
    "class ExpLog(UnaryElementwiseOp):\n",
    "    \"\"\"Exponentiate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"exp({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "\n",
    "        def logexp(x):\n",
    "            return np.where(x < 0, np.log(1 + np.exp(x)), x + np.log(1 + np.exp(-x)))\n",
    "\n",
    "        # self.out = Tensor(logexp(self.args[0].data), name=self.name, op=self)\n",
    "        self.set_out(logexp(self.args[0].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(\n",
    "            self.out.grad * (1 - 1 / (1 + np.exp(self.parents[0].data)))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Tensor:\n",
    "    # op = \"L\"\n",
    "    name: str = \"\"\n",
    "\n",
    "    def __init__(self, data, name=None, op=None, eps=1e-8, requires_grad=False):\n",
    "        self.data = np.asarray(data)\n",
    "\n",
    "        self.grad = (\n",
    "            np.zeros_like(self.data, dtype=np.float32) if requires_grad else None\n",
    "        )\n",
    "        self.eps = eps\n",
    "        self.op = op or Load(name=name)\n",
    "        self.name = name or self.op.name\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    def __repr__(self):\n",
    "        value_str = f\"v={lovely(self.data)}\"\n",
    "        grad_str = f\"∇={lovely(self.grad)}\" if self.grad is not None else \"\"\n",
    "        parents = (\n",
    "            f\" parents=[\" + \",\".join([p.name for p in self.op.parents]) + \"]\"\n",
    "            if self.op.parents\n",
    "            else \"\"\n",
    "        )\n",
    "        return f'Tensor{list(self.data.shape)}(name=\"{self.name}\" op={type(self.op).__name__}{parents}):\\n    {value_str}\\n    {grad_str}'\n",
    "\n",
    "    def accum_grad(self, grad):\n",
    "        if not self.requires_grad:\n",
    "            return\n",
    "\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "    def broadcast(self, target_shape, name=None):\n",
    "        return Broadcast(self, target_shape, name=name).out\n",
    "\n",
    "    def add(self, other, name=None):\n",
    "        return Add(self, other, name=name).out\n",
    "\n",
    "    def sub(self, other, name=None):\n",
    "        return Sub(self, other, name=name).out\n",
    "\n",
    "    def mul(self, other, name=None):\n",
    "        return Mul(self, other, name=name).out\n",
    "\n",
    "    def div(self, other, name=None):\n",
    "        return Div(self, other, name=name).out\n",
    "\n",
    "    def neg(self, name=None):\n",
    "        return Neg(self, name=name).out\n",
    "\n",
    "    def pow(self, power, name=None):\n",
    "        return Pow(self, power, name=name).out\n",
    "\n",
    "    def log(self, name=None):\n",
    "        return Log(self, name=name).out\n",
    "\n",
    "    def exp(self, name=None):\n",
    "        return Exp(self, name=name).out\n",
    "\n",
    "    def mmul(self, other, name=None):\n",
    "        return Matmul(self, other, name=name).out\n",
    "\n",
    "    def sum(self, name=None, axis=None, keepdims=False):\n",
    "        return Sum(self, name=name, axis=axis, keepdims=keepdims).out\n",
    "\n",
    "    def mean(self, name=None, axis=None, keepdims=False):\n",
    "        reduced = np.prod(self.data.shape)\n",
    "        if isinstance(axis, int):\n",
    "            axis = (axis,)\n",
    "        if axis:\n",
    "            reduced = np.prod([self.data.shape[i] for i in axis])\n",
    "        return Sum(self, name=name, axis=axis, keepdims=keepdims).out / reduced\n",
    "\n",
    "    def std(self, name=None, axis=None, keepdims=False, correction=1):\n",
    "        if isinstance(axis, int):\n",
    "            axis = (axis,)\n",
    "\n",
    "        v1 = self - self.mean(axis=axis, keepdims=True)\n",
    "        v1.name = \"v1\"\n",
    "        # print(v1)\n",
    "\n",
    "        var = (v1) ** 2\n",
    "        var.name = \"var\"\n",
    "        # print(var)\n",
    "\n",
    "        numel = (\n",
    "            np.prod(self.data.shape)\n",
    "            if axis is None\n",
    "            else np.prod([self.data.shape[i] for i in axis])\n",
    "        )\n",
    "\n",
    "        corrected = var.sum(axis=axis, keepdims=keepdims) / (\n",
    "            numel - correction\n",
    "        )\n",
    "\n",
    "        return corrected**0.5\n",
    "\n",
    "    # def lt(self, other, name=None):\n",
    "    #     return LessThan(self, other, name=name).out\n",
    "\n",
    "    # def where(self, other1, other2, name=None):\n",
    "    #     return Where(self, other1, other2, name=name).out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.add(other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.add(other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self.sub(other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return -(self.sub(other))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return self.mul(other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.mul(other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self.div(other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self.neg()\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        return self.pow(power)\n",
    "\n",
    "    def equal(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return self.data == other.data\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    # def __lt__(self, other):\n",
    "    #     return self.lt(other)\n",
    "\n",
    "    def backward(self):\n",
    "        # Create a list of all parent nodes, in reverse order\n",
    "        # Start with the current node\n",
    "        visited = []\n",
    "        nodes = []\n",
    "\n",
    "        assert self.data.size == 1, \"Cannot call backward on non-scalar tensor\"\n",
    "\n",
    "        def walk(node):\n",
    "            for p in node.op.parents:\n",
    "                if p not in visited and p.requires_grad:\n",
    "                    visited.append(p)\n",
    "                    walk(p)\n",
    "                    nodes.append(p)\n",
    "\n",
    "        walk(self)\n",
    "        nodes.append(self)\n",
    "\n",
    "        # print(nodes)\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for n in nodes[::-1]:\n",
    "            if hasattr(n.op, \"backward\"):\n",
    "                n.op.backward()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        assert self.requires_grad, \"Cannot zero grad on non-differentiable tensor\"\n",
    "        self.grad.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor[2, 3](name=\"x\" op=Load):\n",
       "    v=array[2, 3] n=6 x∈[-1.591, 2.410] μ=0.177 σ=1.231 [[-0.272, 0.850, 2.410], [0.039, -1.591, -0.376]]\n",
       "    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(np.random.randn(2, 3), name=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_eq, test_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(calculate_target_shape((1, 2, 3), (2, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (2, 1)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 1)), (1, 2, 3))\n",
    "\n",
    "test_eq(calculate_target_shape((1, 5), (3, 1)), (3, 5))\n",
    "\n",
    "test_fail(calculate_target_shape, args=((1, 2, 3), (2, 2)), contains=\"Cannot broadcast\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
