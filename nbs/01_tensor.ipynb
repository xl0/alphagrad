{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | default_exp tensor\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "from lovely_numpy import lovely\n",
    "\n",
    "import tidygrad.ops as ops\n",
    "import tidygrad.tensor_helpers as helpers\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "\n",
    "class no_grad:\n",
    "    def __enter__(self):\n",
    "        self.old_grad = ops.common._grad\n",
    "        ops.common._grad = False\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        ops.common._grad = self.old_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "_num_tensors = 0\n",
    "\n",
    "class Tensor:\n",
    "    pass\n",
    "\n",
    "def simplify_trace(trace):\n",
    "    return ' -> '.join(f'{frame.name} at {frame.filename}:{frame.lineno}' for frame in trace if '/python' not in frame.filename)\n",
    "\n",
    "alloc_log = {}\n",
    "\n",
    "class Tensor:\n",
    "    name: str = \"\"\n",
    "\n",
    "    def __init__(self, data, name=None, op=None, eps=1e-8, requires_grad=False):\n",
    "        global _num_tensors\n",
    "        _num_tensors += 1\n",
    "\n",
    "        trace = traceback.extract_stack()\n",
    "        simplified_trace = simplify_trace(trace)\n",
    "        alloc_log[id(self)] = simplified_trace\n",
    "        \n",
    "        # Increment allocation count\n",
    "\n",
    "        # if _num_tensors > 620:\n",
    "        #     raise Exception(\"Too many tensors\")\n",
    "\n",
    "        self.data = np.asarray(data) # , dtype=np.float32\n",
    "        if self.data.dtype == np.float64:\n",
    "            self.data = self.data.astype(np.float32)\n",
    "\n",
    "        self.grad = (np.zeros_like(self.data, dtype=np.float32) if requires_grad else None)\n",
    "        self.eps = eps\n",
    "        self.op = op or ops.Load(name=name)\n",
    "        self.name = name or self.op.name\n",
    "        self._requires_grad = requires_grad\n",
    "\n",
    "    def __del__(self):\n",
    "        # print(f\"Tensor {self.name} deleted\")\n",
    "        del alloc_log[id(self)]\n",
    "        global _num_tensors\n",
    "        _num_tensors -= 1\n",
    "\n",
    "    @property\n",
    "    def requires_grad(self):\n",
    "        return self._requires_grad\n",
    "\n",
    "    @requires_grad.setter\n",
    "    def requires_grad(self, requires_grad):\n",
    "        if requires_grad and self.grad is None:\n",
    "            self.grad = np.zeros_like(self.data)\n",
    "        \n",
    "        self._requires_grad = requires_grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        value_str = f\"v={lovely(self.data)}\"\n",
    "        grad_str = f\"âˆ‡={lovely(self.grad)}\" if self.grad is not None else \"\"\n",
    "        parents = (f\" parents=[\" + \",\".join([p.name for p in self.op.parents]) + \"]\" if self.op.parents else \"\")\n",
    "        \n",
    "        return f'Tensor{list(self.data.shape)}(name=\"{self.name}\" op={type(self.op).__name__}{parents}):\\n    {value_str}\\n    {grad_str}'\n",
    "\n",
    "    def accum_grad(self, grad):\n",
    "        if not self._requires_grad:\n",
    "            return\n",
    "\n",
    "        if self.grad is None:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "    def broadcast(self, target_shape, name=None):\n",
    "        return ops.Broadcast(self, target_shape, name=name).out\n",
    "\n",
    "    def add(self, other, name=None):\n",
    "        return ops.Add(self, other, name=name).out\n",
    "\n",
    "    def sub(self, other, name=None):\n",
    "        return ops.Sub(self, other, name=name).out\n",
    "\n",
    "    def mul(self, other, name=None):\n",
    "        return ops.Mul(self, other, name=name).out\n",
    "\n",
    "    def div(self, other, name=None):\n",
    "        return ops.Div(self, other, name=name).out\n",
    "\n",
    "    def neg(self, name=None):\n",
    "        return ops.Neg(self, name=name).out\n",
    "\n",
    "    def pow(self, power, name=None):\n",
    "        return ops.Pow(self, power, name=name).out\n",
    "\n",
    "    def log(self, name=None):\n",
    "        return ops.Log(self, name=name).out\n",
    "\n",
    "    def exp(self, name=None):\n",
    "        return ops.Exp(self, name=name).out\n",
    "\n",
    "    def mmul(self, other, name=None):\n",
    "        return ops.Matmul(self, other, name=name).out\n",
    "\n",
    "    # XXX move name to the end of arg list\n",
    "    def sum(self, name=None, axis=None, keepdims=False, ):\n",
    "        return ops.Sum(self,  axis=axis, keepdims=keepdims, name=name,).out\n",
    "\n",
    "    def transpose(\n",
    "        self,\n",
    "        dim0: int,\n",
    "        dim1: int,\n",
    "        name=None,\n",
    "    ):\n",
    "        return ops.Transpose(\n",
    "            self,\n",
    "            dim0,\n",
    "            dim1,\n",
    "            name=name,\n",
    "        ).out\n",
    "\n",
    "    # def softmax(input, name=None):\n",
    "    #     return func.softmax(input, name=name)\n",
    "\n",
    "    def mean(self, name=None, axis=None, keepdims=False):\n",
    "        return helpers.mean(self, name=name, axis=axis, keepdims=keepdims)\n",
    "\n",
    "    def std(self, name=None, axis=None, keepdims=False, correction=1):\n",
    "        return helpers.std(self, name=name, axis=axis, keepdims=keepdims, correction=correction)\n",
    "\n",
    "    def split(self, n, axis=0):\n",
    "        return helpers.split(self, n, axis=axis)\n",
    "\n",
    "    # def lt(self, other, name=None):\n",
    "    #     return LessThan(self, other, name=name).out\n",
    "\n",
    "    # def where(self, other1, other2, name=None):\n",
    "    #     return Where(self, other1, other2, name=name).out\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return self.add(other)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.add(other)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self.sub(other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return -(self.sub(other))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return self.mul(other)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self.mul(other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self.div(other)\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self.neg()\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        return self.pow(power)\n",
    "\n",
    "    def equal(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return self.data == other.data\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return ops.Slice(self, key).out\n",
    "\n",
    "    # def __setitem__(self, key, value):\n",
    "    #     return SetSlice(self, key, value)\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    # def __lt__(self, other):\n",
    "    #     return self.lt(other)\n",
    "\n",
    "    def backward(self):\n",
    "        # Create a list of all parent nodes, in reverse order\n",
    "        # Start with the current node\n",
    "        visited = []\n",
    "        nodes = []\n",
    "\n",
    "        assert self.data.size == 1, \"Cannot call backward on non-scalar tensor\"\n",
    "\n",
    "        def walk(node):\n",
    "            for p in node.op.parents:\n",
    "                if p not in visited and p.requires_grad:\n",
    "                    visited.append(p)\n",
    "                    walk(p)\n",
    "                    nodes.append(p)\n",
    "\n",
    "        walk(self)\n",
    "        nodes.append(self)\n",
    "\n",
    "        # print(nodes)\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for n in nodes[::-1]:\n",
    "            if hasattr(n.op, \"backward\"):\n",
    "                n.op.backward()\n",
    "                n.op = None\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        assert self._requires_grad, \"Cannot zero grad on non-differentiable tensor\"\n",
    "        self.grad.fill(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
