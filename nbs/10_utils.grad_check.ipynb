{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp utils.grad_check\n",
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "import tidygrad as tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def grad_check(func, inputs, params: tuple = (), eps=1e-5, n=1000, verbose=False):\n",
    "    grad_failed = False\n",
    "    # for p in reversed(params):\n",
    "    #     p.grad = np.zeros_like(p.data)\n",
    "    # loss = func(inputs, params)\n",
    "    # loss.backward()\n",
    "\n",
    "    with tg.no_grad():\n",
    "        for p in reversed(params):\n",
    "            # Reshape to 1D so it's easier to sample random indices\n",
    "            num_failed = num_skipped = num_checked = 0\n",
    "            data_view = p.data.reshape(-1)  # This does not make a copy\n",
    "            grad_view = p.grad.reshape(-1)\n",
    "\n",
    "            slow_grad = np.zeros_like(p.grad)\n",
    "\n",
    "            scaled_slow_grad_view = slow_grad.reshape(-1)\n",
    "\n",
    "            indices = np.random.choice(np.arange(grad_view.size), size=min(n, grad_view.size), replace=False)\n",
    "            good_indices = []\n",
    "            # indices = list(filter(lambda idx: abs(slow_grad_view[idx]) > eps, indices))  # XXX?\n",
    "            # if len(indices) == 0:\n",
    "            #     print(f\"Skipping {p.name} because all gradients are zero\")\n",
    "            #     continue\n",
    "            # else:\n",
    "            #     print(f\"Checking {p.name} with {len(indices)} non-zero gradients\")\n",
    "            for idx in indices:\n",
    "                old_val = data_view[idx]\n",
    "\n",
    "                loss = func(inputs, params)\n",
    "\n",
    "                data_view[idx] = old_val + eps\n",
    "                loss_plus_h = func(inputs, params)\n",
    "\n",
    "                scaled_slow_grad_view[idx] = (loss_plus_h.data - loss.data) / eps\n",
    "                # slow_grad_view[idx] =\n",
    "\n",
    "                # (loss_plus_h.data - loss.data) / eps\n",
    "\n",
    "                if verbose:\n",
    "                    print(\n",
    "                        f\"{idx}: loss_plus_h: {loss_plus_h.data}, loss: {loss.data}, diff: {loss_plus_h.data - loss.data}, grad: {grad_view[idx]}, slow_grad: {scaled_slow_grad_view[idx] / eps}\"\n",
    "                    )\n",
    "                data_view[idx] = old_val\n",
    "\n",
    "                if abs(scaled_slow_grad_view[idx]) > eps:\n",
    "                    good_indices.append(idx)\n",
    "\n",
    "            differences = ( (scaled_slow_grad_view[good_indices] - grad_view[good_indices])\n",
    "                            / (grad_view[good_indices])\n",
    "            )\n",
    "\n",
    "            # slow_grad /= eps\n",
    "\n",
    "            max_grad_diff = np.max(np.abs(differences))\n",
    "            print(f\"Max fractional gradient difference for {p.name}: {max_grad_diff*100:.4f}%\")\n",
    "            if max_grad_diff > 1e-2:\n",
    "                grad_failed = True\n",
    "                print(\"Failed!\")\n",
    "                print(\"Slow grad: \", slow_grad)\n",
    "                print(\"Fast grad: \", p.grad)\n",
    "                print(\"Differences: \", differences)\n",
    "\n",
    "    if grad_failed: raise ValueError(f\"Gradient check failed for {p.name}: Max error: {max_grad_diff*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lovely_numpy import Lo\n",
    "from tidygrad import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array[32, 10] n=320 (2.5Kb) x∈[-7.871, 6.829] μ=-0.001 σ=2.903"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lo((np.random.randn(32, 28 * 28) @ (np.random.randn(28 * 28, 100) * 0.1) + np.random.randn(100)) @ (np.random.randn(100, 10) * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max fractional gradient difference for w2: 0.0011%\n",
      "Max fractional gradient difference for b1: 0.0010%\n",
      "Max fractional gradient difference for w1: 0.0159%\n"
     ]
    }
   ],
   "source": [
    "i += 1\n",
    "np.random.seed(i)\n",
    "\n",
    "x = Tensor(np.random.randn(32, 28 * 28), \"X\")\n",
    "# Create a 1-hot encoded tensor with 1 random 1\n",
    "y = np.zeros((32, 10))\n",
    "y[np.arange(32), np.random.choice(10, 32)] = 1\n",
    "y = Tensor(y, \"y\")\n",
    "\n",
    "w1 = Tensor(np.random.randn(28 * 28, 100) * 0.1, \"w1\", requires_grad=True)\n",
    "b1 = Tensor(np.random.randn(100), \"b1\", requires_grad=True)\n",
    "w2 = Tensor(np.random.randn(100, 10) * 0.1, \"w2\", requires_grad=True)\n",
    "\n",
    "def NN(inputs, params: tuple):\n",
    "    x, y = inputs\n",
    "    w1, b1, w2 = params\n",
    "    z1 = x.mmul(w1, \"tmp\").add(b1, \"z1\")\n",
    "    a1 = tg.sigmoid(z1)\n",
    "    z2 = a1.mmul(w2)\n",
    "\n",
    "    loss = -tg.BCE_loss(z2, y).sum(\"loss\")\n",
    "\n",
    "    return loss\n",
    "\n",
    "debug = []\n",
    "loss = NN(inputs=(x, y), params=(w1, b1, w2))\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# grad_check(NN, (x, y), (w1, b1, w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
