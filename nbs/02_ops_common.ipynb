{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | default_exp ops.common\n",
    "import nbdev\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations: Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "import numpy as np\n",
    "_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def calculate_target_shape(s1, s2):\n",
    "    \"\"\"Calculate the target shape for broadcasting two tensors\"\"\"\n",
    "\n",
    "    # expand shaped to be the same length. Note (1,) * <negative> is empty\n",
    "    s2 = (1, ) * (len(s1) - len(s2)) + s2\n",
    "    s1 = (1, ) * (len(s2) - len(s1)) + s1\n",
    "\n",
    "    out_shape = ()\n",
    "    for dims in list(zip(reversed(s1), reversed(s2))):\n",
    "        if dims[0] != 1 and dims[1] != 1 and dims[0] != dims[1]:\n",
    "            raise ValueError(f\"Cannot broadcast {s1} and {s2}\")\n",
    "        out_shape = (max(dims), ) + out_shape\n",
    "\n",
    "    return out_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "def maybe_broadcast_elementwise(a, b):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes\"\"\"\n",
    "    if a.data.shape != b.data.shape:\n",
    "        target_shape = calculate_target_shape(a.data.shape, b.data.shape)\n",
    "        # print(\n",
    "        #     f\"Elementwise broadcasted {a.data.shape} and {b.data.shape} to {target_shape}\"\n",
    "        # )\n",
    "        a = a.broadcast(target_shape) if a.data.shape != target_shape else a\n",
    "        b = b.broadcast(target_shape) if b.data.shape != target_shape else b\n",
    "\n",
    "    return a, b\n",
    "\n",
    "def maybe_broadcast_matmul(a, b):\n",
    "    \"\"\"Broadcast two tensors if they have different shapes, except for the last two dimensions\"\"\"\n",
    "\n",
    "    a_short_shape = a.data.shape[:-2]\n",
    "    b_short_shape = b.data.shape[:-2]\n",
    "\n",
    "    if a_short_shape != b_short_shape:\n",
    "        target_shape = calculate_target_shape(a_short_shape, b_short_shape)\n",
    "        # print(\n",
    "        #     f\"Matmul broadcasted {a.data.shape} and {b.data.shape} to {target_shape + a.data.shape[-2:]} and {target_shape + b.data.shape[-2:]}\"\n",
    "        # )\n",
    "        a = (a.broadcast(target_shape + a.data.shape[-2:]) if a_short_shape != target_shape else a)\n",
    "        b = (b.broadcast(target_shape + b.data.shape[-2:]) if b_short_shape != target_shape else b)\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_eq, test_fail\n",
    "\n",
    "test_eq(calculate_target_shape((1, 2, 3), (2, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (2, 1)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 3)), (1, 2, 3))\n",
    "test_eq(calculate_target_shape((1, 2, 3), (1, 1)), (1, 2, 3))\n",
    "\n",
    "test_eq(calculate_target_shape((1, 5), (3, 1)), (3, 5))\n",
    "\n",
    "test_fail(calculate_target_shape, args=((1, 2, 3), (2, 2)), contains=\"Cannot broadcast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "_num_ops = 0\n",
    "\n",
    "class BaseOp:\n",
    "    \"\"\"Base class for all operations\"\"\"\n",
    "\n",
    "    name_template = \"??\"\n",
    "\n",
    "    # out: Tensor\n",
    "\n",
    "    def __init__(self, *args, name: str = None):\n",
    "        from tidygrad.tensor import Tensor\n",
    "        global _num_ops\n",
    "        _num_ops += 1\n",
    "        assert isinstance(name, (str, type(None))), f\"name= should be str, got {type(name)}. You probably meant something else.\"\n",
    "\n",
    "        self.args = [arg if isinstance(arg, Tensor) else Tensor(data=np.asarray(arg, dtype=np.float32)) for arg in args]\n",
    "        self.name = \"\"  # (self.name_template.format(*[arg.name for arg in self.args]) if name is None else name)\n",
    "        self.requires_grad = any(arg.requires_grad for arg in self.args) and _grad\n",
    "        self.parents = []\n",
    "\n",
    "    def set_out(self, data):\n",
    "        from tidygrad.tensor import Tensor\n",
    "        self.out = Tensor(data=data, requires_grad=self.requires_grad, name=self.name, op=self)\n",
    "\n",
    "    def check_backward(self):\n",
    "        # Add more checks here?\n",
    "        assert (self.out.requires_grad), f\"You are trying to backpropagate through a non-differentiable operation:\\n{self}\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"{self.__class__.__name__}({', '.join([str(arg) for arg in self.args])})\")\n",
    "\n",
    "class BinaryElementwiseOp(BaseOp):\n",
    "    \"\"\"Base class for binary elementwise operations\"\"\"\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.args = maybe_broadcast_elementwise(*self.args)\n",
    "        if self.requires_grad:\n",
    "            self.parents = self.args\n",
    "\n",
    "class UnaryElementwiseOp(BaseOp):\n",
    "    \"\"\"Base class for unary elementwise operations\"\"\"\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        if self.requires_grad:\n",
    "            self.parents = self.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Load(BaseOp):\n",
    "    \"\"\"Load a tensor\"\"\"\n",
    "\n",
    "    name_template = \"?\"\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Add(BinaryElementwiseOp):\n",
    "    \"\"\"Add two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}+{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.set_out(self.args[0].data + self.args[1].data)\n",
    "\n",
    "    # def __call__(self, a, b):\n",
    "    #     return Add(a, b, name=self.name)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad)\n",
    "        self.parents[1].accum_grad(self.out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Sub(BinaryElementwiseOp):\n",
    "    \"\"\"Subtract two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}-{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.set_out(self.args[0].data - self.args[1].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad)\n",
    "        self.parents[1].accum_grad(-self.out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Mul(BinaryElementwiseOp):\n",
    "    \"\"\"Multiply two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}*{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.set_out(self.args[0].data * self.args[1].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad * self.parents[1].data)\n",
    "        self.parents[1].accum_grad(self.out.grad * self.parents[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Div(BinaryElementwiseOp):\n",
    "    \"\"\"Divide two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}/{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.set_out(self.args[0].data / self.args[1].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad / self.parents[1].data)\n",
    "        self.parents[1].accum_grad(-self.out.grad * self.parents[0].data / (self.parents[1].data**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Neg(UnaryElementwiseOp):\n",
    "    \"\"\"Negate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"(-{})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.set_out(-self.args[0].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(-self.out.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Pow(UnaryElementwiseOp):\n",
    "    \"\"\"Raise a tensor to a power\"\"\"\n",
    "    def __init__(self, a, power, name=None):\n",
    "        self.name_template = f\"pow({{}},{power})\"\n",
    "        super().__init__(a, name=name)\n",
    "        self.power = power\n",
    "        self.set_out(self.args[0].data**power)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad((self.out.grad * self.power * self.parents[0].data**(self.power - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Log(UnaryElementwiseOp):\n",
    "    \"\"\"Take the natural logarithm of a tensor\"\"\"\n",
    "\n",
    "    name_template = \"log({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.set_out(np.log(self.args[0].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad / self.parents[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Exp(UnaryElementwiseOp):\n",
    "    \"\"\"Exponentiate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"exp({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.set_out(np.exp(self.args[0].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad * self.out.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ExpLog(UnaryElementwiseOp):\n",
    "    \"\"\"Exponentiate a tensor\"\"\"\n",
    "\n",
    "    name_template = \"exp({})\"\n",
    "\n",
    "    def __init__(self, a, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "\n",
    "        def logexp(x):\n",
    "            return np.where(x < 0, np.log(1 + np.exp(x)), x + np.log(1 + np.exp(-x)))\n",
    "\n",
    "        self.set_out(logexp(self.args[0].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad * (1 - 1 / (1 + np.exp(self.parents[0].data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Matmul(BaseOp):\n",
    "    \"\"\"Matrix multiplication of two tensors\"\"\"\n",
    "\n",
    "    name_template = \"({}@{})\"\n",
    "\n",
    "    def __init__(self, a, b, name=None):\n",
    "        super().__init__(a, b, name=name)\n",
    "        self.args = maybe_broadcast_matmul(*self.args)\n",
    "        if self.requires_grad:\n",
    "            self.parents = self.args\n",
    "\n",
    "        self.set_out(np.matmul(self.args[0].data, self.args[1].data))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(np.matmul(self.out.grad, self.parents[1].data.swapaxes(-1, -2)))\n",
    "        self.parents[1].accum_grad(np.matmul(self.parents[0].data.swapaxes(-1, -2), self.out.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Sum(BaseOp):\n",
    "    \"\"\"Sum-reduce a tensor along the given axis (int or tuple of ints)\"\"\"\n",
    "\n",
    "    name_template = \"sum({})\"\n",
    "\n",
    "    def __init__(self, a, name=None, axis=None, keepdims=False):\n",
    "        super().__init__(a, name=name)\n",
    "        self.parents = self.args if self.requires_grad else []\n",
    "        self.set_out(np.sum(self.args[0].data, axis=axis, keepdims=keepdims))\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].accum_grad(self.out.grad)  # This will broadcast correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Broadcast(BaseOp):\n",
    "    \"\"\"Broadcast a tensor to the given shape\"\"\"\n",
    "\n",
    "    name_template = \"broadcast({})\"\n",
    "\n",
    "    def __init__(self, a, target_shape, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.target_shape = target_shape\n",
    "        self.parents = self.args if self.requires_grad else []\n",
    "        self_shape = self.args[0].data.shape\n",
    "        assert self_shape != target_shape, \"Why are you broadcasting to the same shape?\"\n",
    "\n",
    "        if len(self_shape) < len(target_shape):\n",
    "            expanded_shape = (len(target_shape) - len(self_shape)) * (1, ) + self_shape\n",
    "        else:\n",
    "            expanded_shape = self_shape\n",
    "\n",
    "        final_shape = ()\n",
    "        broadcasted_dims = ()\n",
    "\n",
    "        for s_expanded, s_target in reversed(list(zip(expanded_shape, target_shape))):\n",
    "            if s_expanded != s_target:\n",
    "                if s_expanded != 1:\n",
    "                    raise ValueError(f\"Cannot broadcast {self_shape} to {target_shape}\")\n",
    "                else:\n",
    "                    broadcasted_dims = (True, ) + broadcasted_dims\n",
    "                    final_shape = (s_target, ) + final_shape\n",
    "            else:\n",
    "                broadcasted_dims = (False, ) + broadcasted_dims\n",
    "                final_shape = (s_expanded, ) + final_shape\n",
    "\n",
    "        broadcasted_data = np.broadcast_to(self.args[0].data, final_shape)\n",
    "\n",
    "        assert final_shape == broadcasted_data.shape\n",
    "\n",
    "        data = broadcasted_data\n",
    "        self.broadcasted_dims = broadcasted_dims\n",
    "\n",
    "        self.set_out(data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        axis = tuple([i for i, dim in enumerate(self.broadcasted_dims) if dim])\n",
    "        summed = self.out.grad.sum(axis=axis, keepdims=True)\n",
    "\n",
    "        if summed.shape != self.parents[0].data.shape:\n",
    "            summed = summed.reshape(self.parents[0].data.shape)\n",
    "\n",
    "        self.parents[0].accum_grad(summed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Slice(UnaryElementwiseOp):\n",
    "    name_template = \"slice({})\"\n",
    "\n",
    "    def __init__(self, a, key, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.key = key\n",
    "        self.set_out(self.args[0].data[key])\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        p = self.parents[0]\n",
    "\n",
    "        if not p.requires_grad:\n",
    "            return\n",
    "\n",
    "        if p.grad is None:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "        p.grad[self.key] += self.out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LessThan(BinaryElementwiseOp):\n",
    "#     name_template = \"({}<{})\"\n",
    "\n",
    "#     def __init__(self, a, b, name=None):\n",
    "#         super().__init__(a, b, name=name)\n",
    "#         self.out = Tensor(\n",
    "#             data=self.args[0].data < self.args[1].data, name=self.name, op=self\n",
    "#         )\n",
    "\n",
    "#     # def backward(self):\n",
    "#     #     self.parents[0].accum_grad(self.out.grad * (self.parents[0].data < self.parents[1].data)\n",
    "#     #     self.parents[1].accum_grad(self.out.grad * (self.parents[0].data >= self.parents[1].data)\n",
    "\n",
    "# class Where(BaseOp):\n",
    "#     name_template = \"where({})\"\n",
    "\n",
    "#     def __init__(self, a, b, c, name=None):\n",
    "#         super().__init__(a, b, c, name=name)\n",
    "#         self.parents = self.args\n",
    "#         self.out = Tensor(\n",
    "#             data=np.where(self.args[0].data, self.args[1].data, self.args[2].data),\n",
    "#             name=self.name,\n",
    "#             op=self,\n",
    "#         )\n",
    "\n",
    "#     def backward(self):\n",
    "#         # self.parents[0].accum_grad(self.out.grad * self.parents[1].data\n",
    "#         # self.parents[0].accum_grad(self.out.grad * self.parents[2].data\n",
    "\n",
    "#         self.parents[1].accum_grad(self.out.grad * self.parents[0].data\n",
    "#         self.parents[2].accum_grad(self.out.grad * (1 - self.parents[0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Transpose(UnaryElementwiseOp):\n",
    "    \"\"\"Transpose a tensor\"\"\"\n",
    "\n",
    "    name_template = \"transpose({})\"\n",
    "\n",
    "    def __init__(self, a, dim0, dim1, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.set_out(np.swapaxes(self.args[0].data, dim0, dim1))\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Dropout(UnaryElementwiseOp):\n",
    "    \"\"\"Apply Dropout to a tensor\"\"\"\n",
    "\n",
    "    name_template = \"dropout({})\"\n",
    "\n",
    "    def __init__(self, a, p_drop=0.1, training=True, name=None):\n",
    "        if p_drop == 0:\n",
    "            return a\n",
    "\n",
    "        super().__init__(a, name=name)\n",
    "        assert 0 < p_drop < 1, f\"p_drop must in (0, 1), got {p_drop}\"\n",
    "        self.p_drop = p_drop\n",
    "        self.training = training\n",
    "        if training:\n",
    "            # Note: We scale up the outputs during training rather than scaling down during inference.\n",
    "            scale_factor = 1 / (1-p_drop)\n",
    "            self.mask = np.random.binomial(scale_factor, 1 - p_drop, size=self.args[0].data.shape)\n",
    "            self.set_out(self.args[0].data * self.mask)\n",
    "        else:\n",
    "            self.set_out(self.args[0].data)\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].grad += self.out.grad * (self.mask if self.training else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class Embedding(UnaryElementwiseOp):\n",
    "    \"\"\"Embedding layer\"\"\"\n",
    "\n",
    "    name_template = \"embedding({})\"\n",
    "\n",
    "    def __init__(self, a, indices, name=None):\n",
    "        super().__init__(a, name=name)\n",
    "        self.indices = indices\n",
    "        self.set_out(self.args[0].data[self.indices])\n",
    "\n",
    "    def backward(self):\n",
    "        self.check_backward()\n",
    "        self.parents[0].grad[self.indices] += self.out.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
