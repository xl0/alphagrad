{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad as tg\n",
    "from tidygrad import Tensor\n",
    "import numpy as np\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = datasets.load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 1024\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "ndim = 512\n",
    "ctx_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_new(n_vocab, n_layers, n_heads, ndim):\n",
    "    shape_dict = {\n",
    "        \"wte\": [n_vocab, ndim],\n",
    "        \"wpe\": [ctx_len, ndim],\n",
    "        \"ln_f.weight\": [ndim],\n",
    "        \"ln_f.bias\": [ndim],\n",
    "    }\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        shape_dict[f\"h.{i}.ln_1.weight\"] = [ndim]\n",
    "        shape_dict[f\"h.{i}.ln_1.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.attn.c_attn.weight\"] = [ndim, 3 * ndim]\n",
    "        shape_dict[f\"h.{i}.attn.c_attn.bias\"] = [3 * ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.attn.c_proj.weight\"] = [ndim, ndim]\n",
    "        shape_dict[f\"h.{i}.attn.c_proj.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.ln_2.weight\"] = [ndim]\n",
    "        shape_dict[f\"h.{i}.ln_2.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.mlp.c_fc.weight\"] = [ndim, 4 * ndim]\n",
    "        shape_dict[f\"h.{i}.mlp.c_fc.bias\"] = [4 * ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.mlp.c_proj.weight\"] = [4 * ndim, ndim]\n",
    "        shape_dict[f\"h.{i}.mlp.c_proj.bias\"] = [ndim]\n",
    "\n",
    "    return tg.model.Model(shape_dict)\n",
    "\n",
    "model = gpt2_new(n_vocab=n_vocab, n_layers=n_layers, n_heads=n_heads, ndim=ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = Tensor(123, requires_grad=False)\n",
    "t1 = t + t\n",
    "\n",
    "t1.requires_grad is False\n",
    "t1.parents is []\n",
    "\n",
    "\n",
    "t1.requires_grad(True)\n",
    "\n",
    "t1.requires_grad is True\n",
    "\n",
    "But it has no parents!!!1\n",
    "\n",
    "t1.op should be Load, not Add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_init(model):\n",
    "    for k in model.params.keys():\n",
    "        if k.endswith(\".weight\"):\n",
    "            model.params[k] = Tensor(np.random.randn(*model.params[k].shape), name=k) * 0.02\n",
    "        elif k.endswith(\".bias\"):\n",
    "            model.params[k] = Tensor(np.zeros(model.params[k].shape), name=k)\n",
    "\n",
    "    model.params[\"wte\"] = Tensor(np.random.randn(*model.params[\"wte\"].shape), name=\"wte\") * 0.02\n",
    "    model.params[\"wpe\"] = Tensor(np.random.randn(*model.params[\"wpe\"].shape), name=\"wpe\") * 0.01\n",
    "    \n",
    "\n",
    "gpt2_init(model)\n",
    "model.requires_grad(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad.func as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_transformer_block(model: tg.model.Model, x, n_heads, i):\n",
    "    def get_params(s):\n",
    "        return model.params[f\"h.{i}.{s}\"]\n",
    "\n",
    "    ln_1 = F.layer_norm(x, get_params(\"ln_1.weight\"), get_params(\"ln_1.bias\"))\n",
    "\n",
    "    attn_w_qkv = get_params(\"attn.c_attn.weight\")\n",
    "    attn_b_qkv = get_params(\"attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = attn_w_qkv.split(3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = attn_b_qkv.split(3, axis=-1)\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "\n",
    "\n",
    "    q_chunked = F.stack(q.split(n=n_heads, axis=-1), axis=0)\n",
    "    k_chunked = F.stack(k.split(n=n_heads, axis=-1), axis=0)\n",
    "    v_chunked = F.stack(v.split(n=n_heads, axis=-1), axis=0)\n",
    "\n",
    "    dim = q_chunked.shape[-1]\n",
    "    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(dim / n_heads)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = softmaxed.mmul(v_chunked)\n",
    "    attention_chunks = attention_output.split(axis=0, n=n_heads)\n",
    "    # print(\"attention_chunks\", attention_chunks)\n",
    "\n",
    "    attention_reshaped = F.concat(attention_chunks, axis=-1)\n",
    "    attention_reshaped = attention_reshaped[0]\n",
    "    # print(\"attention_reshaped\", attention_reshaped)\n",
    "\n",
    "    cproj_w = get_params(\"attn.c_proj.weight\")\n",
    "    cproj_b = get_params(\"attn.c_proj.bias\")\n",
    "    # attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + x\n",
    "    # print(\"after_residual\", after_residual)\n",
    "    ln2_w = get_params(\"ln_2.weight\")\n",
    "    ln2_b = get_params(\"ln_2.bias\")\n",
    "\n",
    "    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = get_params(\"mlp.c_fc.weight\")\n",
    "    mlp_c_fc_b = get_params(\"mlp.c_fc.bias\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "    # print(\"after_up\", after_up)\n",
    "\n",
    "    after_up_a = F.gelu(after_up)\n",
    "    # print(\"after_up_a\", after_up_a)\n",
    "\n",
    "    mlp_c_proj_w = get_params(\"mlp.c_proj.weight\")\n",
    "    mlp_c_proj_b = get_params(\"mlp.c_proj.bias\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "def gpt2(model, input, n_layers, n_heads):\n",
    "    def get_params(s):\n",
    "        return model.params[s]\n",
    "\n",
    "    token_embeddings = F.embedding(get_params(\"wte\"), input)\n",
    "    position_embeddings = F.embedding(get_params(\"wpe\"), np.arange(input.shape[-1]))\n",
    "\n",
    "    x = token_embeddings + position_embeddings\n",
    "\n",
    "    # print(\"first embedding\", x)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        print(\"layer\", i)\n",
    "        x = gpt2_transformer_block(model=model, x=x, n_heads=n_heads, i=i)\n",
    "\n",
    "\n",
    "    return F.layer_norm(x, w=get_params(\"ln_f.weight\"), b=get_params(\"ln_f.bias\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = gpt2(model, np.arange(256).reshape(2, -1), n_layers=n_layers, n_heads=n_heads)\n",
    "# res.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tidygrad.training import one_hot_encode_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_batch(y, n_classes):\n",
    "    diag = np.eye(n_classes)\n",
    "    return Tensor(diag[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "layer 1\n"
     ]
    }
   ],
   "source": [
    "def language_modeling_loss(model, input, target, n_layers, n_heads):\n",
    "    res = gpt2(model, input, n_layers, n_heads)\n",
    "    # print(\"res\", res)\n",
    "    # print(\"wte\", model.params[\"wte\"])\n",
    "    logits = res.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n",
    "\n",
    "    # print(\"logits\", logits)\n",
    "    loss = F.CrossEntropy_loss(logits, one_hot_encode_batch(target, n_classes=n_vocab))\n",
    "    return loss\n",
    "\n",
    "\n",
    "loss = language_modeling_loss(\n",
    "    model,\n",
    "    input=np.random.randint(0, n_vocab, size=(2, ctx_len)),\n",
    "    target=np.random.randint(0, n_vocab, size=(2, ctx_len)),\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads\n",
    ")\n",
    "\n",
    "# print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[2, 128, 1](name=\"\" op=Div parents=[,]):\n",
      "    v=array[2, 128, 1] n=256 (2Kb) x∈[0.007, 0.007] μ=0.007 σ=9.689e-06\n",
      "    ∇=array[2, 128, 1] n=256 (2Kb) \u001b[38;2;127;127;127mall_zeros\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.seterr(all=\"raise\")\n",
    "l = loss.sum()\n",
    "print(loss)\n",
    "\n",
    "l.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
