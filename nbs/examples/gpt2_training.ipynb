{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-Nano training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad as tg\n",
    "from tidygrad import Tensor\n",
    "import tidygrad.tensor\n",
    "import numpy as np\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = datasets.load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 1024\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "ndim = 512\n",
    "ctx_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_new(n_vocab, n_layers, n_heads, ndim):\n",
    "    shape_dict = {\n",
    "        \"wte\": [n_vocab, ndim],\n",
    "        \"wpe\": [ctx_len, ndim],\n",
    "        \"ln_f.weight\": [ndim],\n",
    "        \"ln_f.bias\": [ndim],\n",
    "    }\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        shape_dict[f\"h.{i}.ln_1.weight\"] = [ndim]\n",
    "        shape_dict[f\"h.{i}.ln_1.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.attn.c_attn.weight\"] = [ndim, 3 * ndim]\n",
    "        shape_dict[f\"h.{i}.attn.c_attn.bias\"] = [3 * ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.attn.c_proj.weight\"] = [ndim, ndim]\n",
    "        shape_dict[f\"h.{i}.attn.c_proj.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.ln_2.weight\"] = [ndim]\n",
    "        shape_dict[f\"h.{i}.ln_2.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.mlp.c_fc.weight\"] = [ndim, 4 * ndim]\n",
    "        shape_dict[f\"h.{i}.mlp.c_fc.bias\"] = [4 * ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.mlp.c_proj.weight\"] = [4 * ndim, ndim]\n",
    "        shape_dict[f\"h.{i}.mlp.c_proj.bias\"] = [ndim]\n",
    "\n",
    "    return tg.model.Model(shape_dict)\n",
    "\n",
    "model = gpt2_new(n_vocab=n_vocab, n_layers=n_layers, n_heads=n_heads, ndim=ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = Tensor(123, requires_grad=False)\n",
    "t1 = t + t\n",
    "\n",
    "t1.requires_grad is False\n",
    "t1.parents is []\n",
    "\n",
    "t1.requires_grad(True)\n",
    "\n",
    "t1.requires_grad is True\n",
    "\n",
    "But it has no parents!!!1\n",
    "\n",
    "t1.op should be Load, not Add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_init(model):\n",
    "    for k in model.params.keys():\n",
    "        if k.endswith(\".weight\"):\n",
    "            model.params[k] = Tensor(np.random.randn(*model.params[k].shape), name=k) * 0.02\n",
    "        elif k.endswith(\".bias\"):\n",
    "            model.params[k] = Tensor(np.zeros(model.params[k].shape), name=k)\n",
    "\n",
    "    model.params[\"wte\"] = Tensor(np.random.randn(*model.params[\"wte\"].shape), name=\"wte\") * 0.02\n",
    "    model.params[\"wpe\"] = Tensor(np.random.randn(*model.params[\"wpe\"].shape), name=\"wpe\") * 0.01\n",
    "\n",
    "gpt2_init(model)\n",
    "model.requires_grad(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidygrad.tensor._num_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad.func as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_transformer_block(model: tg.model.Model, x, n_heads, i):\n",
    "    def get_params(s):\n",
    "        return model.params[f\"h.{i}.{s}\"]\n",
    "\n",
    "    ln_1 = F.layer_norm(x, get_params(\"ln_1.weight\"), get_params(\"ln_1.bias\"))\n",
    "\n",
    "    attn_w_qkv = get_params(\"attn.c_attn.weight\")\n",
    "    attn_b_qkv = get_params(\"attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = attn_w_qkv.split(3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = attn_b_qkv.split(3, axis=-1)\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "    q_chunked = F.stack(q.split(n=n_heads, axis=-1), axis=0)\n",
    "    k_chunked = F.stack(k.split(n=n_heads, axis=-1), axis=0)\n",
    "    v_chunked = F.stack(v.split(n=n_heads, axis=-1), axis=0)\n",
    "\n",
    "    dim = q_chunked.shape[-1]\n",
    "    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(dim / n_heads)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = softmaxed.mmul(v_chunked)\n",
    "    attention_chunks = attention_output.split(axis=0, n=n_heads)\n",
    "    # print(\"attention_chunks\", attention_chunks)\n",
    "\n",
    "    attention_reshaped = F.concat(attention_chunks, axis=-1)\n",
    "    attention_reshaped = attention_reshaped[0]\n",
    "    # print(\"attention_reshaped\", attention_reshaped)\n",
    "\n",
    "    cproj_w = get_params(\"attn.c_proj.weight\")\n",
    "    cproj_b = get_params(\"attn.c_proj.bias\")\n",
    "    # attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + x\n",
    "    # print(\"after_residual\", after_residual)\n",
    "    ln2_w = get_params(\"ln_2.weight\")\n",
    "    ln2_b = get_params(\"ln_2.bias\")\n",
    "\n",
    "    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = get_params(\"mlp.c_fc.weight\")\n",
    "    mlp_c_fc_b = get_params(\"mlp.c_fc.bias\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "    # print(\"after_up\", after_up)\n",
    "\n",
    "    after_up_a = F.gelu(after_up)\n",
    "    # print(\"after_up_a\", after_up_a)\n",
    "\n",
    "    mlp_c_proj_w = get_params(\"mlp.c_proj.weight\")\n",
    "    mlp_c_proj_b = get_params(\"mlp.c_proj.bias\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "def gpt2(model, input, n_layers, n_heads):\n",
    "    def get_params(s):\n",
    "        return model.params[s]\n",
    "\n",
    "    token_embeddings = F.embedding(get_params(\"wte\"), input)\n",
    "    position_embeddings = F.embedding(get_params(\"wpe\"), np.arange(input.shape[-1]))\n",
    "\n",
    "    x = token_embeddings + position_embeddings\n",
    "\n",
    "    # print(\"first embedding\", x)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        print(\"layer\", i)\n",
    "        x = gpt2_transformer_block(model=model, x=x, n_heads=n_heads, i=i)\n",
    "\n",
    "    return F.layer_norm(x, w=get_params(\"ln_f.weight\"), b=get_params(\"ln_f.bias\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = gpt2(model, np.arange(256).reshape(2, -1), n_layers=n_layers, n_heads=n_heads)\n",
    "# res.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tidygrad.training import one_hot_encode_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(batch, n_classes):\n",
    "    batch_size, sequence_length = batch.shape\n",
    "    one_hot = np.zeros((batch_size, sequence_length, n_classes))\n",
    "    rows, cols = np.indices((batch_size, sequence_length))\n",
    "    one_hot[rows, cols, batch] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_modeling_loss(model, input, target, n_layers, n_heads):\n",
    "    res = gpt2(model, input, n_layers, n_heads)\n",
    "    # print(\"res\", res)\n",
    "    # print(\"wte\", model.params[\"wte\"])\n",
    "    logits = res.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n",
    "\n",
    "    # print(\"logits\", logits)\n",
    "    loss = F.CrossEntropy_loss(logits, one_hot_encode(target, n_classes=n_vocab))\n",
    "    return loss\n",
    "\n",
    "# loss = language_modeling_loss(\n",
    "#     model, input=np.random.randint(0, n_vocab, size=(2, ctx_len)), target=np.random.randint(0, n_vocab, size=(2, ctx_len)), n_layers=n_layers, n_heads=n_heads\n",
    "# )\n",
    "\n",
    "# print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(all=\"raise\")\n",
    "# l = loss.sum()\n",
    "# print(loss)\n",
    "\n",
    "# l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"datasets/TinyStories/TinyStories.txt\", \"r\") as file:\n",
    "#     tokens = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset:\n",
    "\n",
    "# dataset = [\"Lilly gsdsgfsdfsd sf sfds\"] <- You can no sample from ths\n",
    "\n",
    "# dataset = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15.....]\n",
    "\n",
    "# ctx len = 5\n",
    "\n",
    "# dataset[0] = [1,2,3,4,5]\n",
    "# dataset[1] = [2,3,4,5,6]\n",
    "# dataset[2] = [3,4,5,6,7]\n",
    "# dataset[3] = [4,5,6,7,8]\n",
    "\n",
    "from tidygrad.utils.datasets import Dataset, DataLoader\n",
    "\n",
    "tokens = np.load(\"./datasets/TinyStories/TinyStories_1percent_ids.npy\")\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    def __init__(self, token_array, ctx_len):\n",
    "        self.token_array = token_array\n",
    "        self.ctx_len = ctx_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_array) - self.ctx_len - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.token_array[i:i + self.ctx_len], self.token_array[i + 1:i + self.ctx_len + 1]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # print(\"batch\", batch) # [(x1, y1), (x2, y2), (x3, y3)]\n",
    "        return np.stack([x for x, y in batch]), np.stack([y for x, y in batch])\n",
    "\n",
    "dataset = TSDataset(tokens, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, batch_tfms=None, ctx_len=128, fake_epoch_len=50, seed=1337):\n",
    "        super().__init__(dataset=dataset, batch_size=batch_size, batch_tfms=batch_tfms)\n",
    "        self.fake_epoch_len = fake_epoch_len\n",
    "        self.ctx_len = ctx_len\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min((len(self.dataset) // self.batch_size) // self.ctx_len, self.fake_epoch_len)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= min(len(self), self.fake_epoch_len):\n",
    "            raise StopIteration\n",
    "\n",
    "        idxs = self.rng.integers(0, len(self.dataset), size=(self.batch_size, ))\n",
    "\n",
    "        batch = [self.dataset[i] for i in idxs]\n",
    "        batch = self.dataset.collate_fn(batch)\n",
    "\n",
    "        self.i += 1\n",
    "\n",
    "        return batch\n",
    "\n",
    "dataloader = TSDataLoader(dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.utils.data import DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (64, 2)\n",
      "y (64, 2)\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(dataloader))\n",
    "\n",
    "print(\"X\", X.shape)\n",
    "print(\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.training import Learner\n",
    "\n",
    "from tidygrad.optim import Adam\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, y):\n",
    "    # y = Tensor(y)\n",
    "    logits = X.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n",
    "\n",
    "    # print(\"X\", X)\n",
    "    # print(\"y\", y)\n",
    "    # print(\"logits\", logits)\n",
    "\n",
    "    one_one_hot = one_hot_encode(y, n_vocab)\n",
    "\n",
    "    loss = F.CrossEntropy_loss(logits, one_one_hot, reduction=\"sum\")\n",
    "\n",
    "    print(\"loss\", loss)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    print(\"post_epoch num tensors\", tidygrad.tensor._num_tensors)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.training import DictLoggerCallback, ProgressBarCallback, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneBatchCallback:\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "\n",
    "    def post_loss(self, learner):\n",
    "        print(\"post_batch_backward\", self.i)\n",
    "        if self.i == 1:\n",
    "            raise Exception(\"post_batch_backward\")\n",
    "        self.i += 1\n",
    "\n",
    "class MemleakCallback:\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        print(\"init\")\n",
    "\n",
    "    def post_epoch(self, learner):\n",
    "        print(\"post_epoch num tensors\", tidygrad.tensor._num_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "model_funct = partial(gpt2, n_layers=n_layers, n_heads=n_heads)\n",
    "\n",
    "def model_funct(input):\n",
    "    return gpt2(model, input, n_layers=n_layers, n_heads=n_heads)\n",
    "\n",
    "optim = Adam(lr=0.001, params=model.parameter_list())\n",
    "\n",
    "ler = Learner(\n",
    "    model=model_funct,\n",
    "    dataloaders=DataLoaders(train=dataloader, test=dataloader),\n",
    "    loss_func=loss_function,\n",
    "    optimizer=optim,\n",
    "    callbacks=[DictLoggerCallback(metrics=[Loss()]),\n",
    "               ProgressBarCallback(metrics=[\n",
    "                   \"loss\", \n",
    "               ], plot_train_skip_ylim=15, plot_smooth_training=5),\n",
    "               MemleakCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbaf7ca2a4448798396c8bf40c8d7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56c900a91724d4398018883da280c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "|          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><pre>Ep  | loss       | val_loss  \n",
       "</pre></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.908, 6.953] μ=6.931 σ=0.009\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.859, 6.951] μ=6.925 σ=0.016\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.842, 6.959] μ=6.913 σ=0.029\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/common.py:190: RuntimeWarning: underflow encountered in multiply\n",
      "  self.parents[1].accum_grad(self.out.grad * self.parents[0].data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.792, 6.958] μ=6.903 σ=0.036\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.778, 6.972] μ=6.898 σ=0.048\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.732, 7.001] μ=6.886 σ=0.058\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.722, 6.983] μ=6.874 σ=0.065\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.690, 6.980] μ=6.876 σ=0.072\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.654, 6.980] μ=6.846 σ=0.089\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.616, 7.003] μ=6.837 σ=0.088\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.582, 7.011] μ=6.819 σ=0.113\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.539, 7.025] μ=6.810 σ=0.116\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.502, 7.034] μ=6.806 σ=0.140\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.454, 7.036] μ=6.739 σ=0.152\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.405, 7.077] μ=6.742 σ=0.180\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.353, 7.075] μ=6.723 σ=0.188\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.297, 7.135] μ=6.711 σ=0.210\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.235, 7.099] μ=6.649 σ=0.229\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.170, 7.114] μ=6.679 σ=0.255\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.106, 7.183] μ=6.623 σ=0.291\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[6.031, 7.202] μ=6.594 σ=0.319\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.958, 7.233] μ=6.567 σ=0.339\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.881, 7.248] μ=6.526 σ=0.370\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.802, 7.284] μ=6.502 σ=0.424\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.719, 7.288] μ=6.453 σ=0.409\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.626, 7.339] μ=6.376 σ=0.449\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.531, 7.442] μ=6.382 σ=0.517\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.437, 7.460] μ=6.409 σ=0.536\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.338, 7.498] μ=6.301 σ=0.585\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.236, 7.532] μ=6.301 σ=0.591\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.130, 7.617] μ=6.231 σ=0.674\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[5.022, 7.613] μ=6.274 σ=0.697\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[4.912, 7.725] μ=6.229 σ=0.735\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[4.794, 7.797] μ=6.069 σ=0.782\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[4.675, 7.856] μ=6.163 σ=0.873\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n",
      "layer 0\n",
      "layer 1\n",
      "loss Tensor[64, 2, 1](name=\"\" op=Sum parents=[]):\n",
      "    v=array[64, 2, 1] f32 n=128 x∈[4.557, 7.870] μ=5.978 σ=0.844\n",
      "    ∇=array[64, 2, 1] f32 n=128 \u001b[38;2;127;127;127mall_zeros\u001b[0m\n",
      "post_epoch num tensors 325\n"
     ]
    }
   ],
   "source": [
    "ler.fit(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "print(tidygrad.tensor._num_tensors)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "print(tidygrad.tensor._num_tensors)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
