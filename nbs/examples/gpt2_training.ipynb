{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2-Nano training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad as tg\n",
    "from tidygrad import Tensor\n",
    "import tidygrad.tensor\n",
    "import numpy as np\n",
    "\n",
    "import huggingface_hub\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = datasets.load_dataset(\"roneneldan/TinyStories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = 1024\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "ndim = 128\n",
    "ctx_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_new(n_vocab, n_layers, n_heads, ndim):\n",
    "    shape_dict = {\n",
    "        \"wte\": [n_vocab, ndim],\n",
    "        \"wpe\": [ctx_len, ndim],\n",
    "        \"ln_f.weight\": [ndim],\n",
    "        \"ln_f.bias\": [ndim],\n",
    "    }\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        shape_dict[f\"h.{i}.ln_1.weight\"] = [ndim]\n",
    "        shape_dict[f\"h.{i}.ln_1.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.attn.c_attn.weight\"] = [ndim, 3 * ndim]\n",
    "        shape_dict[f\"h.{i}.attn.c_attn.bias\"] = [3 * ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.attn.c_proj.weight\"] = [ndim, ndim]\n",
    "        shape_dict[f\"h.{i}.attn.c_proj.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.ln_2.weight\"] = [ndim]\n",
    "        shape_dict[f\"h.{i}.ln_2.bias\"] = [ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.mlp.c_fc.weight\"] = [ndim, 4 * ndim]\n",
    "        shape_dict[f\"h.{i}.mlp.c_fc.bias\"] = [4 * ndim]\n",
    "\n",
    "        shape_dict[f\"h.{i}.mlp.c_proj.weight\"] = [4 * ndim, ndim]\n",
    "        shape_dict[f\"h.{i}.mlp.c_proj.bias\"] = [ndim]\n",
    "\n",
    "    return tg.model.Model(shape_dict)\n",
    "\n",
    "model = gpt2_new(n_vocab=n_vocab, n_layers=n_layers, n_heads=n_heads, ndim=ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = Tensor(123, requires_grad=False)\n",
    "t1 = t + t\n",
    "\n",
    "t1.requires_grad is False\n",
    "t1.parents is []\n",
    "\n",
    "t1.requires_grad(True)\n",
    "\n",
    "t1.requires_grad is True\n",
    "\n",
    "But it has no parents!!!1\n",
    "\n",
    "t1.op should be Load, not Add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_init(model):\n",
    "    for k in model.params.keys():\n",
    "        if k.endswith(\".weight\"):\n",
    "            model.params[k] = Tensor(np.random.randn(*model.params[k].shape), name=k) * 0.02\n",
    "        elif k.endswith(\".bias\"):\n",
    "            model.params[k] = Tensor(np.zeros(model.params[k].shape), name=k)\n",
    "\n",
    "    model.params[\"wte\"] = Tensor(np.random.randn(*model.params[\"wte\"].shape), name=\"wte\") * 0.02\n",
    "    model.params[\"wpe\"] = Tensor(np.random.randn(*model.params[\"wpe\"].shape), name=\"wpe\") * 0.01\n",
    "\n",
    "gpt2_init(model)\n",
    "model.requires_grad(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tg.model.Model(\"model.safetensors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidygrad.tensor._num_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad.func as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_transformer_block(model: tg.model.Model, x, n_heads, i):\n",
    "    def get_params(s):\n",
    "        return model.params[f\"h.{i}.{s}\"]\n",
    "\n",
    "    ln_1 = F.layer_norm(x, get_params(\"ln_1.weight\"), get_params(\"ln_1.bias\"))\n",
    "\n",
    "    attn_w_qkv = get_params(\"attn.c_attn.weight\")\n",
    "    attn_b_qkv = get_params(\"attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = attn_w_qkv.split(3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = attn_b_qkv.split(3, axis=-1)\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "    q_chunked = F.stack(q.split(n=n_heads, axis=-1), axis=0)\n",
    "    k_chunked = F.stack(k.split(n=n_heads, axis=-1), axis=0)\n",
    "    v_chunked = F.stack(v.split(n=n_heads, axis=-1), axis=0)\n",
    "\n",
    "    dim = q_chunked.shape[-1]\n",
    "    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(dim / n_heads)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = softmaxed.mmul(v_chunked)\n",
    "    attention_chunks = attention_output.split(axis=0, n=n_heads)\n",
    "    # print(\"attention_chunks\", attention_chunks)\n",
    "\n",
    "    attention_reshaped = F.concat(attention_chunks, axis=-1)\n",
    "    attention_reshaped = attention_reshaped[0]\n",
    "    # print(\"attention_reshaped\", attention_reshaped)\n",
    "\n",
    "    cproj_w = get_params(\"attn.c_proj.weight\")\n",
    "    cproj_b = get_params(\"attn.c_proj.bias\")\n",
    "    # attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + x\n",
    "    # print(\"after_residual\", after_residual)\n",
    "    ln2_w = get_params(\"ln_2.weight\")\n",
    "    ln2_b = get_params(\"ln_2.bias\")\n",
    "\n",
    "    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = get_params(\"mlp.c_fc.weight\")\n",
    "    mlp_c_fc_b = get_params(\"mlp.c_fc.bias\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "    # print(\"after_up\", after_up)\n",
    "\n",
    "    after_up_a = F.gelu(after_up)\n",
    "    # print(\"after_up_a\", after_up_a)\n",
    "\n",
    "    mlp_c_proj_w = get_params(\"mlp.c_proj.weight\")\n",
    "    mlp_c_proj_b = get_params(\"mlp.c_proj.bias\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "def gpt2(model, input, n_layers, n_heads):\n",
    "    def get_params(s):\n",
    "        return model.params[s]\n",
    "    \n",
    "    input = np.array(input)\n",
    "\n",
    "    token_embeddings = F.embedding(get_params(\"wte\"), input)\n",
    "    position_embeddings = F.embedding(get_params(\"wpe\"), np.arange(input.shape[-1]))\n",
    "\n",
    "    x = token_embeddings + position_embeddings\n",
    "\n",
    "    # print(\"first embedding\", x)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        # print(\"layer\", i)\n",
    "        x = gpt2_transformer_block(model=model, x=x, n_heads=n_heads, i=i)\n",
    "\n",
    "    return F.layer_norm(x, w=get_params(\"ln_f.weight\"), b=get_params(\"ln_f.bias\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = gpt2(model, np.arange(256).reshape(2, -1), n_layers=n_layers, n_heads=n_heads)\n",
    "# res.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tidygrad.training import one_hot_encode_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(batch, n_classes):\n",
    "    batch_size, sequence_length = batch.shape\n",
    "    one_hot = np.zeros((batch_size, sequence_length, n_classes))\n",
    "    rows, cols = np.indices((batch_size, sequence_length))\n",
    "    one_hot[rows, cols, batch] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_modeling_loss(model, input, target, n_layers, n_heads):\n",
    "    res = gpt2(model, input, n_layers, n_heads)\n",
    "    # print(\"res\", res)\n",
    "    # print(\"wte\", model.params[\"wte\"])\n",
    "    logits = res.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n",
    "\n",
    "    # print(\"logits\", logits)\n",
    "    loss = F.CrossEntropy_loss(logits, one_hot_encode(target, n_classes=n_vocab))\n",
    "    return loss\n",
    "\n",
    "# loss = language_modeling_loss(\n",
    "#     model, input=np.random.randint(0, n_vocab, size=(2, ctx_len)), target=np.random.randint(0, n_vocab, size=(2, ctx_len)), n_layers=n_layers, n_heads=n_heads\n",
    "# )\n",
    "\n",
    "# print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.seterr(all=\"raise\")\n",
    "# l = loss.sum()\n",
    "# print(loss)\n",
    "\n",
    "# l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"datasets/TinyStories/TinyStories.txt\", \"r\") as file:\n",
    "#     tokens = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset:\n",
    "\n",
    "# dataset = [\"Lilly gsdsgfsdfsd sf sfds\"] <- You can no sample from ths\n",
    "\n",
    "# dataset = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15.....]\n",
    "\n",
    "# ctx len = 5\n",
    "\n",
    "# dataset[0] = [1,2,3,4,5]\n",
    "# dataset[1] = [2,3,4,5,6]\n",
    "# dataset[2] = [3,4,5,6,7]\n",
    "# dataset[3] = [4,5,6,7,8]\n",
    "\n",
    "from tidygrad.utils.datasets import Dataset, DataLoader\n",
    "\n",
    "tokens = np.load(\"./datasets/TinyStories/TinyStories_1percent_ids.npy\")\n",
    "\n",
    "class TSDataset(Dataset):\n",
    "    def __init__(self, token_array, ctx_len):\n",
    "        self.token_array = token_array\n",
    "        self.ctx_len = ctx_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_array) - self.ctx_len - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.token_array[i:i + self.ctx_len], self.token_array[i + 1:i + self.ctx_len + 1]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        # print(\"batch\", batch) # [(x1, y1), (x2, y2), (x3, y3)]\n",
    "        return np.stack([x for x, y in batch]), np.stack([y for x, y in batch])\n",
    "\n",
    "dataset = TSDataset(tokens, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, batch_tfms=None, ctx_len=ctx_len, fake_epoch_len=256, seed=1337):\n",
    "        super().__init__(dataset=dataset, batch_size=batch_size, batch_tfms=batch_tfms)\n",
    "        self.fake_epoch_len = fake_epoch_len\n",
    "        self.ctx_len = ctx_len\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return min((len(self.dataset) // self.batch_size) // self.ctx_len, self.fake_epoch_len)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i >= min(len(self), self.fake_epoch_len):\n",
    "            raise StopIteration\n",
    "\n",
    "        idxs = self.rng.integers(0, len(self.dataset), size=(self.batch_size, ))\n",
    "\n",
    "        batch = [self.dataset[i] for i in idxs]\n",
    "        batch = self.dataset.collate_fn(batch)\n",
    "\n",
    "        self.i += 1\n",
    "\n",
    "        return batch\n",
    "\n",
    "dataloader = TSDataLoader(dataset, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.utils.data import DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (128, 2)\n",
      "y (128, 2)\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(dataloader))\n",
    "\n",
    "print(\"X\", X.shape)\n",
    "print(\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.training import Learner\n",
    "\n",
    "from tidygrad.optim import Adam\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad.tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, y):\n",
    "    # y = Tensor(y)\n",
    "    logits = X.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n",
    "\n",
    "    # print(\"X\", X)\n",
    "    # print(\"y\", y)\n",
    "    # print(\"logits\", logits)\n",
    "\n",
    "    one_one_hot = one_hot_encode(y, n_vocab)\n",
    "\n",
    "    loss = F.CrossEntropy_loss(logits, one_one_hot, reduction=\"sum\")\n",
    "\n",
    "    print(\"loss\", loss)\n",
    "    loss = loss.mean()\n",
    "\n",
    "    print(\"post_epoch num tensors\", tidygrad.tensor._num_tensors)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.training import DictLoggerCallback, ProgressBarCallback, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneBatchCallback:\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "\n",
    "    def post_loss(self, learner):\n",
    "        print(\"post_batch_backward\", self.i)\n",
    "        if self.i == 1:\n",
    "            raise Exception(\"post_batch_backward\")\n",
    "        self.i += 1\n",
    "\n",
    "class MemleakCallback:\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        print(\"init\")\n",
    "\n",
    "    def post_epoch(self, learner):\n",
    "        print(\"post_epoch num tensors\", tidygrad.tensor._num_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "model_funct = partial(gpt2, n_layers=n_layers, n_heads=n_heads)\n",
    "\n",
    "def model_funct(input):\n",
    "    return gpt2(model, input, n_layers=n_layers, n_heads=n_heads)\n",
    "\n",
    "optim = Adam(lr=0.001, params=model.parameter_list())\n",
    "\n",
    "ler = Learner(\n",
    "    model=model_funct,\n",
    "    dataloaders=DataLoaders(train=dataloader, test=dataloader),\n",
    "    loss_func=loss_function,\n",
    "    optimizer=optim,\n",
    "    callbacks=[DictLoggerCallback(metrics=[Loss()]),\n",
    "               ProgressBarCallback(metrics=[\n",
    "                   \"loss\", \n",
    "               ], plot_train_skip_ylim=15, plot_smooth_training=5),\n",
    "               MemleakCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ler.fit(epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# json.dump(ler.history, open(\"history.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "TS_PATH = Path(\"./datasets/TinyStories/\")\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(str(TS_PATH / \"wordpiece_1024.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt2_language_modeling(model, input, n_layers, n_heads, temperature=0):\n",
    "    res = gpt2(model, input, n_layers, n_heads)\n",
    "\n",
    "    last_position = res[:, -1, :]\n",
    "\n",
    "    # print(\"wte\", model.params[\"wte\"])\n",
    "    logits = last_position.mmul(model.params[\"wte\"].transpose(-1, -2), name=\"logits\")\n",
    "    return logits, logits.data.argmax(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor[1, 1024](name=\"\" op=Load):\n",
       "     v=array[1, 1024] f32 4Kb x∈[-12.651, 7.492] μ=-3.202 σ=3.977\n",
       "     ,\n",
       " array([16]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_language_modeling(model, [[1,2,3,5]], n_layers=n_layers, n_heads=n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302]\n",
      "=== Generating ===\n",
      "Input:  Once\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e37c6981f3489286b29ff4622d8914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Once upon\n",
      "Output: Once upon a\n",
      "Output: Once upon a time\n",
      "Output: Once upon a time ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_625194/3847700028.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  tokens.append(int(logits.data.argmax(axis=-1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: Once upon a time , a\n",
      "Output: Once upon a time , a time\n",
      "Output: Once upon a time , a time ,\n",
      "Output: Once upon a time , a time , a\n",
      "Output: Once upon a time , a time , a time\n",
      "Output: Once upon a time , a time , a time ,\n",
      "Output: Once upon a time , a time , a time , there\n",
      "Output: Once upon a time , a time , a time , there was\n",
      "Output: Once upon a time , a time , a time , there was a\n",
      "Output: Once upon a time , a time , a time , there was a little\n",
      "Output: Once upon a time , a time , a time , there was a little girl\n",
      "Output: Once upon a time , a time , a time , there was a little girl named\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily .\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily .\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She was\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She was very\n",
      "Output: Once upon a time , a time , a time , there was a little girl named Lily . She was a little girl named Lily . She was very happy\n"
     ]
    }
   ],
   "source": [
    "text = \"Once\"\n",
    "# text = \"<|endoftext|>\"\n",
    "tokens = tokenizer.encode(text).ids  # returns a list of integers\n",
    "print(tokens)\n",
    "print(\"=== Generating ===\")\n",
    "print(\"Input: \", tokenizer.decode(tokens))\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    for i in tqdm(range(30)):\n",
    "        logits, res = gpt2_language_modeling(model, [tokens], n_layers=n_layers, n_heads=n_heads)\n",
    "        tokens.append(int(logits.data.argmax(axis=-1)))\n",
    "        del logits, res\n",
    "        # gc.collect()\n",
    "        # print(tokens)\n",
    "        print(\"Output:\", tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
