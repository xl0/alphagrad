{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad\n",
    "from tidygrad.tensor import Tensor\n",
    "from tidygrad.functional import Embedding, embedding\n",
    "import numpy as np\n",
    "from lovely_numpy import Lo\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "# tokens = Tensor(tokens)\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = safe_open(\"model.safetensors\", framework=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, w, b, eps=1e-5):\n",
    "    mu = x.mean(axis=-1, keepdims=True)\n",
    "    sigma = x.std(axis=-1, keepdims=True, correction=0)\n",
    "\n",
    "    return ((x-mu) / (sigma+eps)) * w + b  #  tensor[10, 768] n=7680 (30Kb) x∈[-0.788, 0.579] μ=-0.005 σ=0.106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.functional import sigmoid, tanh\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x: Tensor):\n",
    "    return x * sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def new_gelu(input):\n",
    "    return (\n",
    "        0.5\n",
    "        * input\n",
    "        * (1.0\n",
    "           + tanh(\n",
    "                math.sqrt(2.0 / math.pi) * (input + 0.044715 * input.pow(3))))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tensor(t: Tensor, axis: int, n: int):\n",
    "    step = t.shape[axis] // n\n",
    "    assert step * n == t.shape[axis], \"Can't split tensor evenly\"\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(n):\n",
    "        start = i * step\n",
    "        end = (i + 1) * step\n",
    "        chunks.append(t[..., start:end])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def stack_tensors(tensors: list, axis=0):\n",
    "\n",
    "    assert axis == 0, \"Only axis=0 is supported for now\"\n",
    "    out_shape = (tensors[0].shape[0] * len(tensors), *tensors[0].shape[1:])\n",
    "\n",
    "    out = Tensor(np.zeros(out_shape))\n",
    "    for i, t in enumerate(tensors):\n",
    "        assert tensors[i].shape[1:] == tensors[0].shape[1:], \"All tensors must have the same shape\"\n",
    "        out[i::len(tensors)] = t\n",
    "    return out\n",
    "\n",
    "\n",
    "def transformer_block(model, i, inputs):\n",
    "    # print(f\" === Block {i} ===\")\n",
    "\n",
    "    ln_1_w = model.get_tensor(f\"h.{i}.ln_1.weight\")\n",
    "    ln_1_b = model.get_tensor(f\"h.{i}.ln_1.bias\")\n",
    "\n",
    "    # ln_1 = embeddings\n",
    "    ln_1 = layer_norm(inputs, ln_1_w, ln_1_b)\n",
    "    # print(\"ln_1\", ln_1)\n",
    "\n",
    "    attn_w_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.weight\")\n",
    "    attn_b_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = np.split(attn_w_qkv, 3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = np.split(attn_b_qkv, 3, axis=-1)\n",
    "\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "    \n",
    "\n",
    "    q_chunked_np = np.split(q.data, 12, axis=-1)\n",
    "    k_chunked_np = np.split(k.data, 12, axis=-1)\n",
    "    v_chunked_np = np.split(v.data, 12, axis=-1)\n",
    "\n",
    "    # q_chunked = split_tensor(q, axis=-1, n=12)\n",
    "    # k_chunked = split_tensor(k, axis=-1, n=12)\n",
    "    # v_chunked = split_tensor(v, axis=-1, n=12)\n",
    "\n",
    "    q_chunked_np = np.stack(q_chunked_np, axis=0)\n",
    "    k_chunked_np = np.stack(k_chunked_np, axis=0)\n",
    "    v_chunked_np = np.stack(v_chunked_np, axis=0)\n",
    "\n",
    "\n",
    "    # q_chunked = (q_chunked, axis=0)\n",
    "    # k_chunked = (k_chunked, axis=0)\n",
    "    # v_chunked = (v_chunked, axis=0)\n",
    "\n",
    "\n",
    "    attention = np.matmul(q_chunked_np, k_chunked_np.swapaxes(-1, -2)) / np.sqrt(64)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)  # * (np.finfo(float).min)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = np.matmul(softmaxed, v_chunked_np)\n",
    "    # print(\"attention_output\", Lo(attention_output))\n",
    "\n",
    "    attention_chunks = attention_output[:]\n",
    "    attention_reshaped_np = np.concatenate(attention_chunks, axis=-1)\n",
    "\n",
    "    cproj_w = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.weight\"))\n",
    "    cproj_b = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.bias\"))\n",
    "\n",
    "    attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + inputs\n",
    "\n",
    "    # print(\"after_residual\", after_residual)\n",
    "\n",
    "    ln2_w = Tensor(model.get_tensor(f\"h.{i}.ln_2.weight\"), name=\"ln2_w\")\n",
    "    ln2_b = Tensor(model.get_tensor(f\"h.{i}.ln_2.bias\"), name=\"ln2_b\")\n",
    "\n",
    "    after_ln2 = layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.weight\"), name=\"fc_w\")\n",
    "    mlp_c_fc_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.bias\"), name=\"fc_b\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "    # print(\"after_up\", after_up)\n",
    "    after_up_a = new_gelu(after_up)\n",
    "\n",
    "    # print(\"after_up_a\", after_up_a)\n",
    "\n",
    "    mlp_c_proj_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.weight\"), name=\"proj_w\")\n",
    "    mlp_c_proj_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.bias\"), name=\"proj_b\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "\n",
    "# res = transformer_block(model, 0, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[10, 768](\" op=Add):\n",
      "    v=array[10, 768] n=7680 (60Kb) x∈[-15.634, 197.272] μ=0.346 σ=6.708\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def transformer(model, tokens):\n",
    "    wte = Tensor(model.get_tensor(\"wte.weight\"))\n",
    "    wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n",
    "\n",
    "    token_embeddings = embedding(wte, tokens)\n",
    "\n",
    "    positions = np.arange(len(tokens))\n",
    "    position_embeddings = embedding(wpe, positions)\n",
    "\n",
    "    embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "    for i in range(12):\n",
    "        embeddings = transformer_block(model, i, embeddings)\n",
    "        # print(\"Embedding out:\", embeddings)\n",
    "        # print(tidygrad.tensor._num_tensors)\n",
    "        # print(tidygrad.tensor._num_ops)\n",
    "\n",
    "    ln_f_w = Tensor(model.get_tensor(\"ln_f.weight\"))\n",
    "    ln_f_b = Tensor(model.get_tensor(\"ln_f.bias\"))\n",
    "\n",
    "    res = layer_norm(embeddings, ln_f_w, ln_f_b)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "tidygrad.tensor._grad = False\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "res = transformer(model, tokens)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[818, 257, 7604, 287, 262, 2323, 612, 5615, 257]\n",
      "Tensor[9, 50257](\" op=Matmul):\n",
      "    v=array[9, 50257] n=452313 (3.5Mb) x∈[-119.274, -27.010] μ=-87.018 σ=20.724\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' man'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "print(tokens)\n",
    "# tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "def gpt2_language_model(model, token_ids):\n",
    "    res = transformer(model, token_ids)\n",
    "\n",
    "    # res = Tensor(res.data[:, -1, :])\n",
    "\n",
    "    wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))\n",
    "    logits = res.mmul(wte)\n",
    "    return logits\n",
    "\n",
    "res = gpt2_language_model(model, tokens)\n",
    "print(res)\n",
    "tokenizer.decode( res.data.argmax(axis=-1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating ===\n",
      "Input:  In a hole in the ground there lived a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: In a hole in the ground there lived a man\n",
      "Output: In a hole in the ground there lived a man who\n",
      "Output: In a hole in the ground there lived a man who had\n",
      "Output: In a hole in the ground there lived a man who had been\n",
      "Output: In a hole in the ground there lived a man who had been killed\n",
      "Output: In a hole in the ground there lived a man who had been killed by\n",
      "Output: In a hole in the ground there lived a man who had been killed by a\n",
      "Output: In a hole in the ground there lived a man who had been killed by a bullet\n",
      "Output: In a hole in the ground there lived a man who had been killed by a bullet.\n",
      "Output: In a hole in the ground there lived a man who had been killed by a bullet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Generating ===\")\n",
    "print(\"Input: \", tokenizer.decode(tokens))\n",
    "for i in range(10):\n",
    "    res = gpt2_language_model(model, tokens)\n",
    "    tokens.append(res.data.argmax(axis=-1)[-1])\n",
    "    print(\"Output:\", tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
