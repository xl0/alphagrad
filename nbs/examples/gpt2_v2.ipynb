{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad\n",
    "from tidygrad.tensor import Tensor\n",
    "\n",
    "import tidygrad.func as F\n",
    "# from tidygrad.func import  embedding, layer_norm, stack, concat\n",
    "import numpy as np\n",
    "from lovely_numpy import Lo\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c https://huggingface.co/gpt2/resolve/main/model.safetensors        -O ./downloaded_weights/gpt2.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-medium/resolve/main/model.safetensors -O ./downloaded_weights/gpt2-medium.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-large/resolve/main/model.safetensors  -O ./downloaded_weights/gpt2-large.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-xl/resolve/main/model.safetensors     -O ./downloaded_weights/gpt2-xl.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2Variant:\n",
    "    def __init__(self, weight_file, n_head, n_layer):\n",
    "        self.weight_file = weight_file\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "\n",
    "gpt2_variants = {\n",
    "    \"gpt2\": Gpt2Variant(\"gpt2.safetensors\", 12, 12),\n",
    "    \"gpt2-medium\": Gpt2Variant(\"gpt2-medium.safetensors\", 16, 24),\n",
    "    \"gpt2-large\": Gpt2Variant(\"gpt2-large.safetensors\", 20, 36),\n",
    "    \"gpt2-xl\": Gpt2Variant(\"gpt2-xl.safetensors\", 25, 48),\n",
    "}\n",
    "\n",
    "gpt2_variant = \"gpt2-xl\"\n",
    "weights_dir = \"./downloaded_weights/\"\n",
    "\n",
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_variant)\n",
    "\n",
    "# tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "# tokens = Tensor(tokens)\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = safe_open(weights_dir + gpt2_variants[gpt2_variant].weight_file, framework=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor[50257, 1600](name=\"\" op=Load):\n",
       "    v=array[50257, 1600] f32 n=80411200 (0.3Gb) x∈[-0.325, 0.385] μ=-0.000 σ=0.048\n",
       "    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(model.get_tensor(\"wte.weight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad.func as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Module capabilities:\n",
    "\n",
    "0. Abstract neural network \"modules\", like Linear of Conv2D.\n",
    "\n",
    "1. Assignment tracks parameters\n",
    "\n",
    "class MyModel(Module):\n",
    "    def __init__():\n",
    "        self.w1 = Tensort(...)\n",
    "        self.b2 = Tens....\n",
    "\n",
    "    # w1, b1 are tracked as parameters\n",
    "\n",
    "Then you can call model.parameters() to get a list of parameters.\n",
    "\n",
    "\n",
    "2. Save / load weights. Also, count weights.\n",
    "\n",
    "3. Fun forward/backward pass on the model.\n",
    "\n",
    "\n",
    "#### Pytorch\n",
    "\n",
    "class nn.Linear():\n",
    "    ....\n",
    "\n",
    "class Model(nn.Module):\n",
    "    __init__:\n",
    "        self.l1 = nn.Linear(...)\n",
    "        self.ln = ...\n",
    "    \n",
    "    forward(x):\n",
    "        x = self.l1(x) \n",
    "        x = self.conv(x)\n",
    "        ....\n",
    "        return x\n",
    "\n",
    "model = Model(...)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "#### TidyGrad\n",
    "\n",
    "y = x.mmul(w) + b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelTensors(Dict):\n",
    "    __init__\n",
    "\n",
    "\n",
    "    load(st: safetensor):\n",
    "        for k in st.keys():\n",
    "            self.params[k] = st.get_tensor(k)\n",
    "\n",
    "    save():\n",
    "        .....\n",
    "        return st\n",
    "\n",
    "\n",
    "model = ModelTensors\n",
    "    \n",
    "        \n",
    "a = model[\"h0.ln1.w\"] # Returns Tensor\n",
    "a = models.h0.ln1.w\n",
    "\n",
    "\n",
    "model.parameters() ==> Return list of params\n",
    "\n",
    "\n",
    "optim = SGD(model.params(), lr=9000)\n",
    "\n",
    "def transformer()...\n",
    "\n",
    "loss = transformer(X, y, model)\n",
    "loss.backwards()\n",
    "\n",
    "optim.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(model, i, input, n_head):\n",
    "    dim = input.shape[-1]\n",
    "    assert dim % n_head == 0\n",
    "\n",
    "    ln_1_w = model.get_tensor(f\"h.{i}.ln_1.weight\")\n",
    "    ln_1_b = model.get_tensor(f\"h.{i}.ln_1.bias\")\n",
    "\n",
    "    ln_1 = F.layer_norm(input, ln_1_w, ln_1_b)\n",
    "    # ln_1.ad\n",
    "\n",
    "    attn_w_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.weight\")\n",
    "    attn_b_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = np.split(attn_w_qkv, 3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = np.split(attn_b_qkv, 3, axis=-1)\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "    # q_chunked = split_tensor(q, axis=-1, n=12)\n",
    "    # k_chunked = split_tensor(k, axis=-1, n=12)\n",
    "    # v_chunked = split_tensor(v, axis=-1, n=12)\n",
    "\n",
    "    q_chunked = F.stack(q.split(n=n_head, axis=-1), axis=0)\n",
    "    k_chunked = F.stack(k.split(n=n_head, axis=-1), axis=0)\n",
    "    v_chunked = F.stack(v.split(n=n_head, axis=-1), axis=0)\n",
    "\n",
    "    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(dim / n_head)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = softmaxed.mmul(v_chunked)\n",
    "    attention_chunks = attention_output.split(axis=0, n=n_head)\n",
    "    # print(\"attention_chunks\", attention_chunks)\n",
    "\n",
    "    attention_reshaped = F.concat(attention_chunks, axis=-1)\n",
    "    attention_reshaped = attention_reshaped[0]\n",
    "    # print(\"attention_reshaped\", attention_reshaped)\n",
    "\n",
    "    cproj_w = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.weight\"))\n",
    "    cproj_b = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.bias\"))\n",
    "    # attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + input\n",
    "    # print(\"after_residual\", after_residual)\n",
    "\n",
    "    ln2_w = Tensor(model.get_tensor(f\"h.{i}.ln_2.weight\"), name=\"ln2_w\")\n",
    "    ln2_b = Tensor(model.get_tensor(f\"h.{i}.ln_2.bias\"), name=\"ln2_b\")\n",
    "\n",
    "    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.weight\"), name=\"fc_w\")\n",
    "    mlp_c_fc_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.bias\"), name=\"fc_b\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "    # print(\"after_up\", after_up)\n",
    "\n",
    "    after_up_a = F.gelu(after_up)\n",
    "    # print(\"after_up_a\", after_up_a)\n",
    "\n",
    "    mlp_c_proj_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.weight\"), name=\"proj_w\")\n",
    "    mlp_c_proj_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.bias\"), name=\"proj_b\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "# res = transformer_block(model, 0, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(model, tokens, n_layer, n_head):\n",
    "    wte = Tensor(model.get_tensor(\"wte.weight\"))\n",
    "    wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n",
    "\n",
    "    token_embeddings = F.embedding(wte, tokens)\n",
    "\n",
    "    positions = np.arange(len(tokens))\n",
    "    position_embeddings = F.embedding(wpe, positions)\n",
    "\n",
    "    embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        # print(\"Layer\", i)\n",
    "        embeddings = transformer_block(model, i, embeddings, n_head)\n",
    "        # print(\"Embedding out:\", embeddings)\n",
    "        # print(tidygrad.tensor._num_tensors)\n",
    "        # print(tidygrad.tensor._num_ops)\n",
    "\n",
    "    ln_f_w = Tensor(model.get_tensor(\"ln_f.weight\"))\n",
    "    ln_f_b = Tensor(model.get_tensor(\"ln_f.bias\"))\n",
    "\n",
    "    res = F.layer_norm(embeddings, ln_f_w, ln_f_b)\n",
    "\n",
    "    return res\n",
    "\n",
    "# tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# with tidygrad.no_grad():\n",
    "#     res = transformer(model, tokens, gpt2_variants[gpt2_variant].n_layer, gpt2_variants[gpt2_variant].n_head)\n",
    "#     print(res)\n",
    "\n",
    "# import gc\n",
    "# del res\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50257, 1600)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_tensor(\"wte.weight\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[818, 257, 7604, 287, 262, 2323, 612, 5615, 257]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/activation.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  self.set_out(1 / (1 + np.exp(-self.args[0].data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[1600](name=\"\" op=Slice):\n",
      "    v=array[1600] f32 6.2Kb x∈[-5.825, 4.088] μ=0.007 σ=1.243\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' hob'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_variant)\n",
    "\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "print(tokens)\n",
    "# tokens = list(range(1000))\n",
    "\n",
    "def gpt2_language_model(model, token_ids, n_layer, n_head):\n",
    "    wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))\n",
    "    res = transformer(model, token_ids, n_layer, n_head)\n",
    "\n",
    "    res = res[-1, :]\n",
    "    logits = res.mmul(wte)\n",
    "    return logits, res\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    logits, res = gpt2_language_model(model, tokens, n_layer=gpt2_variants[gpt2_variant].n_layer, n_head=gpt2_variants[gpt2_variant].n_head)\n",
    "    print(res)\n",
    "tokenizer.decode(logits.data.argmax(axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdel\u001b[39;00m logits, res\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "del logits, res\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(np.random.randn(5,5)).data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(5, 5).astype(np.float32)\n",
    "b = np.random.randn(5, 5).astype(np.float32)\n",
    "\n",
    "(a+b).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((1000_000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating ===\n",
      "Input:  In a hole in the ground there lived a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936355aa1482433f88b369374375e9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: In a hole in the ground there lived a hob\n",
      "Output: In a hole in the ground there lived a hobbit\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet,\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands,\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he came\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he came to\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he came to the\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he came to the edge\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he came to the edge of\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he came to the edge of the\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet, big hands, and big ears.\n",
      "\n",
      "He smelled of fish and damp earth.\n",
      "\n",
      "He had a hole in his heart,\n",
      "\n",
      "and he was hungry.\n",
      "\n",
      "One day he came to the edge of the world\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb Cell 20\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m tidygrad\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m)):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         logits, res \u001b[39m=\u001b[39m gpt2_language_model(model, tokens, n_layer\u001b[39m=\u001b[39;49mgpt2_variants[gpt2_variant]\u001b[39m.\u001b[39;49mn_layer, n_head\u001b[39m=\u001b[39;49mgpt2_variants[gpt2_variant]\u001b[39m.\u001b[39;49mn_head)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         tokens\u001b[39m.\u001b[39mappend(logits\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39mdel\u001b[39;00m logits, res\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgpt2_language_model\u001b[39m(model, token_ids, n_layer, n_head):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     wte \u001b[39m=\u001b[39m Tensor(model\u001b[39m.\u001b[39mget_tensor(\u001b[39m\"\u001b[39m\u001b[39mwte.weight\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mswapaxes(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     res \u001b[39m=\u001b[39m transformer(model, token_ids, n_layer, n_head)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     res \u001b[39m=\u001b[39m res[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     logits \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mmmul(wte)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m embeddings \u001b[39m=\u001b[39m token_embeddings \u001b[39m+\u001b[39m position_embeddings\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layer):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m transformer_block(model, i, embeddings, n_head)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# print(\"Embedding out:\", embeddings)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# print(tidygrad.tensor._num_tensors)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# print(tidygrad.tensor._num_ops)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ln_f_w \u001b[39m=\u001b[39m Tensor(model\u001b[39m.\u001b[39mget_tensor(\u001b[39m\"\u001b[39m\u001b[39mln_f.weight\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ln_1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlayer_norm(\u001b[39minput\u001b[39m, ln_1_w, ln_1_b)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# ln_1.ad\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m attn_w_qkv \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mget_tensor(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mh.\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m}\u001b[39;49;00m\u001b[39m.attn.c_attn.weight\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m attn_b_qkv \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_tensor(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mh.\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.attn.c_attn.bias\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2_v2.ipynb#Y133sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m attn_w_q, attn_w_k, attn_w_v \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msplit(attn_w_qkv, \u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "\n",
    "print(\"=== Generating ===\")\n",
    "print(\"Input: \", tokenizer.decode(tokens))\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    for i in tqdm(range(100)):\n",
    "        logits, res = gpt2_language_model(model, tokens, n_layer=gpt2_variants[gpt2_variant].n_layer, n_head=gpt2_variants[gpt2_variant].n_head)\n",
    "        tokens.append(logits.data.argmax(axis=-1))\n",
    "        del logits, res\n",
    "        # gc.collect()\n",
    "        print(\"Output:\", tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
