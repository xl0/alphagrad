{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad\n",
    "from tidygrad.tensor import Tensor\n",
    "\n",
    "import tidygrad.func as F\n",
    "# from tidygrad.func import  embedding, layer_norm, stack, concat\n",
    "import numpy as np\n",
    "from lovely_numpy import Lo\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model weights if needed\n",
    "# !wget -c https://huggingface.co/gpt2/resolve/main/model.safetensors -O gpt2.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-medium/resolve/main/model.safetensors -O gpt2-medium.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-large/resolve/main/model.safetensors -O gpt2-large.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-xl/resolve/main/model.safetensors -O gpt2-xl.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2Variant:\n",
    "    def __init__(self, weight_file, n_head, n_layer):\n",
    "        self.weight_file = weight_file\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "\n",
    "gpt2_variants = {\n",
    "    \"gpt2\": Gpt2Variant(\"gpt2.safetensors\", 12, 12),\n",
    "    \"gpt2-medium\": Gpt2Variant(\"gpt2-medium.safetensors\", 16, 24),\n",
    "    \"gpt2-large\": Gpt2Variant(\"gpt2-large.safetensors\", 20, 36),\n",
    "    \"gpt2-xl\": Gpt2Variant(\"gpt2-xl.safetensors\", 25, 48),\n",
    "}\n",
    "\n",
    "gpt2_variant = \"gpt2-xl\"\n",
    "\n",
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_variant)\n",
    "\n",
    "# tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "# tokens = Tensor(tokens)\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = safe_open(gpt2_variants[gpt2_variant].weight_file, framework=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad.func as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_block(model, i, input, n_head):\n",
    "    dim = input.shape[-1]\n",
    "    assert dim % n_head == 0\n",
    "\n",
    "    ln_1_w = model.get_tensor(f\"h.{i}.ln_1.weight\")\n",
    "    ln_1_b = model.get_tensor(f\"h.{i}.ln_1.bias\")\n",
    "\n",
    "    ln_1 = F.layer_norm(input, ln_1_w, ln_1_b)\n",
    "    # ln_1.ad\n",
    "\n",
    "    attn_w_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.weight\")\n",
    "    attn_b_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = np.split(attn_w_qkv, 3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = np.split(attn_b_qkv, 3, axis=-1)\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "    # q_chunked = split_tensor(q, axis=-1, n=12)\n",
    "    # k_chunked = split_tensor(k, axis=-1, n=12)\n",
    "    # v_chunked = split_tensor(v, axis=-1, n=12)\n",
    "\n",
    "    q_chunked = F.stack(q.split(n=n_head, axis=-1), axis=0)\n",
    "    k_chunked = F.stack(k.split(n=n_head, axis=-1), axis=0)\n",
    "    v_chunked = F.stack(v.split(n=n_head, axis=-1), axis=0)\n",
    "\n",
    "    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(dim / n_head)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = softmaxed.mmul(v_chunked)\n",
    "    attention_chunks = attention_output.split(axis=0, n=n_head)\n",
    "    # print(\"attention_chunks\", attention_chunks)\n",
    "\n",
    "    attention_reshaped = F.concat(attention_chunks, axis=-1)\n",
    "    attention_reshaped = attention_reshaped[0]\n",
    "    # print(\"attention_reshaped\", attention_reshaped)\n",
    "\n",
    "    cproj_w = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.weight\"))\n",
    "    cproj_b = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.bias\"))\n",
    "    # attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + input\n",
    "    # print(\"after_residual\", after_residual)\n",
    "\n",
    "    ln2_w = Tensor(model.get_tensor(f\"h.{i}.ln_2.weight\"), name=\"ln2_w\")\n",
    "    ln2_b = Tensor(model.get_tensor(f\"h.{i}.ln_2.bias\"), name=\"ln2_b\")\n",
    "\n",
    "    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.weight\"), name=\"fc_w\")\n",
    "    mlp_c_fc_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.bias\"), name=\"fc_b\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "    # print(\"after_up\", after_up)\n",
    "\n",
    "    after_up_a = F.gelu(after_up)\n",
    "    # print(\"after_up_a\", after_up_a)\n",
    "\n",
    "    mlp_c_proj_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.weight\"), name=\"proj_w\")\n",
    "    mlp_c_proj_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.bias\"), name=\"proj_b\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "# res = transformer_block(model, 0, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[10, 1600](\" op=Add):\n",
      "    v=array[10, 1600] f32 n=16000 (62Kb) x∈[-5.412, 10.720] μ=0.017 σ=1.065\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def transformer(model, tokens, n_layer, n_head):\n",
    "    wte = Tensor(model.get_tensor(\"wte.weight\"))\n",
    "    wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n",
    "\n",
    "    token_embeddings = F.embedding(wte, tokens)\n",
    "\n",
    "    positions = np.arange(len(tokens))\n",
    "    position_embeddings = F.embedding(wpe, positions)\n",
    "\n",
    "    embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        embeddings = transformer_block(model, i, embeddings, n_head)\n",
    "        # print(\"Embedding out:\", embeddings)\n",
    "        # print(tidygrad.tensor._num_tensors)\n",
    "        # print(tidygrad.tensor._num_ops)\n",
    "\n",
    "    ln_f_w = Tensor(model.get_tensor(\"ln_f.weight\"))\n",
    "    ln_f_b = Tensor(model.get_tensor(\"ln_f.bias\"))\n",
    "\n",
    "    res = F.layer_norm(embeddings, ln_f_w, ln_f_b)\n",
    "\n",
    "    return res\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    res = transformer(model, tokens, gpt2_variants[gpt2_variant].n_layer, gpt2_variants[gpt2_variant].n_head)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[818, 257, 7604, 287, 262, 2323, 612, 5615, 257]\n",
      "Tensor[1600](\" op=Slice):\n",
      "    v=array[1600] f32 6.2Kb x∈[-5.825, 4.088] μ=0.007 σ=1.243\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' hob'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(gpt2_variant)\n",
    "\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "print(tokens)\n",
    "# tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "def gpt2_language_model(model, token_ids, wte, n_layer, n_head):\n",
    "    res = transformer(model, token_ids, n_layer, n_head)\n",
    "\n",
    "    res = res[-1, :]\n",
    "    logits = res.mmul(wte)\n",
    "    return logits, res\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    logits, res = gpt2_language_model(model, tokens, wte, n_layer=gpt2_variants[gpt2_variant].n_layer, n_head=gpt2_variants[gpt2_variant].n_head)\n",
    "    print(res)\n",
    "tokenizer.decode(logits.data.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating ===\n",
      "Input:  In a hole in the ground there lived a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f49317d500b64c9a9a86bb75c17174c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: In a hole in the ground there lived a hob\n",
      "Output: In a hole in the ground there lived a hobbit\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet\n",
      "Output: In a hole in the ground there lived a hobbit.\n",
      "\n",
      "He had big feet,\n"
     ]
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "\n",
    "print(\"=== Generating ===\")\n",
    "print(\"Input: \", tokenizer.decode(tokens))\n",
    "wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    for i in tqdm(range(10)):\n",
    "        logits, res = gpt2_language_model(model, tokens, wte, n_layer=gpt2_variants[gpt2_variant].n_layer, n_head=gpt2_variants[gpt2_variant].n_head)\n",
    "        tokens.append(logits.data.argmax(axis=-1))\n",
    "        print(\"Output:\", tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
