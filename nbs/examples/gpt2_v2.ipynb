{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad\n",
    "from tidygrad.tensor import Tensor\n",
    "\n",
    "import tidygrad.func as F\n",
    "# from tidygrad.func import  embedding, layer_norm, stack, concat\n",
    "import numpy as np\n",
    "from lovely_numpy import Lo\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model weights if needed\n",
    "# !wget -c https://huggingface.co/gpt2/resolve/main/model.safetensors -O gpt2.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-medium/resolve/main/model.safetensors -O gpt2-medium.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-large/resolve/main/model.safetensors -O gpt2-large.safetensors\n",
    "# !wget -c https://huggingface.co/gpt2-xl/resolve/main/model.safetensors -O gpt2-xl.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "# tokens = Tensor(tokens)\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = safe_open(\"gpt2.safetensors\", framework=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tidygrad.func import sigmoid, tanh, stack, concat\n",
    "import tidygrad.func as F\n",
    "\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gelu(x: Tensor):\n",
    "#     return x * sigmoid(1.702 * x)\n",
    "\n",
    "# def new_gelu(input):\n",
    "#     return (0.5 * input * (1.0 + tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * input.pow(3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transformer_block(model, i, inputs):\n",
    "    ln_1_w = model.get_tensor(f\"h.{i}.ln_1.weight\")\n",
    "    ln_1_b = model.get_tensor(f\"h.{i}.ln_1.bias\")\n",
    "\n",
    "    ln_1 = F.layer_norm(inputs, ln_1_w, ln_1_b)\n",
    "    # ln_1.ad\n",
    "\n",
    "    attn_w_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.weight\")\n",
    "    attn_b_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = np.split(attn_w_qkv, 3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = np.split(attn_b_qkv, 3, axis=-1)\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "    # q_chunked = split_tensor(q, axis=-1, n=12)\n",
    "    # k_chunked = split_tensor(k, axis=-1, n=12)\n",
    "    # v_chunked = split_tensor(v, axis=-1, n=12)\n",
    "\n",
    "    q_chunked = F.stack(q.split(n=12, axis=-1 ), axis=0)\n",
    "    k_chunked = F.stack(k.split(n=12, axis=-1 ), axis=0)\n",
    "    v_chunked = F.stack(v.split(n=12, axis=-1 ), axis=0)\n",
    "\n",
    "    attention = q_chunked.mmul(k_chunked.transpose(-1, -2)) / np.sqrt(64)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = softmaxed.mmul(v_chunked)\n",
    "    attention_chunks = attention_output.split( axis=0, n=12)\n",
    "    # print(\"attention_chunks\", attention_chunks)\n",
    "\n",
    "    attention_reshaped = F.concat(attention_chunks, axis=-1)\n",
    "    attention_reshaped = attention_reshaped[0]\n",
    "    # print(\"attention_reshaped\", attention_reshaped)\n",
    "\n",
    "    cproj_w = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.weight\"))\n",
    "    cproj_b = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.bias\"))\n",
    "    # attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + inputs\n",
    "    # print(\"after_residual\", after_residual)\n",
    "\n",
    "    ln2_w = Tensor(model.get_tensor(f\"h.{i}.ln_2.weight\"), name=\"ln2_w\")\n",
    "    ln2_b = Tensor(model.get_tensor(f\"h.{i}.ln_2.bias\"), name=\"ln2_b\")\n",
    "\n",
    "    after_ln2 = F.layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.weight\"), name=\"fc_w\")\n",
    "    mlp_c_fc_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.bias\"), name=\"fc_b\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "    # print(\"after_up\", after_up)\n",
    "\n",
    "    after_up_a = F.gelu(after_up)\n",
    "    # print(\"after_up_a\", after_up_a)\n",
    "\n",
    "    mlp_c_proj_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.weight\"), name=\"proj_w\")\n",
    "    mlp_c_proj_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.bias\"), name=\"proj_b\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "\n",
    "# res = transformer_block(model, 0, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/activation.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  self.set_out(1 / (1 + np.exp(-self.args[0].data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[10, 768](\" op=Add):\n",
      "    v=array[10, 768] f32 n=7680 (30Kb) x∈[-15.634, 197.272] μ=0.346 σ=6.708\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def transformer(model, tokens):\n",
    "    wte = Tensor(model.get_tensor(\"wte.weight\"))\n",
    "    wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n",
    "\n",
    "    token_embeddings = F.embedding(wte, tokens)\n",
    "\n",
    "    positions = np.arange(len(tokens))\n",
    "    position_embeddings = F.embedding(wpe, positions)\n",
    "\n",
    "    embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "    for i in range(12):\n",
    "        embeddings = transformer_block(model, i, embeddings)\n",
    "        # print(\"Embedding out:\", embeddings)\n",
    "        # print(tidygrad.tensor._num_tensors)\n",
    "        # print(tidygrad.tensor._num_ops)\n",
    "\n",
    "    ln_f_w = Tensor(model.get_tensor(\"ln_f.weight\"))\n",
    "    ln_f_b = Tensor(model.get_tensor(\"ln_f.bias\"))\n",
    "\n",
    "    res = F.layer_norm(embeddings, ln_f_w, ln_f_b)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    res = transformer(model, tokens)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[818, 257, 7604, 287, 262, 2323, 612, 5615, 257]\n",
      "Tensor[768](\" op=Slice):\n",
      "    v=array[768] f32 3Kb x∈[-50.634, 188.688] μ=0.388 σ=8.312\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' man'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "print(tokens)\n",
    "# tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "def gpt2_language_model(model, token_ids, wte):\n",
    "    res = transformer(model, token_ids)\n",
    "\n",
    "    res = res[-1, :]\n",
    "    logits = res.mmul(wte)\n",
    "    return logits, res\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    logits, res = gpt2_language_model(model, tokens, wte)\n",
    "    print(res)\n",
    "tokenizer.decode(logits.data.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generating ===\n",
      "Input:  In a hole in the ground there lived a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d5373eb832428ca83d71186f6dda58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xl0/work/projects/grads/tidygrad/tidygrad/ops/activation.py:33: RuntimeWarning: overflow encountered in exp\n",
      "  self.set_out(1 / (1 + np.exp(-self.args[0].data)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: In a hole in the ground there lived a man\n",
      "Output: In a hole in the ground there lived a man who\n",
      "Output: In a hole in the ground there lived a man who had\n",
      "Output: In a hole in the ground there lived a man who had been\n",
      "Output: In a hole in the ground there lived a man who had been killed\n",
      "Output: In a hole in the ground there lived a man who had been killed by\n",
      "Output: In a hole in the ground there lived a man who had been killed by a\n",
      "Output: In a hole in the ground there lived a man who had been killed by a bullet\n",
      "Output: In a hole in the ground there lived a man who had been killed by a bullet.\n",
      "Output: In a hole in the ground there lived a man who had been killed by a bullet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "\n",
    "print(\"=== Generating ===\")\n",
    "print(\"Input: \", tokenizer.decode(tokens))\n",
    "wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))\n",
    "\n",
    "with tidygrad.no_grad():\n",
    "    for i in tqdm(range(10)):\n",
    "        logits, res = gpt2_language_model(model, tokens, wte)\n",
    "        tokens.append(logits.data.argmax(axis=-1))\n",
    "        print(\"Output:\", tokenizer.decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
