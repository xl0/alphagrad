{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.tensor import Tensor\n",
    "from tidygrad.functional import Embedding, embedding\n",
    "import numpy as np\n",
    "from lovely_numpy import Lo\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "# tokens = Tensor(tokens)\n",
    "\n",
    "tokens = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = safe_open(\"model.safetensors\", framework=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h.9.ln_2.bias',\n",
       " 'h.9.ln_2.weight',\n",
       " 'h.9.mlp.c_fc.bias',\n",
       " 'h.9.mlp.c_fc.weight',\n",
       " 'h.9.mlp.c_proj.bias',\n",
       " 'h.9.mlp.c_proj.weight',\n",
       " 'ln_f.bias',\n",
       " 'ln_f.weight',\n",
       " 'wpe.weight',\n",
       " 'wte.weight']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[1024, 768](name=\"?\" op=Load):\n",
      "    v=array[1024, 768] f32 n=786432 (3Mb) x∈[-4.538, 4.065] μ=-0.001 σ=0.123\n",
      "    \n",
      "Tensor[50257, 768](name=\"?\" op=Load):\n",
      "    v=array[50257, 768] f32 n=38597376 (0.1Gb) x∈[-1.270, 1.785] μ=0.000 σ=0.144\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "wte = Tensor(model.get_tensor(\"wte.weight\"))\n",
    "wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n",
    "\n",
    "print(wpe)\n",
    "print(wte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor[10, 768](name=\"(embedding(?)+embedding(?))\" op=Add):\n",
       "    v=array[10, 768] f32 n=7680 (30Kb) x∈[-4.511, 3.938] μ=-9.411e-05 σ=0.219\n",
       "    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = embedding(wte, tokens)\n",
    "\n",
    "positions = np.arange(len(tokens))\n",
    "position_embeddings = embedding(wpe, positions)\n",
    "\n",
    "embeddings = token_embeddings + position_embeddings\n",
    "Lo(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_1_w = model.get_tensor(\"h.0.ln_1.weight\")\n",
    "ln_1_b = model.get_tensor(\"h.0.ln_1.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x, w, b, eps=1e-5):\n",
    "    mu = x.mean(axis=-1, keepdims=True)\n",
    "    sigma = x.std(axis=-1, keepdims=True, correction=0)\n",
    "\n",
    "    return (\n",
    "        (x - mu) / (sigma + eps)\n",
    "    ) * w + b  #  tensor[10, 768] n=7680 (30Kb) x∈[-0.788, 0.579] μ=-0.005 σ=0.106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor[10, 768](name=\"(((((embedding(?)+embedding(?))-(sum((embedding(?)+embedding(?)))/?))/(pow((sum(var)/?),0.5)+?))*?)+?)\" op=Add):\n",
       "    v=array[10, 768] f32 n=7680 (30Kb) x∈[-0.788, 0.579] μ=-0.005 σ=0.106\n",
       "    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_1 = layer_norm(embeddings, ln_1_w, ln_1_b)\n",
    "ln_1\n",
    "\n",
    "#  tensor[10, 768] n=7680 (30Kb) x∈[-0.788, 0.579] μ=-0.005 σ=0.106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_w_qkv = model.get_tensor(\"h.0.attn.c_attn.weight\")\n",
    "attn_b_qkv = model.get_tensor(\"h.0.attn.c_attn.bias\")\n",
    "\n",
    "attn_w_q, attn_w_k, attn_w_v = np.split(attn_w_qkv, 3, axis=-1)\n",
    "attn_b_q, attn_b_k, attn_b_v = np.split(attn_b_qkv, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array[12, 10, 64] f32 n=7680 (30Kb) x∈[-4.234, 4.473] μ=-0.064 σ=0.971\n",
      "array[12, 10, 64] f32 n=7680 (30Kb) x∈[-6.097, 6.787] μ=0.034 σ=1.350\n",
      "array[12, 64, 10] f32 n=7680 (30Kb) x∈[-6.097, 6.787] μ=0.034 σ=1.350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array[12, 10, 10] f32 n=1200 (4.7Kb) x∈[-7.848, 11.893] μ=-0.591 σ=2.526"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "q_chunked_np = np.array_split(q.data, 12, axis=-1)\n",
    "k_chunked_np = np.array_split(k.data, 12, axis=-1)\n",
    "v_chunked_np = np.array_split(v.data, 12, axis=-1)\n",
    "\n",
    "q_chunked_np = np.stack(q_chunked_np, axis=0)\n",
    "k_chunked_np = np.stack(k_chunked_np, axis=0)\n",
    "v_chunked_np = np.stack(v_chunked_np, axis=0)\n",
    "\n",
    "# q_chunked = Tensor(q_chunked_np, name=\"q_chunked\")\n",
    "# k_chunked = Tensor(k_chunked_np, name=\"k_chunked\")\n",
    "# v_chunked = Tensor(v_chunked_np, name=\"v_chunked\")\n",
    "\n",
    "# attention = q_chunked_np.mmul(k_chunked_np.transpose(-1, -2)) / np.sqrt(64)\n",
    "\n",
    "print(Lo(q_chunked_np))\n",
    "print(Lo(k_chunked_np))\n",
    "print(Lo(k_chunked_np.swapaxes(-1, -2)))\n",
    "\n",
    "attention = np.matmul(q_chunked_np, k_chunked_np.swapaxes(-1, -2)) / np.sqrt(64)\n",
    "Lo(attention)\n",
    "\n",
    "# Lo(q_chunked_np).chans(scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array[10, 768] n=7680 (60Kb) x∈[-1.057, 1.432] μ=0.003 σ=0.166"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones(attention.shape), k=0)  # * (np.finfo(float).min)\n",
    "ee = np.exp(attention) * mask\n",
    "\n",
    "softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# print(Lo(softmaxed))\n",
    "\n",
    "attention_output = np.matmul(softmaxed, v_chunked_np)\n",
    "# print(Lo(attention_output))\n",
    "\n",
    "attention_chunks = attention_output[:]\n",
    "Lo(attention_chunks[0])\n",
    "attention_reshaped_np = np.concatenate(attention_chunks, axis=-1)\n",
    "Lo(attention_reshaped_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[10, 768](name=\"((?@?)+?)\" op=Add):\n",
      "    v=array[10, 768] n=7680 (60Kb) x∈[-14.188, 14.257] μ=0.011 σ=1.083\n",
      "    \n",
      "Tensor[10, 768](name=\"(((?@?)+?)+(embedding(?)+embedding(?)))\" op=Add):\n",
      "    v=array[10, 768] n=7680 (60Kb) x∈[-14.241, 14.485] μ=0.011 σ=1.123\n",
      "    \n",
      "Tensor[10, 768](name=\"(((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)\" op=Add):\n",
      "    v=array[10, 768] n=7680 (60Kb) x∈[-2.793, 1.674] μ=0.005 σ=0.160\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "cproj_w_np = model.get_tensor(\"h.0.attn.c_proj.weight\")\n",
    "cproj_b_np = model.get_tensor(\"h.0.attn.c_proj.bias\")\n",
    "\n",
    "cproj_w = Tensor(cproj_w_np)\n",
    "cproj_b = Tensor(cproj_b_np)\n",
    "\n",
    "attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "print(crosstalk)\n",
    "\n",
    "after_residual = crosstalk + embeddings\n",
    "print(after_residual)\n",
    "\n",
    "ln2_w = Tensor(model.get_tensor(\"h.0.ln_2.weight\"), name=\"ln2_w\")\n",
    "ln2_b = Tensor(model.get_tensor(\"h.0.ln_2.bias\"), name=\"ln2_b\")\n",
    "\n",
    "after_ln2 = layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "print(after_ln2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[10, 3072](name=\"(((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)\" op=Add):\n",
      "    v=array[10, 3072] n=30720 (0.2Mb) x∈[-6.346, 10.617] μ=-1.086 σ=0.855\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "mlp_c_fc_w = Tensor(model.get_tensor(\"h.0.mlp.c_fc.weight\"), name=\"fc_w\")\n",
    "mlp_c_fc_b = Tensor(model.get_tensor(\"h.0.mlp.c_fc.bias\"), name=\"fc_b\")\n",
    "\n",
    "after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "\n",
    "print(after_up)\n",
    "# mlp_c_fca = gelu(mlp_c_fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.functional import sigmoid, tanh\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x: Tensor):\n",
    "    return x * sigmoid(1.702 * x)\n",
    "\n",
    "def new_gelu(input):\n",
    "    return (0.5 * input * (1.0 + tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * input.pow(3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor[10, 768](name=\"((((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)*?)*(tanh((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)+(pow((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b),3)*?))*?))+?))@proj_w)+proj_b)+(((?@?)+?)+(embedding(?)+embedding(?))))\" op=Add):\n",
       "    v=array[10, 768] n=7680 (60Kb) x∈[-67.477, 97.448] μ=0.023 σ=2.375\n",
       "    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_up_a = new_gelu(after_up)\n",
    "\n",
    "mlp_c_proj_w = Tensor(model.get_tensor(\"h.0.mlp.c_proj.weight\"), name=\"proj_w\")\n",
    "mlp_c_proj_b = Tensor(model.get_tensor(\"h.0.mlp.c_proj.bias\"), name=\"proj_b\")\n",
    "\n",
    "after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "attention_output = after_down + after_residual\n",
    "attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Block 0 ===\n",
      "ln_1 Tensor[10, 768](name=\"(((((embedding(?)+embedding(?))-(sum((embedding(?)+embedding(?)))/?))/(pow((sum(var)/?),0.5)+?))*?)+?)\" op=Add):\n",
      "    v=array[10, 768] f32 n=7680 (30Kb) x∈[-0.788, 0.579] μ=-0.005 σ=0.106\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def transformer_block(model_weigts, i, inputs):\n",
    "\n",
    "    print(f\" === Block {i} ===\")\n",
    "\n",
    "    ln_1_w = model.get_tensor(f\"h.{i}.ln_1.weight\")\n",
    "    ln_1_b = model.get_tensor(f\"h.{i}.ln_1.bias\")\n",
    "\n",
    "    # ln_1 = embeddings\n",
    "    ln_1 = layer_norm(embeddings, ln_1_w, ln_1_b)\n",
    "    print(\"ln_1\", ln_1)\n",
    "\n",
    "    attn_w_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.weight\")\n",
    "    attn_b_qkv = model.get_tensor(f\"h.{i}.attn.c_attn.bias\")\n",
    "\n",
    "    attn_w_q, attn_w_k, attn_w_v = np.split(attn_w_qkv, 3, axis=-1)\n",
    "    attn_b_q, attn_b_k, attn_b_v = np.split(attn_b_qkv, 3, axis=-1)\n",
    "\n",
    "    q = ln_1.mmul(attn_w_q) + attn_b_q\n",
    "    k = ln_1.mmul(attn_w_k) + attn_b_k\n",
    "    v = ln_1.mmul(attn_w_v) + attn_b_v\n",
    "\n",
    "    q_chunked_np = np.array_split(q.data, 12, axis=-1)\n",
    "    k_chunked_np = np.array_split(k.data, 12, axis=-1)\n",
    "    v_chunked_np = np.array_split(v.data, 12, axis=-1)\n",
    "\n",
    "    q_chunked_np = np.stack(q_chunked_np, axis=0)\n",
    "    k_chunked_np = np.stack(k_chunked_np, axis=0)\n",
    "    v_chunked_np = np.stack(v_chunked_np, axis=0)\n",
    "\n",
    "    attention = np.matmul(q_chunked_np, k_chunked_np.swapaxes(-1, -2)) / np.sqrt(64)\n",
    "\n",
    "    mask = np.tril(np.ones(attention.shape), k=0)  # * (np.finfo(float).min)\n",
    "    ee = np.exp(attention) * mask\n",
    "\n",
    "    softmaxed = ee / ee.sum(axis=-1, keepdims=True)\n",
    "\n",
    "    attention_output = np.matmul(softmaxed, v_chunked_np)\n",
    "    attention_chunks = attention_output[:]\n",
    "    attention_reshaped_np = np.concatenate(attention_chunks, axis=-1)\n",
    "\n",
    "    cproj_w = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.weight\"))\n",
    "    cproj_b = Tensor(model.get_tensor(f\"h.{i}.attn.c_proj.bias\"))\n",
    "\n",
    "    attention_reshaped = Tensor(attention_reshaped_np)\n",
    "\n",
    "    crosstalk = attention_reshaped.mmul(cproj_w) + cproj_b\n",
    "\n",
    "    after_residual = crosstalk + embeddings\n",
    "\n",
    "    ln2_w = Tensor(model.get_tensor(f\"h.{i}.ln_2.weight\"), name=\"ln2_w\")\n",
    "    ln2_b = Tensor(model.get_tensor(f\"h.{i}.ln_2.bias\"), name=\"ln2_b\")\n",
    "\n",
    "    after_ln2 = layer_norm(after_residual, ln2_w, ln2_b)\n",
    "\n",
    "    mlp_c_fc_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.weight\"), name=\"fc_w\")\n",
    "    mlp_c_fc_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_fc.bias\"), name=\"fc_b\")\n",
    "\n",
    "    after_up = after_ln2.mmul(mlp_c_fc_w) + mlp_c_fc_b\n",
    "\n",
    "    after_up_a = new_gelu(after_up)\n",
    "\n",
    "    mlp_c_proj_w = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.weight\"), name=\"proj_w\")\n",
    "    mlp_c_proj_b = Tensor(model.get_tensor(f\"h.{i}.mlp.c_proj.bias\"), name=\"proj_b\")\n",
    "\n",
    "    after_down = after_up_a.mmul(mlp_c_proj_w) + mlp_c_proj_b\n",
    "\n",
    "    output = after_down + after_residual\n",
    "    return output\n",
    "\n",
    "\n",
    "res = transformer_block(model, 0, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Block 0 ===\n",
      "ln_1 Tensor[10, 768](name=\"(((((embedding(?)+embedding(?))-(sum((embedding(?)+embedding(?)))/?))/(pow((sum(var)/?),0.5)+?))*?)+?)\" op=Add):\n",
      "    v=array[10, 768] f32 n=7680 (30Kb) x∈[-0.788, 0.579] μ=-0.005 σ=0.106\n",
      "    \n",
      "Embedding out: Tensor[10, 768](name=\"((((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)*?)*(tanh((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)+(pow((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b),3)*?))*?))+?))@proj_w)+proj_b)+(((?@?)+?)+(embedding(?)+embedding(?))))\" op=Add):\n",
      "    v=array[10, 768] n=7680 (60Kb) x∈[-67.477, 97.448] μ=0.023 σ=2.375\n",
      "    \n",
      " === Block 1 ===\n",
      "ln_1 Tensor[10, 768](name=\"(((((embedding(?)+embedding(?))-(sum((embedding(?)+embedding(?)))/?))/(pow((sum(var)/?),0.5)+?))*?)+?)\" op=Add):\n",
      "    v=array[10, 768] f32 n=7680 (30Kb) x∈[-3.292, 2.614] μ=-0.005 σ=0.247\n",
      "    \n",
      "Embedding out: Tensor[10, 768](name=\"((((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)*?)*(tanh((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)+(pow((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b),3)*?))*?))+?))@proj_w)+proj_b)+(((?@?)+?)+(embedding(?)+embedding(?))))\" op=Add):\n",
      "    v=array[10, 768] n=7680 (60Kb) x∈[-33.394, 360.662] μ=0.620 σ=5.143\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tensor[10, 768](name=\"((((((((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)*?)*(tanh((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)+(pow((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b),3)*?))*?))+?))@proj_w)+proj_b)+(((?@?)+?)+(embedding(?)+embedding(?))))-(sum(((((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)*?)*(tanh((((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b)+(pow((((((((((?@?)+?)+(embedding(?)+embedding(?)))-(sum((((?@?)+?)+(embedding(?)+embedding(?))))/?))/(pow((sum(var)/?),0.5)+?))*ln2_w)+ln2_b)@fc_w)+fc_b),3)*?))*?))+?))@proj_w)+proj_b)+(((?@?)+?)+(embedding(?)+embedding(?)))))/?))/(pow((sum(var)/?),0.5)+?))*?)+?)\" op=Add):\n",
       "    v=array[10, 768] n=7680 (60Kb) x∈[-10.000, 24.001] μ=-0.032 σ=0.977\n",
       "    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer(model, token_ids):\n",
    "    wte = Tensor(model.get_tensor(\"wte.weight\"))\n",
    "    wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n",
    "\n",
    "    token_embeddings = embedding(wte, tokens)\n",
    "\n",
    "    positions = np.arange(len(tokens))\n",
    "    position_embeddings = embedding(wpe, positions)\n",
    "\n",
    "    embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "    for i in range(2):\n",
    "        embeddings = transformer_block(model, i, embeddings)\n",
    "        print(\"Embedding out:\", embeddings)\n",
    "\n",
    "    ln_f_w = Tensor(model.get_tensor(\"ln_f.weight\"))\n",
    "    ln_f_b = Tensor(model.get_tensor(\"ln_f.bias\"))\n",
    "\n",
    "    res = layer_norm(embeddings, ln_f_w, ln_f_b)\n",
    "\n",
    "    return res\n",
    "\n",
    "tidygrad.tensor._grad = True\n",
    "\n",
    "res = transformer(model, tokens)\n",
    "\n",
    "# def gpt2_language_model(model, token_ids):\n",
    "#     res = transformer(model, token_ids)\n",
    "\n",
    "#     wte = Tensor(model.get_tensor(\"wte.weight\").swapaxes(-1, -2))\n",
    "#     logits = res.mmul(wte)\n",
    "#     return logits\n",
    "\n",
    "# res = gpt2_language_model(model, tokens)\n",
    "res\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
