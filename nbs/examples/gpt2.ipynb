{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.tensor import Tensor\n",
    "from tidygrad.functional import Embedding, embedding\n",
    "import numpy as np\n",
    "from lovely_numpy import Lo\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In a hole in the ground there lived a\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokens = tokenizer.encode(text)  # returns a list of integers\n",
    "# tokens = Tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[818, 257, 7604, 287, 262, 2323, 612, 5615, 257]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = safe_open(\"model.safetensors\", framework=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h.9.ln_2.bias',\n",
       " 'h.9.ln_2.weight',\n",
       " 'h.9.mlp.c_fc.bias',\n",
       " 'h.9.mlp.c_fc.weight',\n",
       " 'h.9.mlp.c_proj.bias',\n",
       " 'h.9.mlp.c_proj.weight',\n",
       " 'ln_f.bias',\n",
       " 'ln_f.weight',\n",
       " 'wpe.weight',\n",
       " 'wte.weight']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor[1024, 768](name=\"?\" op=Load):\n",
      "    v=array[1024, 768] f32 n=786432 (3Mb) x∈[-4.538, 4.065] μ=-0.001 σ=0.123\n",
      "    \n",
      "Tensor[50257, 768](name=\"?\" op=Load):\n",
      "    v=array[50257, 768] f32 n=38597376 (0.1Gb) x∈[-1.270, 1.785] μ=0.000 σ=0.144\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "wte = Tensor(model.get_tensor(\"wte.weight\"))\n",
    "wpe = Tensor(model.get_tensor(\"wpe.weight\"))\n",
    "\n",
    "print(wpe)\n",
    "print(wte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tidygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor[9, 768](name=\"(embedding(?)+embedding(?))\" op=Add):\n",
       "    v=array[9, 768] f32 n=6912 (27Kb) x∈[-4.546, 3.674] μ=-0.004 σ=0.216\n",
       "    "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = embedding(wte, tokens)\n",
    "\n",
    "positions = np.arange(len(tokens))\n",
    "position_embeddings = embedding(wpe, positions)\n",
    "\n",
    "embeddings = token_embeddings + position_embeddings\n",
    "Lo(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x,  w, b, eps=1e-5):\n",
    "    mu = x.mean(axis=-1, keepdims=False)\n",
    "    sigma = x.std(axis=-1, keepdims=True)\n",
    "\n",
    "    # return ((x - mu) / (sigma + eps) ) * w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_1_w = model.get_tensor(\"h.0.ln_1.weight\")\n",
    "ln_1_b = model.get_tensor(\"h.0.ln_1.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Replacement index 1 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ln_1 \u001b[39m=\u001b[39m layer_norm(embeddings, ln_1_w, ln_1_b)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlayer_norm\u001b[39m(x,  w, b, eps\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     mu \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/examples/gpt2.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     sigma \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mstd(axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, keepdims\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/work/projects/grads/tidygrad/tidygrad/tensor.py:492\u001b[0m, in \u001b[0;36mTensor.std\u001b[0;34m(self, name, axis, keepdims, correction)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstd\u001b[39m(\u001b[39mself\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, correction\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 492\u001b[0m     var \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean(axis\u001b[39m=\u001b[39;49maxis, keepdims\u001b[39m=\u001b[39;49mkeepdims)) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[1;32m    494\u001b[0m     corrected \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39maxis, keepdims\u001b[39m=\u001b[39mkeepdims) \u001b[39m/\u001b[39m (var\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m correction)\n\u001b[1;32m    496\u001b[0m     \u001b[39mreturn\u001b[39;00m corrected\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m0.5\u001b[39m\n",
      "File \u001b[0;32m~/work/projects/grads/tidygrad/tidygrad/tensor.py:529\u001b[0m, in \u001b[0;36mTensor.__pow__\u001b[0;34m(self, power)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__pow__\u001b[39m(\u001b[39mself\u001b[39m, power):\n\u001b[0;32m--> 529\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpow(power)\n",
      "File \u001b[0;32m~/work/projects/grads/tidygrad/tidygrad/tensor.py:469\u001b[0m, in \u001b[0;36mTensor.pow\u001b[0;34m(self, power, name)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpow\u001b[39m(\u001b[39mself\u001b[39m, power, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 469\u001b[0m     \u001b[39mreturn\u001b[39;00m Pow(\u001b[39mself\u001b[39;49m, power, name\u001b[39m=\u001b[39;49mname)\u001b[39m.\u001b[39mout\n",
      "File \u001b[0;32m~/work/projects/grads/tidygrad/tidygrad/tensor.py:226\u001b[0m, in \u001b[0;36mPow.__init__\u001b[0;34m(self, a, power, name)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, a, power, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 226\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(a, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpower \u001b[39m=\u001b[39m power\n\u001b[1;32m    228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m power)\n",
      "File \u001b[0;32m~/work/projects/grads/tidygrad/tidygrad/tensor.py:125\u001b[0m, in \u001b[0;36mUnaryElementwiseOp.__init__\u001b[0;34m(self, a, name)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, a, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 125\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(a, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad \u001b[39melse\u001b[39;00m []\n",
      "File \u001b[0;32m~/work/projects/grads/tidygrad/tidygrad/tensor.py:89\u001b[0m, in \u001b[0;36mBaseOp.__init__\u001b[0;34m(self, name, *args)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m     79\u001b[0m     name, (\u001b[39mstr\u001b[39m, \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m))\n\u001b[1;32m     80\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mname= should be str, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(name)\u001b[39m}\u001b[39;00m\u001b[39m. You probably meant something else.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m [\n\u001b[1;32m     83\u001b[0m     arg\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg, Tensor)\n\u001b[1;32m     85\u001b[0m     \u001b[39melse\u001b[39;00m Tensor(data\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39masarray(arg, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     86\u001b[0m     \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args\n\u001b[1;32m     87\u001b[0m ]\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname_template\u001b[39m.\u001b[39;49mformat(\u001b[39m*\u001b[39;49m[arg\u001b[39m.\u001b[39;49mname \u001b[39mfor\u001b[39;49;00m arg \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs])\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[39melse\u001b[39;00m name\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(arg\u001b[39m.\u001b[39mrequires_grad \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs) \u001b[39mand\u001b[39;00m _grad\n",
      "\u001b[0;31mIndexError\u001b[0m: Replacement index 1 out of range for positional args tuple"
     ]
    }
   ],
   "source": [
    "ln_1 = layer_norm(embeddings, ln_1_w, ln_1_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h.0.attn.bias',\n",
       " 'h.0.attn.c_attn.bias',\n",
       " 'h.0.attn.c_attn.weight',\n",
       " 'h.0.attn.c_proj.bias',\n",
       " 'h.0.attn.c_proj.weight',\n",
       " 'h.0.ln_1.bias',\n",
       " 'h.0.ln_1.weight',\n",
       " 'h.0.ln_2.bias',\n",
       " 'h.0.ln_2.weight',\n",
       " 'h.0.mlp.c_fc.bias',\n",
       " 'h.0.mlp.c_fc.weight',\n",
       " 'h.0.mlp.c_proj.bias',\n",
       " 'h.0.mlp.c_proj.weight',\n",
       " 'h.1.attn.bias',\n",
       " 'h.1.attn.c_attn.bias',\n",
       " 'h.1.attn.c_attn.weight',\n",
       " 'h.1.attn.c_proj.bias',\n",
       " 'h.1.attn.c_proj.weight',\n",
       " 'h.1.ln_1.bias',\n",
       " 'h.1.ln_1.weight',\n",
       " 'h.1.ln_2.bias',\n",
       " 'h.1.ln_2.weight',\n",
       " 'h.1.mlp.c_fc.bias',\n",
       " 'h.1.mlp.c_fc.weight',\n",
       " 'h.1.mlp.c_proj.bias',\n",
       " 'h.1.mlp.c_proj.weight',\n",
       " 'h.10.attn.bias',\n",
       " 'h.10.attn.c_attn.bias',\n",
       " 'h.10.attn.c_attn.weight',\n",
       " 'h.10.attn.c_proj.bias',\n",
       " 'h.10.attn.c_proj.weight',\n",
       " 'h.10.ln_1.bias',\n",
       " 'h.10.ln_1.weight',\n",
       " 'h.10.ln_2.bias',\n",
       " 'h.10.ln_2.weight',\n",
       " 'h.10.mlp.c_fc.bias',\n",
       " 'h.10.mlp.c_fc.weight',\n",
       " 'h.10.mlp.c_proj.bias',\n",
       " 'h.10.mlp.c_proj.weight',\n",
       " 'h.11.attn.bias',\n",
       " 'h.11.attn.c_attn.bias',\n",
       " 'h.11.attn.c_attn.weight',\n",
       " 'h.11.attn.c_proj.bias',\n",
       " 'h.11.attn.c_proj.weight',\n",
       " 'h.11.ln_1.bias',\n",
       " 'h.11.ln_1.weight',\n",
       " 'h.11.ln_2.bias',\n",
       " 'h.11.ln_2.weight',\n",
       " 'h.11.mlp.c_fc.bias',\n",
       " 'h.11.mlp.c_fc.weight',\n",
       " 'h.11.mlp.c_proj.bias',\n",
       " 'h.11.mlp.c_proj.weight',\n",
       " 'h.2.attn.bias',\n",
       " 'h.2.attn.c_attn.bias',\n",
       " 'h.2.attn.c_attn.weight',\n",
       " 'h.2.attn.c_proj.bias',\n",
       " 'h.2.attn.c_proj.weight',\n",
       " 'h.2.ln_1.bias',\n",
       " 'h.2.ln_1.weight',\n",
       " 'h.2.ln_2.bias',\n",
       " 'h.2.ln_2.weight',\n",
       " 'h.2.mlp.c_fc.bias',\n",
       " 'h.2.mlp.c_fc.weight',\n",
       " 'h.2.mlp.c_proj.bias',\n",
       " 'h.2.mlp.c_proj.weight',\n",
       " 'h.3.attn.bias',\n",
       " 'h.3.attn.c_attn.bias',\n",
       " 'h.3.attn.c_attn.weight',\n",
       " 'h.3.attn.c_proj.bias',\n",
       " 'h.3.attn.c_proj.weight',\n",
       " 'h.3.ln_1.bias',\n",
       " 'h.3.ln_1.weight',\n",
       " 'h.3.ln_2.bias',\n",
       " 'h.3.ln_2.weight',\n",
       " 'h.3.mlp.c_fc.bias',\n",
       " 'h.3.mlp.c_fc.weight',\n",
       " 'h.3.mlp.c_proj.bias',\n",
       " 'h.3.mlp.c_proj.weight',\n",
       " 'h.4.attn.bias',\n",
       " 'h.4.attn.c_attn.bias',\n",
       " 'h.4.attn.c_attn.weight',\n",
       " 'h.4.attn.c_proj.bias',\n",
       " 'h.4.attn.c_proj.weight',\n",
       " 'h.4.ln_1.bias',\n",
       " 'h.4.ln_1.weight',\n",
       " 'h.4.ln_2.bias',\n",
       " 'h.4.ln_2.weight',\n",
       " 'h.4.mlp.c_fc.bias',\n",
       " 'h.4.mlp.c_fc.weight',\n",
       " 'h.4.mlp.c_proj.bias',\n",
       " 'h.4.mlp.c_proj.weight',\n",
       " 'h.5.attn.bias',\n",
       " 'h.5.attn.c_attn.bias',\n",
       " 'h.5.attn.c_attn.weight',\n",
       " 'h.5.attn.c_proj.bias',\n",
       " 'h.5.attn.c_proj.weight',\n",
       " 'h.5.ln_1.bias',\n",
       " 'h.5.ln_1.weight',\n",
       " 'h.5.ln_2.bias',\n",
       " 'h.5.ln_2.weight',\n",
       " 'h.5.mlp.c_fc.bias',\n",
       " 'h.5.mlp.c_fc.weight',\n",
       " 'h.5.mlp.c_proj.bias',\n",
       " 'h.5.mlp.c_proj.weight',\n",
       " 'h.6.attn.bias',\n",
       " 'h.6.attn.c_attn.bias',\n",
       " 'h.6.attn.c_attn.weight',\n",
       " 'h.6.attn.c_proj.bias',\n",
       " 'h.6.attn.c_proj.weight',\n",
       " 'h.6.ln_1.bias',\n",
       " 'h.6.ln_1.weight',\n",
       " 'h.6.ln_2.bias',\n",
       " 'h.6.ln_2.weight',\n",
       " 'h.6.mlp.c_fc.bias',\n",
       " 'h.6.mlp.c_fc.weight',\n",
       " 'h.6.mlp.c_proj.bias',\n",
       " 'h.6.mlp.c_proj.weight',\n",
       " 'h.7.attn.bias',\n",
       " 'h.7.attn.c_attn.bias',\n",
       " 'h.7.attn.c_attn.weight',\n",
       " 'h.7.attn.c_proj.bias',\n",
       " 'h.7.attn.c_proj.weight',\n",
       " 'h.7.ln_1.bias',\n",
       " 'h.7.ln_1.weight',\n",
       " 'h.7.ln_2.bias',\n",
       " 'h.7.ln_2.weight',\n",
       " 'h.7.mlp.c_fc.bias',\n",
       " 'h.7.mlp.c_fc.weight',\n",
       " 'h.7.mlp.c_proj.bias',\n",
       " 'h.7.mlp.c_proj.weight',\n",
       " 'h.8.attn.bias',\n",
       " 'h.8.attn.c_attn.bias',\n",
       " 'h.8.attn.c_attn.weight',\n",
       " 'h.8.attn.c_proj.bias',\n",
       " 'h.8.attn.c_proj.weight',\n",
       " 'h.8.ln_1.bias',\n",
       " 'h.8.ln_1.weight',\n",
       " 'h.8.ln_2.bias',\n",
       " 'h.8.ln_2.weight',\n",
       " 'h.8.mlp.c_fc.bias',\n",
       " 'h.8.mlp.c_fc.weight',\n",
       " 'h.8.mlp.c_proj.bias',\n",
       " 'h.8.mlp.c_proj.weight',\n",
       " 'h.9.attn.bias',\n",
       " 'h.9.attn.c_attn.bias',\n",
       " 'h.9.attn.c_attn.weight',\n",
       " 'h.9.attn.c_proj.bias',\n",
       " 'h.9.attn.c_proj.weight',\n",
       " 'h.9.ln_1.bias',\n",
       " 'h.9.ln_1.weight',\n",
       " 'h.9.ln_2.bias',\n",
       " 'h.9.ln_2.weight',\n",
       " 'h.9.mlp.c_fc.bias',\n",
       " 'h.9.mlp.c_fc.weight',\n",
       " 'h.9.mlp.c_proj.bias',\n",
       " 'h.9.mlp.c_proj.weight',\n",
       " 'ln_f.bias',\n",
       " 'ln_f.weight',\n",
       " 'wpe.weight',\n",
       " 'wte.weight']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
