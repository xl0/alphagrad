{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp training\n",
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from tidygrad import Tensor\n",
    "from tidygrad.utils import noop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_callbacks(func):\n",
    "    # print(\"Adding callbacks\", func.__name__)\n",
    "\n",
    "    def decorator(self):\n",
    "        full_name = func.__name__.replace(\"do_\", \"\")\n",
    "        pre_name = f\"pre_{full_name}\"\n",
    "        post_name = f\"post_{full_name}\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        for callback in self.callbacks:\n",
    "            getattr(callback, pre_name, noop)(self)\n",
    "\n",
    "        func(self)\n",
    "        for callback in self.callbacks:\n",
    "            getattr(callback, post_name, noop)(self)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # if end_time - start_time > 0.5:\n",
    "        #     print(f\"{full_name} took {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self, train=True, valid=True):\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "\n",
    "    @staticmethod\n",
    "    def calc() -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MultiClassAccuracy(Metric):\n",
    "    name = \"accuracy\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calc(learner) -> float:\n",
    "        _, y = learner.batch\n",
    "        return float((learner.preds.data.argmax(axis=-1) == y.data).mean())\n",
    "\n",
    "class Loss(Metric):\n",
    "    name = \"loss\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calc(learner) -> float:\n",
    "        return float(learner.loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class DictLoggerCallback:\n",
    "    val_loss = 0\n",
    "    val_error = 0\n",
    "\n",
    "    def __init__(self, metrics=None, history=None):\n",
    "        self.metrics = [MultiClassAccuracy(), Loss()] if metrics is None else metrics\n",
    "        self.history = [] if history is None else history\n",
    "\n",
    "    def log(self, learner, metric, value, accum=False, step: int = None):\n",
    "        if not hasattr(learner, \"history\"): learner.history = self.history\n",
    "        if step is None:\n",
    "            step = self.history[-1][\"step\"] + 1 if self.history else 0\n",
    "\n",
    "        if not self.history or step != self.history[-1][\"step\"]:\n",
    "            self.history.append({\"step\": step})\n",
    "\n",
    "        if metric in self.history[-1] and accum:\n",
    "            self.history[-1][metric] += value\n",
    "        else:\n",
    "            self.history[-1][metric] = value\n",
    "\n",
    "    def post_calc_loss(self, learner):\n",
    "        for m in self.metrics:\n",
    "            if learner.training:\n",
    "                if m.train: self.log(learner, f\"{m.name}\", m.calc(learner), accum=False, step=learner.step)\n",
    "            else:\n",
    "                if m.valid: self.log(learner, f\"val_{m.name}\", m.calc(learner), accum=True, step=learner.step)\n",
    "\n",
    "    def post_all_batches(self, learner):\n",
    "        for m in self.metrics:\n",
    "            if f\"val_{m.name}\" in self.history[-1]:\n",
    "                self.history[-1][f\"val_{m.name}\"] /= len(learner.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "class Learner:\n",
    "    # dataloaders - train, test\n",
    "    # model - function that outputs a tensor that can be fed into a loss function\n",
    "    # loss_func - function that takes in a tensor and outputs a scalar\n",
    "    # optimizer - Optimizer object\n",
    "    def __init__(self, dataloaders, model, loss_func, optimizer, callbacks=[]):\n",
    "        self.dataloaders = dataloaders\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = optimizer\n",
    "        self.callbacks = callbacks if callbacks else []\n",
    "\n",
    "        # The state of the learner. These are updated during training by various do_ functions\n",
    "        self.training = False  # True if training, False if val/test.\n",
    "        self.epoch = 0  # current epoch, starts with 1 when you start trainig.\n",
    "        self.step = 0  # current step, increases by 1 every (training) batch\n",
    "        self.dl = None  # current dataloader, could be train or test or val\n",
    "        self.batch = None  # The current batch as a tuple of (x, y)\n",
    "        self.preds: Tensor = None  # Output of the model\n",
    "\n",
    "    def fit(self, epochs, start_epoch=0, start_step=None):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.n_epochs = epochs\n",
    "        self.step = self.step if start_step is None else start_step\n",
    "        self.epochs = range(self.start_epoch, self.start_epoch + self.n_epochs)\n",
    "        self.do_fit()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_fit(self):\n",
    "        for e in self.epochs:\n",
    "            self.epoch = e\n",
    "            self.do_epoch()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_epoch(self):\n",
    "        self.training = True\n",
    "        self.dl = self.dataloaders.train\n",
    "        self.do_all_batches()\n",
    "        self.dl = self.dataloaders.test\n",
    "        self.training = False\n",
    "        self.do_all_batches()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_all_batches(self):\n",
    "        for batch in self.dl:\n",
    "            if self.training: self.step += 1\n",
    "            self.batch = batch\n",
    "            self.do_batch_forward()\n",
    "            self.do_calc_loss()\n",
    "            if self.training: self.do_batch_backward()\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_calc_loss(self):\n",
    "        _, y = self.batch\n",
    "        self.loss = self.loss_func(self.preds, y)\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_batch_forward(self):\n",
    "        x, _ = self.batch\n",
    "        self.preds = self.model(x)\n",
    "\n",
    "    @add_callbacks\n",
    "    def do_batch_backward(self):\n",
    "        self.loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def one_hot_encode_batch(y, n_classes):\n",
    "    diag = np.eye(n_classes)\n",
    "    return Tensor(diag[y])\n",
    "\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    assert batch_size > 0\n",
    "    assert n_classes > 0\n",
    "    # assert y.shape[0] == batch_size\n",
    "    assert np.min(y) >= 0\n",
    "\n",
    "    # Initialize a zero matrix of shape (batch_size, num_classes)\n",
    "    one_hot_matrix = np.zeros((*y.shape, n_classes))\n",
    "\n",
    "    # Fill in the appropriate elements\n",
    "    one_hot_matrix[np.arange(batch_size), y] = 1\n",
    "\n",
    "    return Tensor(one_hot_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def metrics_names_pretty(metrics):\n",
    "    \"\"\"\n",
    "    Return an str with the names of the metrics, extended to at least 8 characters\n",
    "    \"\"\"\n",
    "\n",
    "    pretty = [f\"{name:<10}\" for name in metrics]\n",
    "    return \" \".join(pretty)\n",
    "\n",
    "def metrics_last_pretty(metrics, metrics_dict):\n",
    "    \"\"\"\n",
    "    Return an str with the last values of the metrics,\n",
    "    extended to match the length of the metric names, min 10 characters\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    for name in metrics:\n",
    "        max_len = max(len(name), 10)\n",
    "        if name in metrics_dict:\n",
    "            value = metrics_dict[name]\n",
    "\n",
    "            if isinstance(value, (int, str)):\n",
    "                value = str(value)\n",
    "            elif isinstance(value, float):\n",
    "                value = (f\"{value:.6f}\")\n",
    "            else:\n",
    "                value = \" \" * (max_len)\n",
    "\n",
    "            if len(value) > max_len: value = value[:max_len - 2] + \"..\"\n",
    "\n",
    "            out.append(value + \" \" * (max_len - len(value)))\n",
    "        else:\n",
    "            out.append(\" \" * max_len)\n",
    "\n",
    "    return \" \".join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def print_metrics_header(metrics):\n",
    "    return \"Ep  | \" + metrics_names_pretty(metrics) + \" | \" + metrics_names_pretty([f\"val_{m}\" for m in metrics]) + \"\\n\"\n",
    "\n",
    "def print_metrics(learner, metrics):\n",
    "    train_metrics = {k: v for k, v in learner.history[-1].items() if k in metrics}\n",
    "    val_metrics = {f\"val_{m}\": learner.history[-1].get(f\"val_{m}\", None) for m in metrics}\n",
    "    return \" | \".join([\n",
    "        f\"{learner.epoch:<3}\",\n",
    "        metrics_last_pretty(metrics, train_metrics),\n",
    "        metrics_last_pretty([f\"val_{m}\" for m in metrics], val_metrics),\n",
    "    ]) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "from IPython.display import HTML, display\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "def denoise(L, window_size=10):\n",
    "    i = [i for i, _ in L]\n",
    "    x = np.array([x for _, x in L])\n",
    "\n",
    "    pad_size = window_size // 2\n",
    "    data_padded = np.pad(x, pad_width=pad_size, mode='edge')\n",
    "    xx = np.convolve(data_padded, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "    return [(p, q) for p, q in zip(i, xx)]\n",
    "\n",
    "def plot_metrics(learner, metrics, plot_skip=5, x_lim=None, plot_smooth_training=0):\n",
    "    if x_lim is None:\n",
    "        x_lim = len(learner.dataloaders.train) * learner.n_epochs\n",
    "    fig, ax = plt.subplots(1, len(metrics), figsize=(4 * len(metrics), 4), tight_layout=True)\n",
    "    plt.close(fig)\n",
    "    for i, m in enumerate(metrics):\n",
    "        train_metrics = []\n",
    "        val_metrics = []\n",
    "        if x_lim > len(train_metrics):\n",
    "            ax[i].set_xlim(right=x_lim)\n",
    "        ax[i].set_title(m)\n",
    "        if not hasattr(learner, \"history\"): continue\n",
    "        for idx, data in enumerate(learner.history):\n",
    "            if m in data:\n",
    "                train_metrics.append((idx, data[m]))\n",
    "            if f\"val_{m}\" in data:\n",
    "                val_metrics.append((idx, data[f\"val_{m}\"]))\n",
    "\n",
    "        if plot_smooth_training > 0:\n",
    "            smooth_train_metrics = denoise(train_metrics, plot_smooth_training)\n",
    "            lines = ax[i].plot(*zip(*smooth_train_metrics), label=f\"Train {m}\")\n",
    "            ax[i].plot(*zip(*train_metrics), color=lines[0].get_color(), alpha=0.3)\n",
    "        else:\n",
    "            ax[i].plot(*zip(*train_metrics), label=f\"Train {m}\")\n",
    "        ax[i].plot(*zip(*val_metrics), label=f\"Val {m}\")\n",
    "        if plot_skip > 0:\n",
    "            _train_metrics = train_metrics[plot_skip:]\n",
    "\n",
    "            if _train_metrics and val_metrics:\n",
    "                y_min = min(min(y for _, y in _train_metrics), min(y for _, y in val_metrics))\n",
    "                y_max = max(max(y for _, y in _train_metrics), max(y for _, y in val_metrics))\n",
    "                ax[i].set_ylim([y_min, y_max])\n",
    "        ax[i].legend()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProgressBarCallback:\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics=[\"loss\"],  # metrics to display, must be in the `learner.history`` dict\n",
    "        plot=True,  # plot the metrics\n",
    "        plot_train_skip_ylim=10,  # skip the first N training metrics when calculating the ylim\n",
    "        plot_smooth_training=0  # smooth the training metrics with a moving average of N\n",
    "    ) -> None:\n",
    "        self.metrics = metrics\n",
    "        self.plot = plot\n",
    "        self.plot_skip = plot_train_skip_ylim\n",
    "        self.plot_smooth_trainig = plot_smooth_training\n",
    "        self.plot_handle = None\n",
    "        self.stats_handle = None\n",
    "\n",
    "    def pre_fit(self, learner: Learner):\n",
    "        self.mbar = tqdm(initial=learner.start_epoch, total=learner.start_epoch + learner.n_epochs, desc=\"Epoch\")\n",
    "        self.pbar = tqdm(leave=False, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]{postfix}\")\n",
    "        if self.plot:\n",
    "            self.plot_handle = display(\n",
    "                plot_metrics(\n",
    "                    learner,\n",
    "                    self.metrics,\n",
    "                    plot_skip=self.plot_skip,\n",
    "                    x_lim=learner.n_epochs * len(learner.dataloaders.train),\n",
    "                ), display_id=True\n",
    "            )\n",
    "        self.stats = print_metrics_header(self.metrics)\n",
    "        self.stats_handle = display(HTML(\"<b><pre>\" + self.stats + \"</pre></b>\"), display_id=True)\n",
    "\n",
    "    def post_epoch(self, learner):\n",
    "        self.mbar.update(1)\n",
    "        self.stats += print_metrics(learner, self.metrics)\n",
    "        self.stats_handle.update(HTML(\"<b><pre>\" + self.stats + \"</pre></b>\"))\n",
    "\n",
    "        self.mbar.refresh()\n",
    "        self.pbar.refresh()\n",
    "\n",
    "    def pre_all_batches(self, learner):\n",
    "        self.pbar.reset(total=len(learner.dl))\n",
    "        if learner.training:\n",
    "            self.pbar.set_description_str(f\"{learner.epoch:<3}: Train\")\n",
    "        else:\n",
    "            self.pbar.set_description_str(f\"{learner.epoch:<3}: Valid\")\n",
    "\n",
    "    def post_all_batches(self, learner):\n",
    "        if self.plot:\n",
    "            self.plot_handle.update(\n",
    "                plot_metrics(\n",
    "                    learner,\n",
    "                    self.metrics,\n",
    "                    plot_skip=self.plot_skip,\n",
    "                    x_lim=learner.n_epochs * len(learner.dataloaders.train),\n",
    "                    plot_smooth_training=self.plot_smooth_trainig\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def post_calc_loss(self, learner):\n",
    "        self.pbar.update(1)\n",
    "        hist = learner.history[-1]\n",
    "        metrics = [f\"{k}={v:.4f}\" for k, v in hist.items() if k in self.metrics]\n",
    "        if not learner.training:\n",
    "            val_metrics = {f\"val_{m}\": hist[f\"val_{m}\"] for m in self.metrics if f\"val_{m}\" in hist}\n",
    "            metrics += [f\"{k}={v:.4f}\" for k, v in reversed(val_metrics.items())]\n",
    "\n",
    "        self.pbar.set_postfix_str(\" \".join(metrics), refresh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tidygrad.utils.data import DataLoader, DataLoaders\n",
    "from tidygrad.utils.datasets import MNIST, mnist_batch_tfm\n",
    "from tidygrad.func import sigmoid, relu, BCE_loss, CrossEntropy_loss\n",
    "from tidygrad.optim import Adam\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 1024\n",
    "\n",
    "mnist_train = DataLoader(MNIST(fashion=True), batch_size=BS, shuffle=True, batch_tfms=[mnist_batch_tfm])\n",
    "mnist_test = DataLoader(MNIST(train=False, fashion=True), batch_size=BS, shuffle=False, batch_tfms=[mnist_batch_tfm])\n",
    "INT_DIM = 128\n",
    "\n",
    "w1 = Tensor(np.random.randn(784, INT_DIM) * 0.1, \"w1\", requires_grad=True)\n",
    "b1 = Tensor(np.ones((1, INT_DIM)) * 0.1, \"b1\", requires_grad=True)\n",
    "w2 = Tensor(np.random.randn(INT_DIM, 10) * 0.1, \"w2\", requires_grad=True)\n",
    "# b2 = Tensor(np.zeros((1, 10)), \"b2\")\n",
    "\n",
    "def linear_model(inputs, params, debug=list()):\n",
    "    inputs.data = inputs.data.reshape(inputs.data.shape[0], -1)\n",
    "    x = inputs\n",
    "    w1, b1, w2 = params\n",
    "    z1 = relu(x.mmul(w1, \"z1\") + b1)\n",
    "    z2 = z1.mmul(w2, \"z2\")\n",
    "\n",
    "    return z2\n",
    "\n",
    "MM_func = partial(linear_model, params=[w1, b1, w2])\n",
    "optimizer = Adam([w1, b1, w2], lr=0.005)\n",
    "\n",
    "loss_f = lambda preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets.data, n_classes=10))\n",
    "# loss_f = lambda preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets.data, 10))\n",
    "\n",
    "student = Learner(\n",
    "    dataloaders=DataLoaders(mnist_train, mnist_test),\n",
    "    model=MM_func,\n",
    "    loss_func=loss_f,\n",
    "    optimizer=optimizer,\n",
    "    callbacks=[DictLoggerCallback(), ProgressBarCallback(metrics=[\n",
    "        \"loss\",\n",
    "        \"accuracy\",\n",
    "    ], plot_train_skip_ylim=15, plot_smooth_training=5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca1cf84a7d74983bf209d9eab87ed20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c29691b6e214e14baf786bcde45df92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "|          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b><pre>Ep  | loss       accuracy   | val_loss   val_accuracy\n",
       "</pre></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#| eval: false\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m student\u001b[39m.\u001b[39;49mfit(epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep \u001b[39mif\u001b[39;00m start_step \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m start_step\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epochs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_fit()\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, pre_name, noop)(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m func(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, post_name, noop)(\u001b[39mself\u001b[39m)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m e\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_epoch()\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, pre_name, noop)(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m func(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, post_name, noop)(\u001b[39mself\u001b[39m)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloaders\u001b[39m.\u001b[39mtrain\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_all_batches()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloaders\u001b[39m.\u001b[39mtest\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, pre_name, noop)(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m func(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, post_name, noop)(\u001b[39mself\u001b[39m)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m batch\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_batch_forward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_calc_loss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_batch_backward()\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, pre_name, noop)(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m func(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mgetattr\u001b[39m(callback, post_name, noop)(\u001b[39mself\u001b[39m)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m@add_callbacks\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_calc_loss\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     _, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreds, y)\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m MM_func \u001b[39m=\u001b[39m partial(linear_model, params\u001b[39m=\u001b[39m[w1, b1, w2])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer \u001b[39m=\u001b[39m Adam([w1, b1, w2], lr\u001b[39m=\u001b[39m\u001b[39m0.005\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss_f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets\u001b[39m.\u001b[39;49mdata, n_classes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# loss_f = lambda preds, targets: CrossEntropy_loss(preds, one_hot_encode_batch(targets.data, 10))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m student \u001b[39m=\u001b[39m Learner(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     dataloaders\u001b[39m=\u001b[39mDataLoaders(mnist_train, mnist_test),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     model\u001b[39m=\u001b[39mMM_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     ], plot_train_skip_ylim\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, plot_smooth_training\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n",
      "\u001b[1;32m/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mone_hot_encode_batch\u001b[39m(y, n_classes):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     diag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39meye(n_classes)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/xl0/work/projects/grads/tidygrad/nbs/06_training.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Tensor(diag[y])\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "student.fit(epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
